repo,path,func_name,original_string,language,code,code_tokens,docstring,docstring_tokens,sha,url,partition,summary
ageitgey/face_recognition,examples/face_recognition_knn.py,train,"def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    """"""
    Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        ├── <person1>/
        │   ├── <somename1>.jpeg
        │   ├── <somename2>.jpeg
        │   ├── ...
        ├── <person2>/
        │   ├── <somename1>.jpeg
        │   └── <somename2>.jpeg
        └── ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
    """"""
    X = []
    y = []

    # Loop through each person in the training set
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue

        # Loop through each training image for the current person
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)

            if len(face_bounding_boxes) != 1:
                # If there are no people (or too many people) in a training image, skip the image.
                if verbose:
                    print(""Image {} not suitable for training: {}"".format(img_path, ""Didn't find a face"" if len(face_bounding_boxes) < 1 else ""Found more than one face""))
            else:
                # Add face encoding for current image to the training set
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)

    # Determine how many neighbors to use for weighting in the KNN classifier
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print(""Chose n_neighbors automatically:"", n_neighbors)

    # Create and train the KNN classifier
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)

    # Save the trained KNN classifier
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)

    return knn_clf",python,"def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):
    """"""
    Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        ├── <person1>/
        │   ├── <somename1>.jpeg
        │   ├── <somename2>.jpeg
        │   ├── ...
        ├── <person2>/
        │   ├── <somename1>.jpeg
        │   └── <somename2>.jpeg
        └── ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.
    """"""
    X = []
    y = []

    # Loop through each person in the training set
    for class_dir in os.listdir(train_dir):
        if not os.path.isdir(os.path.join(train_dir, class_dir)):
            continue

        # Loop through each training image for the current person
        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):
            image = face_recognition.load_image_file(img_path)
            face_bounding_boxes = face_recognition.face_locations(image)

            if len(face_bounding_boxes) != 1:
                # If there are no people (or too many people) in a training image, skip the image.
                if verbose:
                    print(""Image {} not suitable for training: {}"".format(img_path, ""Didn't find a face"" if len(face_bounding_boxes) < 1 else ""Found more than one face""))
            else:
                # Add face encoding for current image to the training set
                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])
                y.append(class_dir)

    # Determine how many neighbors to use for weighting in the KNN classifier
    if n_neighbors is None:
        n_neighbors = int(round(math.sqrt(len(X))))
        if verbose:
            print(""Chose n_neighbors automatically:"", n_neighbors)

    # Create and train the KNN classifier
    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')
    knn_clf.fit(X, y)

    # Save the trained KNN classifier
    if model_save_path is not None:
        with open(model_save_path, 'wb') as f:
            pickle.dump(knn_clf, f)

    return knn_clf","['def', 'train', '(', 'train_dir', ',', 'model_save_path', '=', 'None', ',', 'n_neighbors', '=', 'None', ',', 'knn_algo', '=', ""'ball_tree'"", ',', 'verbose', '=', 'False', ')', ':', 'X', '=', '[', ']', 'y', '=', '[', ']', '# Loop through each person in the training set', 'for', 'class_dir', 'in', 'os', '.', 'listdir', '(', 'train_dir', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isdir', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'continue', '# Loop through each training image for the current person', 'for', 'img_path', 'in', 'image_files_in_folder', '(', 'os', '.', 'path', '.', 'join', '(', 'train_dir', ',', 'class_dir', ')', ')', ':', 'image', '=', 'face_recognition', '.', 'load_image_file', '(', 'img_path', ')', 'face_bounding_boxes', '=', 'face_recognition', '.', 'face_locations', '(', 'image', ')', 'if', 'len', '(', 'face_bounding_boxes', ')', '!=', '1', ':', '# If there are no people (or too many people) in a training image, skip the image.', 'if', 'verbose', ':', 'print', '(', '""Image {} not suitable for training: {}""', '.', 'format', '(', 'img_path', ',', '""Didn\'t find a face""', 'if', 'len', '(', 'face_bounding_boxes', ')', '<', '1', 'else', '""Found more than one face""', ')', ')', 'else', ':', '# Add face encoding for current image to the training set', 'X', '.', 'append', '(', 'face_recognition', '.', 'face_encodings', '(', 'image', ',', 'known_face_locations', '=', 'face_bounding_boxes', ')', '[', '0', ']', ')', 'y', '.', 'append', '(', 'class_dir', ')', '# Determine how many neighbors to use for weighting in the KNN classifier', 'if', 'n_neighbors', 'is', 'None', ':', 'n_neighbors', '=', 'int', '(', 'round', '(', 'math', '.', 'sqrt', '(', 'len', '(', 'X', ')', ')', ')', ')', 'if', 'verbose', ':', 'print', '(', '""Chose n_neighbors automatically:""', ',', 'n_neighbors', ')', '# Create and train the KNN classifier', 'knn_clf', '=', 'neighbors', '.', 'KNeighborsClassifier', '(', 'n_neighbors', '=', 'n_neighbors', ',', 'algorithm', '=', 'knn_algo', ',', 'weights', '=', ""'distance'"", ')', 'knn_clf', '.', 'fit', '(', 'X', ',', 'y', ')', '# Save the trained KNN classifier', 'if', 'model_save_path', 'is', 'not', 'None', ':', 'with', 'open', '(', 'model_save_path', ',', ""'wb'"", ')', 'as', 'f', ':', 'pickle', '.', 'dump', '(', 'knn_clf', ',', 'f', ')', 'return', 'knn_clf']","Trains a k-nearest neighbors classifier for face recognition.

    :param train_dir: directory that contains a sub-directory for each known person, with its name.

     (View in source code to see train_dir example tree structure)

     Structure:
        <train_dir>/
        ├── <person1>/
        │   ├── <somename1>.jpeg
        │   ├── <somename2>.jpeg
        │   ├── ...
        ├── <person2>/
        │   ├── <somename1>.jpeg
        │   └── <somename2>.jpeg
        └── ...

    :param model_save_path: (optional) path to save model on disk
    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified
    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree
    :param verbose: verbosity of training
    :return: returns knn classifier that was trained on the given data.","['Trains', 'a', 'k', '-', 'nearest', 'neighbors', 'classifier', 'for', 'face', 'recognition', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L46-L108,train,Train a k - nearest neighbors classifier for face recognition.
ageitgey/face_recognition,examples/face_recognition_knn.py,predict,"def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    """"""
    Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
    """"""
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception(""Invalid image path: {}"".format(X_img_path))

    if knn_clf is None and model_path is None:
        raise Exception(""Must supply knn classifier either thourgh knn_clf or model_path"")

    # Load a trained KNN model (if one was passed in)
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)

    # Load image file and find face locations
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)

    # If no faces are found in the image, return an empty result.
    if len(X_face_locations) == 0:
        return []

    # Find encodings for faces in the test iamge
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)

    # Use the KNN model to find the best matches for the test face
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]

    # Predict classes and remove classifications that aren't within the threshold
    return [(pred, loc) if rec else (""unknown"", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]",python,"def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):
    """"""
    Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.
    """"""
    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:
        raise Exception(""Invalid image path: {}"".format(X_img_path))

    if knn_clf is None and model_path is None:
        raise Exception(""Must supply knn classifier either thourgh knn_clf or model_path"")

    # Load a trained KNN model (if one was passed in)
    if knn_clf is None:
        with open(model_path, 'rb') as f:
            knn_clf = pickle.load(f)

    # Load image file and find face locations
    X_img = face_recognition.load_image_file(X_img_path)
    X_face_locations = face_recognition.face_locations(X_img)

    # If no faces are found in the image, return an empty result.
    if len(X_face_locations) == 0:
        return []

    # Find encodings for faces in the test iamge
    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)

    # Use the KNN model to find the best matches for the test face
    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)
    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]

    # Predict classes and remove classifications that aren't within the threshold
    return [(pred, loc) if rec else (""unknown"", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]","['def', 'predict', '(', 'X_img_path', ',', 'knn_clf', '=', 'None', ',', 'model_path', '=', 'None', ',', 'distance_threshold', '=', '0.6', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'isfile', '(', 'X_img_path', ')', 'or', 'os', '.', 'path', '.', 'splitext', '(', 'X_img_path', ')', '[', '1', ']', '[', '1', ':', ']', 'not', 'in', 'ALLOWED_EXTENSIONS', ':', 'raise', 'Exception', '(', '""Invalid image path: {}""', '.', 'format', '(', 'X_img_path', ')', ')', 'if', 'knn_clf', 'is', 'None', 'and', 'model_path', 'is', 'None', ':', 'raise', 'Exception', '(', '""Must supply knn classifier either thourgh knn_clf or model_path""', ')', '# Load a trained KNN model (if one was passed in)', 'if', 'knn_clf', 'is', 'None', ':', 'with', 'open', '(', 'model_path', ',', ""'rb'"", ')', 'as', 'f', ':', 'knn_clf', '=', 'pickle', '.', 'load', '(', 'f', ')', '# Load image file and find face locations', 'X_img', '=', 'face_recognition', '.', 'load_image_file', '(', 'X_img_path', ')', 'X_face_locations', '=', 'face_recognition', '.', 'face_locations', '(', 'X_img', ')', '# If no faces are found in the image, return an empty result.', 'if', 'len', '(', 'X_face_locations', ')', '==', '0', ':', 'return', '[', ']', '# Find encodings for faces in the test iamge', 'faces_encodings', '=', 'face_recognition', '.', 'face_encodings', '(', 'X_img', ',', 'known_face_locations', '=', 'X_face_locations', ')', '# Use the KNN model to find the best matches for the test face', 'closest_distances', '=', 'knn_clf', '.', 'kneighbors', '(', 'faces_encodings', ',', 'n_neighbors', '=', '1', ')', 'are_matches', '=', '[', 'closest_distances', '[', '0', ']', '[', 'i', ']', '[', '0', ']', '<=', 'distance_threshold', 'for', 'i', 'in', 'range', '(', 'len', '(', 'X_face_locations', ')', ')', ']', ""# Predict classes and remove classifications that aren't within the threshold"", 'return', '[', '(', 'pred', ',', 'loc', ')', 'if', 'rec', 'else', '(', '""unknown""', ',', 'loc', ')', 'for', 'pred', ',', 'loc', ',', 'rec', 'in', 'zip', '(', 'knn_clf', '.', 'predict', '(', 'faces_encodings', ')', ',', 'X_face_locations', ',', 'are_matches', ')', ']']","Recognizes faces in given image using a trained KNN classifier

    :param X_img_path: path to image to be recognized
    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.
    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.
    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance
           of mis-classifying an unknown person as a known one.
    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].
        For faces of unrecognized persons, the name 'unknown' will be returned.","['Recognizes', 'faces', 'in', 'given', 'image', 'using', 'a', 'trained', 'KNN', 'classifier']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L111-L150,train,Predicts faces in a given image using a KNN classifier.
ageitgey/face_recognition,examples/face_recognition_knn.py,show_prediction_labels_on_image,"def show_prediction_labels_on_image(img_path, predictions):
    """"""
    Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
    """"""
    pil_image = Image.open(img_path).convert(""RGB"")
    draw = ImageDraw.Draw(pil_image)

    for name, (top, right, bottom, left) in predictions:
        # Draw a box around the face using the Pillow module
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))

        # There's a bug in Pillow where it blows up with non-UTF-8 text
        # when using the default bitmap font
        name = name.encode(""UTF-8"")

        # Draw a label with a name below the face
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))

    # Remove the drawing library from memory as per the Pillow docs
    del draw

    # Display the resulting image
    pil_image.show()",python,"def show_prediction_labels_on_image(img_path, predictions):
    """"""
    Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:
    """"""
    pil_image = Image.open(img_path).convert(""RGB"")
    draw = ImageDraw.Draw(pil_image)

    for name, (top, right, bottom, left) in predictions:
        # Draw a box around the face using the Pillow module
        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))

        # There's a bug in Pillow where it blows up with non-UTF-8 text
        # when using the default bitmap font
        name = name.encode(""UTF-8"")

        # Draw a label with a name below the face
        text_width, text_height = draw.textsize(name)
        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))

    # Remove the drawing library from memory as per the Pillow docs
    del draw

    # Display the resulting image
    pil_image.show()","['def', 'show_prediction_labels_on_image', '(', 'img_path', ',', 'predictions', ')', ':', 'pil_image', '=', 'Image', '.', 'open', '(', 'img_path', ')', '.', 'convert', '(', '""RGB""', ')', 'draw', '=', 'ImageDraw', '.', 'Draw', '(', 'pil_image', ')', 'for', 'name', ',', '(', 'top', ',', 'right', ',', 'bottom', ',', 'left', ')', 'in', 'predictions', ':', '# Draw a box around the face using the Pillow module', 'draw', '.', 'rectangle', '(', '(', '(', 'left', ',', 'top', ')', ',', '(', 'right', ',', 'bottom', ')', ')', ',', 'outline', '=', '(', '0', ',', '0', ',', '255', ')', ')', ""# There's a bug in Pillow where it blows up with non-UTF-8 text"", '# when using the default bitmap font', 'name', '=', 'name', '.', 'encode', '(', '""UTF-8""', ')', '# Draw a label with a name below the face', 'text_width', ',', 'text_height', '=', 'draw', '.', 'textsize', '(', 'name', ')', 'draw', '.', 'rectangle', '(', '(', '(', 'left', ',', 'bottom', '-', 'text_height', '-', '10', ')', ',', '(', 'right', ',', 'bottom', ')', ')', ',', 'fill', '=', '(', '0', ',', '0', ',', '255', ')', ',', 'outline', '=', '(', '0', ',', '0', ',', '255', ')', ')', 'draw', '.', 'text', '(', '(', 'left', '+', '6', ',', 'bottom', '-', 'text_height', '-', '5', ')', ',', 'name', ',', 'fill', '=', '(', '255', ',', '255', ',', '255', ',', '255', ')', ')', '# Remove the drawing library from memory as per the Pillow docs', 'del', 'draw', '# Display the resulting image', 'pil_image', '.', 'show', '(', ')']","Shows the face recognition results visually.

    :param img_path: path to image to be recognized
    :param predictions: results of the predict function
    :return:","['Shows', 'the', 'face', 'recognition', 'results', 'visually', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/examples/face_recognition_knn.py#L153-L181,train,Show the face recognition results visually.
ageitgey/face_recognition,face_recognition/api.py,_rect_to_css,"def _rect_to_css(rect):
    """"""
    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
    """"""
    return rect.top(), rect.right(), rect.bottom(), rect.left()",python,"def _rect_to_css(rect):
    """"""
    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order
    """"""
    return rect.top(), rect.right(), rect.bottom(), rect.left()","['def', '_rect_to_css', '(', 'rect', ')', ':', 'return', 'rect', '.', 'top', '(', ')', ',', 'rect', '.', 'right', '(', ')', ',', 'rect', '.', 'bottom', '(', ')', ',', 'rect', '.', 'left', '(', ')']","Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order

    :param rect: a dlib 'rect' object
    :return: a plain tuple representation of the rect in (top, right, bottom, left) order","['Convert', 'a', 'dlib', 'rect', 'object', 'to', 'a', 'plain', 'tuple', 'in', '(', 'top', 'right', 'bottom', 'left', ')', 'order']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L32-L39,train,Convert a dlib rect object to a plain tuple in ( top right bottom left
ageitgey/face_recognition,face_recognition/api.py,_trim_css_to_bounds,"def _trim_css_to_bounds(css, image_shape):
    """"""
    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
    """"""
    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)",python,"def _trim_css_to_bounds(css, image_shape):
    """"""
    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order
    """"""
    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)","['def', '_trim_css_to_bounds', '(', 'css', ',', 'image_shape', ')', ':', 'return', 'max', '(', 'css', '[', '0', ']', ',', '0', ')', ',', 'min', '(', 'css', '[', '1', ']', ',', 'image_shape', '[', '1', ']', ')', ',', 'min', '(', 'css', '[', '2', ']', ',', 'image_shape', '[', '0', ']', ')', ',', 'max', '(', 'css', '[', '3', ']', ',', '0', ')']","Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.

    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order
    :param image_shape: numpy shape of the image array
    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order","['Make', 'sure', 'a', 'tuple', 'in', '(', 'top', 'right', 'bottom', 'left', ')', 'order', 'is', 'within', 'the', 'bounds', 'of', 'the', 'image', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L52-L60,train,Trim the given CSS tuple to the bounds of the image.
ageitgey/face_recognition,face_recognition/api.py,face_distance,"def face_distance(face_encodings, face_to_compare):
    """"""
    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
    """"""
    if len(face_encodings) == 0:
        return np.empty((0))

    return np.linalg.norm(face_encodings - face_to_compare, axis=1)",python,"def face_distance(face_encodings, face_to_compare):
    """"""
    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array
    """"""
    if len(face_encodings) == 0:
        return np.empty((0))

    return np.linalg.norm(face_encodings - face_to_compare, axis=1)","['def', 'face_distance', '(', 'face_encodings', ',', 'face_to_compare', ')', ':', 'if', 'len', '(', 'face_encodings', ')', '==', '0', ':', 'return', 'np', '.', 'empty', '(', '(', '0', ')', ')', 'return', 'np', '.', 'linalg', '.', 'norm', '(', 'face_encodings', '-', 'face_to_compare', ',', 'axis', '=', '1', ')']","Given a list of face encodings, compare them to a known face encoding and get a euclidean distance
    for each comparison face. The distance tells you how similar the faces are.

    :param faces: List of face encodings to compare
    :param face_to_compare: A face encoding to compare against
    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array","['Given', 'a', 'list', 'of', 'face', 'encodings', 'compare', 'them', 'to', 'a', 'known', 'face', 'encoding', 'and', 'get', 'a', 'euclidean', 'distance', 'for', 'each', 'comparison', 'face', '.', 'The', 'distance', 'tells', 'you', 'how', 'similar', 'the', 'faces', 'are', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L63-L75,train,Given a list of face encodings compare them to a known face encoding and get a euclidean distance for each comparison face.
ageitgey/face_recognition,face_recognition/api.py,load_image_file,"def load_image_file(file, mode='RGB'):
    """"""
    Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
    """"""
    im = PIL.Image.open(file)
    if mode:
        im = im.convert(mode)
    return np.array(im)",python,"def load_image_file(file, mode='RGB'):
    """"""
    Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array
    """"""
    im = PIL.Image.open(file)
    if mode:
        im = im.convert(mode)
    return np.array(im)","['def', 'load_image_file', '(', 'file', ',', 'mode', '=', ""'RGB'"", ')', ':', 'im', '=', 'PIL', '.', 'Image', '.', 'open', '(', 'file', ')', 'if', 'mode', ':', 'im', '=', 'im', '.', 'convert', '(', 'mode', ')', 'return', 'np', '.', 'array', '(', 'im', ')']","Loads an image file (.jpg, .png, etc) into a numpy array

    :param file: image file name or file object to load
    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.
    :return: image contents as numpy array","['Loads', 'an', 'image', 'file', '(', '.', 'jpg', '.', 'png', 'etc', ')', 'into', 'a', 'numpy', 'array']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L78-L89,train,Loads an image file into a numpy array.
ageitgey/face_recognition,face_recognition/api.py,_raw_face_locations,"def _raw_face_locations(img, number_of_times_to_upsample=1, model=""hog""):
    """"""
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of dlib 'rect' objects of found face locations
    """"""
    if model == ""cnn"":
        return cnn_face_detector(img, number_of_times_to_upsample)
    else:
        return face_detector(img, number_of_times_to_upsample)",python,"def _raw_face_locations(img, number_of_times_to_upsample=1, model=""hog""):
    """"""
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of dlib 'rect' objects of found face locations
    """"""
    if model == ""cnn"":
        return cnn_face_detector(img, number_of_times_to_upsample)
    else:
        return face_detector(img, number_of_times_to_upsample)","['def', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', '=', '1', ',', 'model', '=', '""hog""', ')', ':', 'if', 'model', '==', '""cnn""', ':', 'return', 'cnn_face_detector', '(', 'img', ',', 'number_of_times_to_upsample', ')', 'else', ':', 'return', 'face_detector', '(', 'img', ',', 'number_of_times_to_upsample', ')']","Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of dlib 'rect' objects of found face locations","['Returns', 'an', 'array', 'of', 'bounding', 'boxes', 'of', 'human', 'faces', 'in', 'a', 'image']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L92-L105,train,Returns an array of bounding boxes of human faces in a image.
ageitgey/face_recognition,face_recognition/api.py,face_locations,"def face_locations(img, number_of_times_to_upsample=1, model=""hog""):
    """"""
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """"""
    if model == ""cnn"":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, ""cnn"")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]",python,"def face_locations(img, number_of_times_to_upsample=1, model=""hog""):
    """"""
    Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """"""
    if model == ""cnn"":
        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, ""cnn"")]
    else:
        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]","['def', 'face_locations', '(', 'img', ',', 'number_of_times_to_upsample', '=', '1', ',', 'model', '=', '""hog""', ')', ':', 'if', 'model', '==', '""cnn""', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', '.', 'rect', ')', ',', 'img', '.', 'shape', ')', 'for', 'face', 'in', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', ',', '""cnn""', ')', ']', 'else', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', ')', ',', 'img', '.', 'shape', ')', 'for', 'face', 'in', '_raw_face_locations', '(', 'img', ',', 'number_of_times_to_upsample', ',', 'model', ')', ']']","Returns an array of bounding boxes of human faces in a image

    :param img: An image (as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param model: Which face detection model to use. ""hog"" is less accurate but faster on CPUs. ""cnn"" is a more accurate
                  deep-learning model which is GPU/CUDA accelerated (if available). The default is ""hog"".
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order","['Returns', 'an', 'array', 'of', 'bounding', 'boxes', 'of', 'human', 'faces', 'in', 'a', 'image']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L108-L121,train,Returns an array of bounding boxes of human faces in a image.
ageitgey/face_recognition,face_recognition/api.py,batch_face_locations,"def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):
    """"""
    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """"""
    def convert_cnn_detections_to_css(detections):
        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]

    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)

    return list(map(convert_cnn_detections_to_css, raw_detections_batched))",python,"def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):
    """"""
    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order
    """"""
    def convert_cnn_detections_to_css(detections):
        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]

    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)

    return list(map(convert_cnn_detections_to_css, raw_detections_batched))","['def', 'batch_face_locations', '(', 'images', ',', 'number_of_times_to_upsample', '=', '1', ',', 'batch_size', '=', '128', ')', ':', 'def', 'convert_cnn_detections_to_css', '(', 'detections', ')', ':', 'return', '[', '_trim_css_to_bounds', '(', '_rect_to_css', '(', 'face', '.', 'rect', ')', ',', 'images', '[', '0', ']', '.', 'shape', ')', 'for', 'face', 'in', 'detections', ']', 'raw_detections_batched', '=', '_raw_face_locations_batched', '(', 'images', ',', 'number_of_times_to_upsample', ',', 'batch_size', ')', 'return', 'list', '(', 'map', '(', 'convert_cnn_detections_to_css', ',', 'raw_detections_batched', ')', ')']","Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector
    If you are using a GPU, this can give you much faster results since the GPU
    can process batches of images at once. If you aren't using a GPU, you don't need this function.

    :param img: A list of images (each as a numpy array)
    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.
    :param batch_size: How many images to include in each GPU processing batch.
    :return: A list of tuples of found face locations in css (top, right, bottom, left) order","['Returns', 'an', '2d', 'array', 'of', 'bounding', 'boxes', 'of', 'human', 'faces', 'in', 'a', 'image', 'using', 'the', 'cnn', 'face', 'detector', 'If', 'you', 'are', 'using', 'a', 'GPU', 'this', 'can', 'give', 'you', 'much', 'faster', 'results', 'since', 'the', 'GPU', 'can', 'process', 'batches', 'of', 'images', 'at', 'once', '.', 'If', 'you', 'aren', 't', 'using', 'a', 'GPU', 'you', 'don', 't', 'need', 'this', 'function', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L135-L151,train,Returns a 2d array of bounding boxes of human faces in a given image using the cnn face detectors.
ageitgey/face_recognition,face_recognition/api.py,face_landmarks,"def face_landmarks(face_image, face_locations=None, model=""large""):
    """"""
    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. ""large"" (default) or ""small"" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
    """"""
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]

    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png
    if model == 'large':
        return [{
            ""chin"": points[0:17],
            ""left_eyebrow"": points[17:22],
            ""right_eyebrow"": points[22:27],
            ""nose_bridge"": points[27:31],
            ""nose_tip"": points[31:36],
            ""left_eye"": points[36:42],
            ""right_eye"": points[42:48],
            ""top_lip"": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            ""bottom_lip"": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            ""nose_tip"": [points[4]],
            ""left_eye"": points[2:4],
            ""right_eye"": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError(""Invalid landmarks model type. Supported models are ['small', 'large']."")",python,"def face_landmarks(face_image, face_locations=None, model=""large""):
    """"""
    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. ""large"" (default) or ""small"" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)
    """"""
    landmarks = _raw_face_landmarks(face_image, face_locations, model)
    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]

    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png
    if model == 'large':
        return [{
            ""chin"": points[0:17],
            ""left_eyebrow"": points[17:22],
            ""right_eyebrow"": points[22:27],
            ""nose_bridge"": points[27:31],
            ""nose_tip"": points[31:36],
            ""left_eye"": points[36:42],
            ""right_eye"": points[42:48],
            ""top_lip"": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],
            ""bottom_lip"": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]
        } for points in landmarks_as_tuples]
    elif model == 'small':
        return [{
            ""nose_tip"": [points[4]],
            ""left_eye"": points[2:4],
            ""right_eye"": points[0:2],
        } for points in landmarks_as_tuples]
    else:
        raise ValueError(""Invalid landmarks model type. Supported models are ['small', 'large']."")","['def', 'face_landmarks', '(', 'face_image', ',', 'face_locations', '=', 'None', ',', 'model', '=', '""large""', ')', ':', 'landmarks', '=', '_raw_face_landmarks', '(', 'face_image', ',', 'face_locations', ',', 'model', ')', 'landmarks_as_tuples', '=', '[', '[', '(', 'p', '.', 'x', ',', 'p', '.', 'y', ')', 'for', 'p', 'in', 'landmark', '.', 'parts', '(', ')', ']', 'for', 'landmark', 'in', 'landmarks', ']', '# For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png', 'if', 'model', '==', ""'large'"", ':', 'return', '[', '{', '""chin""', ':', 'points', '[', '0', ':', '17', ']', ',', '""left_eyebrow""', ':', 'points', '[', '17', ':', '22', ']', ',', '""right_eyebrow""', ':', 'points', '[', '22', ':', '27', ']', ',', '""nose_bridge""', ':', 'points', '[', '27', ':', '31', ']', ',', '""nose_tip""', ':', 'points', '[', '31', ':', '36', ']', ',', '""left_eye""', ':', 'points', '[', '36', ':', '42', ']', ',', '""right_eye""', ':', 'points', '[', '42', ':', '48', ']', ',', '""top_lip""', ':', 'points', '[', '48', ':', '55', ']', '+', '[', 'points', '[', '64', ']', ']', '+', '[', 'points', '[', '63', ']', ']', '+', '[', 'points', '[', '62', ']', ']', '+', '[', 'points', '[', '61', ']', ']', '+', '[', 'points', '[', '60', ']', ']', ',', '""bottom_lip""', ':', 'points', '[', '54', ':', '60', ']', '+', '[', 'points', '[', '48', ']', ']', '+', '[', 'points', '[', '60', ']', ']', '+', '[', 'points', '[', '67', ']', ']', '+', '[', 'points', '[', '66', ']', ']', '+', '[', 'points', '[', '65', ']', ']', '+', '[', 'points', '[', '64', ']', ']', '}', 'for', 'points', 'in', 'landmarks_as_tuples', ']', 'elif', 'model', '==', ""'small'"", ':', 'return', '[', '{', '""nose_tip""', ':', '[', 'points', '[', '4', ']', ']', ',', '""left_eye""', ':', 'points', '[', '2', ':', '4', ']', ',', '""right_eye""', ':', 'points', '[', '0', ':', '2', ']', ',', '}', 'for', 'points', 'in', 'landmarks_as_tuples', ']', 'else', ':', 'raise', 'ValueError', '(', '""Invalid landmarks model type. Supported models are [\'small\', \'large\'].""', ')']","Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image

    :param face_image: image to search
    :param face_locations: Optionally provide a list of face locations to check.
    :param model: Optional - which model to use. ""large"" (default) or ""small"" which only returns 5 points but is faster.
    :return: A list of dicts of face feature locations (eyes, nose, etc)","['Given', 'an', 'image', 'returns', 'a', 'dict', 'of', 'face', 'feature', 'locations', '(', 'eyes', 'nose', 'etc', ')', 'for', 'each', 'face', 'in', 'the', 'image']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L168-L200,train,Given an image returns a dict of face feature locations
ageitgey/face_recognition,face_recognition/api.py,face_encodings,"def face_encodings(face_image, known_face_locations=None, num_jitters=1):
    """"""
    Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
    """"""
    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=""small"")
    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]",python,"def face_encodings(face_image, known_face_locations=None, num_jitters=1):
    """"""
    Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)
    """"""
    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model=""small"")
    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]","['def', 'face_encodings', '(', 'face_image', ',', 'known_face_locations', '=', 'None', ',', 'num_jitters', '=', '1', ')', ':', 'raw_landmarks', '=', '_raw_face_landmarks', '(', 'face_image', ',', 'known_face_locations', ',', 'model', '=', '""small""', ')', 'return', '[', 'np', '.', 'array', '(', 'face_encoder', '.', 'compute_face_descriptor', '(', 'face_image', ',', 'raw_landmark_set', ',', 'num_jitters', ')', ')', 'for', 'raw_landmark_set', 'in', 'raw_landmarks', ']']","Given an image, return the 128-dimension face encoding for each face in the image.

    :param face_image: The image that contains one or more faces
    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.
    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)
    :return: A list of 128-dimensional face encodings (one for each face in the image)","['Given', 'an', 'image', 'return', 'the', '128', '-', 'dimension', 'face', 'encoding', 'for', 'each', 'face', 'in', 'the', 'image', '.']",c96b010c02f15e8eeb0f71308c641179ac1f19bb,https://github.com/ageitgey/face_recognition/blob/c96b010c02f15e8eeb0f71308c641179ac1f19bb/face_recognition/api.py#L203-L213,train,Given an image returns the 128 - dimensional face encoding for each face in the image.
apache/spark,python/pyspark/sql/types.py,_parse_datatype_string,"def _parse_datatype_string(s):
    """"""
    Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string(""int "")
    IntegerType
    >>> _parse_datatype_string(""INT "")
    IntegerType
    >>> _parse_datatype_string(""a: byte, b: decimal(  16 , 8   ) "")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string(""a DOUBLE, b STRING"")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string(""a: array< short>"")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string("" map<string , string > "")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string(""blabla"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""a: int,"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""array<int"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""map<int, boolean>>"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    """"""
    sc = SparkContext._active_spark_context

    def from_ddl_schema(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())

    def from_ddl_datatype(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())

    try:
        # DDL format, ""fieldname datatype, fieldname datatype"".
        return from_ddl_schema(s)
    except Exception as e:
        try:
            # For backwards compatibility, ""integer"", ""struct<fieldname: datatype>"" and etc.
            return from_ddl_datatype(s)
        except:
            try:
                # For backwards compatibility, ""fieldname: datatype, fieldname: datatype"" case.
                return from_ddl_datatype(""struct<%s>"" % s.strip())
            except:
                raise e",python,"def _parse_datatype_string(s):
    """"""
    Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string(""int "")
    IntegerType
    >>> _parse_datatype_string(""INT "")
    IntegerType
    >>> _parse_datatype_string(""a: byte, b: decimal(  16 , 8   ) "")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string(""a DOUBLE, b STRING"")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string(""a: array< short>"")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string("" map<string , string > "")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string(""blabla"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""a: int,"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""array<int"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""map<int, boolean>>"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    """"""
    sc = SparkContext._active_spark_context

    def from_ddl_schema(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.types.StructType.fromDDL(type_str).json())

    def from_ddl_datatype(type_str):
        return _parse_datatype_json_string(
            sc._jvm.org.apache.spark.sql.api.python.PythonSQLUtils.parseDataType(type_str).json())

    try:
        # DDL format, ""fieldname datatype, fieldname datatype"".
        return from_ddl_schema(s)
    except Exception as e:
        try:
            # For backwards compatibility, ""integer"", ""struct<fieldname: datatype>"" and etc.
            return from_ddl_datatype(s)
        except:
            try:
                # For backwards compatibility, ""fieldname: datatype, fieldname: datatype"" case.
                return from_ddl_datatype(""struct<%s>"" % s.strip())
            except:
                raise e","['def', '_parse_datatype_string', '(', 's', ')', ':', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'def', 'from_ddl_schema', '(', 'type_str', ')', ':', 'return', '_parse_datatype_json_string', '(', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'types', '.', 'StructType', '.', 'fromDDL', '(', 'type_str', ')', '.', 'json', '(', ')', ')', 'def', 'from_ddl_datatype', '(', 'type_str', ')', ':', 'return', '_parse_datatype_json_string', '(', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'api', '.', 'python', '.', 'PythonSQLUtils', '.', 'parseDataType', '(', 'type_str', ')', '.', 'json', '(', ')', ')', 'try', ':', '# DDL format, ""fieldname datatype, fieldname datatype"".', 'return', 'from_ddl_schema', '(', 's', ')', 'except', 'Exception', 'as', 'e', ':', 'try', ':', '# For backwards compatibility, ""integer"", ""struct<fieldname: datatype>"" and etc.', 'return', 'from_ddl_datatype', '(', 's', ')', 'except', ':', 'try', ':', '# For backwards compatibility, ""fieldname: datatype, fieldname: datatype"" case.', 'return', 'from_ddl_datatype', '(', '""struct<%s>""', '%', 's', '.', 'strip', '(', ')', ')', 'except', ':', 'raise', 'e']","Parses the given data type string to a :class:`DataType`. The data type string format equals
    to :class:`DataType.simpleString`, except that top level struct type can omit
    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead
    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name
    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted
    string and case-insensitive strings.

    >>> _parse_datatype_string(""int "")
    IntegerType
    >>> _parse_datatype_string(""INT "")
    IntegerType
    >>> _parse_datatype_string(""a: byte, b: decimal(  16 , 8   ) "")
    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))
    >>> _parse_datatype_string(""a DOUBLE, b STRING"")
    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))
    >>> _parse_datatype_string(""a: array< short>"")
    StructType(List(StructField(a,ArrayType(ShortType,true),true)))
    >>> _parse_datatype_string("" map<string , string > "")
    MapType(StringType,StringType,true)

    >>> # Error cases
    >>> _parse_datatype_string(""blabla"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""a: int,"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""array<int"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...
    >>> _parse_datatype_string(""map<int, boolean>>"") # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ParseException:...","['Parses', 'the', 'given', 'data', 'type', 'string', 'to', 'a', ':', 'class', ':', 'DataType', '.', 'The', 'data', 'type', 'string', 'format', 'equals', 'to', ':', 'class', ':', 'DataType', '.', 'simpleString', 'except', 'that', 'top', 'level', 'struct', 'type', 'can', 'omit', 'the', 'struct<', '>', 'and', 'atomic', 'types', 'use', 'typeName', '()', 'as', 'their', 'format', 'e', '.', 'g', '.', 'use', 'byte', 'instead', 'of', 'tinyint', 'for', ':', 'class', ':', 'ByteType', '.', 'We', 'can', 'also', 'use', 'int', 'as', 'a', 'short', 'name', 'for', ':', 'class', ':', 'IntegerType', '.', 'Since', 'Spark', '2', '.', '3', 'this', 'also', 'supports', 'a', 'schema', 'in', 'a', 'DDL', '-', 'formatted', 'string', 'and', 'case', '-', 'insensitive', 'strings', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L758-L820,train,Parses a string into a base - level structure type.
apache/spark,python/pyspark/sql/types.py,_int_size_to_type,"def _int_size_to_type(size):
    """"""
    Return the Catalyst datatype from the size of integers.
    """"""
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType",python,"def _int_size_to_type(size):
    """"""
    Return the Catalyst datatype from the size of integers.
    """"""
    if size <= 8:
        return ByteType
    if size <= 16:
        return ShortType
    if size <= 32:
        return IntegerType
    if size <= 64:
        return LongType","['def', '_int_size_to_type', '(', 'size', ')', ':', 'if', 'size', '<=', '8', ':', 'return', 'ByteType', 'if', 'size', '<=', '16', ':', 'return', 'ShortType', 'if', 'size', '<=', '32', ':', 'return', 'IntegerType', 'if', 'size', '<=', '64', ':', 'return', 'LongType']",Return the Catalyst datatype from the size of integers.,"['Return', 'the', 'Catalyst', 'datatype', 'from', 'the', 'size', 'of', 'integers', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L944-L955,train,Return the Catalyst datatype from the size of integers.
apache/spark,python/pyspark/sql/types.py,_infer_type,"def _infer_type(obj):
    """"""Infer the DataType from obj
    """"""
    if obj is None:
        return NullType()

    if hasattr(obj, '__UDT__'):
        return obj.__UDT__

    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        # the precision and scale of `obj` may be different from row to row.
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()

    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError(""not supported type: array(%s)"" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError(""not supported type: %s"" % type(obj))",python,"def _infer_type(obj):
    """"""Infer the DataType from obj
    """"""
    if obj is None:
        return NullType()

    if hasattr(obj, '__UDT__'):
        return obj.__UDT__

    dataType = _type_mappings.get(type(obj))
    if dataType is DecimalType:
        # the precision and scale of `obj` may be different from row to row.
        return DecimalType(38, 18)
    elif dataType is not None:
        return dataType()

    if isinstance(obj, dict):
        for key, value in obj.items():
            if key is not None and value is not None:
                return MapType(_infer_type(key), _infer_type(value), True)
        return MapType(NullType(), NullType(), True)
    elif isinstance(obj, list):
        for v in obj:
            if v is not None:
                return ArrayType(_infer_type(obj[0]), True)
        return ArrayType(NullType(), True)
    elif isinstance(obj, array):
        if obj.typecode in _array_type_mappings:
            return ArrayType(_array_type_mappings[obj.typecode](), False)
        else:
            raise TypeError(""not supported type: array(%s)"" % obj.typecode)
    else:
        try:
            return _infer_schema(obj)
        except TypeError:
            raise TypeError(""not supported type: %s"" % type(obj))","['def', '_infer_type', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'return', 'NullType', '(', ')', 'if', 'hasattr', '(', 'obj', ',', ""'__UDT__'"", ')', ':', 'return', 'obj', '.', '__UDT__', 'dataType', '=', '_type_mappings', '.', 'get', '(', 'type', '(', 'obj', ')', ')', 'if', 'dataType', 'is', 'DecimalType', ':', '# the precision and scale of `obj` may be different from row to row.', 'return', 'DecimalType', '(', '38', ',', '18', ')', 'elif', 'dataType', 'is', 'not', 'None', ':', 'return', 'dataType', '(', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'for', 'key', ',', 'value', 'in', 'obj', '.', 'items', '(', ')', ':', 'if', 'key', 'is', 'not', 'None', 'and', 'value', 'is', 'not', 'None', ':', 'return', 'MapType', '(', '_infer_type', '(', 'key', ')', ',', '_infer_type', '(', 'value', ')', ',', 'True', ')', 'return', 'MapType', '(', 'NullType', '(', ')', ',', 'NullType', '(', ')', ',', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'for', 'v', 'in', 'obj', ':', 'if', 'v', 'is', 'not', 'None', ':', 'return', 'ArrayType', '(', '_infer_type', '(', 'obj', '[', '0', ']', ')', ',', 'True', ')', 'return', 'ArrayType', '(', 'NullType', '(', ')', ',', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'array', ')', ':', 'if', 'obj', '.', 'typecode', 'in', '_array_type_mappings', ':', 'return', 'ArrayType', '(', '_array_type_mappings', '[', 'obj', '.', 'typecode', ']', '(', ')', ',', 'False', ')', 'else', ':', 'raise', 'TypeError', '(', '""not supported type: array(%s)""', '%', 'obj', '.', 'typecode', ')', 'else', ':', 'try', ':', 'return', '_infer_schema', '(', 'obj', ')', 'except', 'TypeError', ':', 'raise', 'TypeError', '(', '""not supported type: %s""', '%', 'type', '(', 'obj', ')', ')']",Infer the DataType from obj,"['Infer', 'the', 'DataType', 'from', 'obj']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1003-L1038,train,"Infer the DataType from obj
   "
apache/spark,python/pyspark/sql/types.py,_infer_schema,"def _infer_schema(row, names=None):
    """"""Infer the schema from dict/namedtuple/object""""""
    if isinstance(row, dict):
        items = sorted(row.items())

    elif isinstance(row, (tuple, list)):
        if hasattr(row, ""__fields__""):  # Row
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, ""_fields""):  # namedtuple
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)

    elif hasattr(row, ""__dict__""):  # object
        items = sorted(row.__dict__.items())

    else:
        raise TypeError(""Can not infer schema for type: %s"" % type(row))

    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)",python,"def _infer_schema(row, names=None):
    """"""Infer the schema from dict/namedtuple/object""""""
    if isinstance(row, dict):
        items = sorted(row.items())

    elif isinstance(row, (tuple, list)):
        if hasattr(row, ""__fields__""):  # Row
            items = zip(row.__fields__, tuple(row))
        elif hasattr(row, ""_fields""):  # namedtuple
            items = zip(row._fields, tuple(row))
        else:
            if names is None:
                names = ['_%d' % i for i in range(1, len(row) + 1)]
            elif len(names) < len(row):
                names.extend('_%d' % i for i in range(len(names) + 1, len(row) + 1))
            items = zip(names, row)

    elif hasattr(row, ""__dict__""):  # object
        items = sorted(row.__dict__.items())

    else:
        raise TypeError(""Can not infer schema for type: %s"" % type(row))

    fields = [StructField(k, _infer_type(v), True) for k, v in items]
    return StructType(fields)","['def', '_infer_schema', '(', 'row', ',', 'names', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'row', ',', 'dict', ')', ':', 'items', '=', 'sorted', '(', 'row', '.', 'items', '(', ')', ')', 'elif', 'isinstance', '(', 'row', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'hasattr', '(', 'row', ',', '""__fields__""', ')', ':', '# Row', 'items', '=', 'zip', '(', 'row', '.', '__fields__', ',', 'tuple', '(', 'row', ')', ')', 'elif', 'hasattr', '(', 'row', ',', '""_fields""', ')', ':', '# namedtuple', 'items', '=', 'zip', '(', 'row', '.', '_fields', ',', 'tuple', '(', 'row', ')', ')', 'else', ':', 'if', 'names', 'is', 'None', ':', 'names', '=', '[', ""'_%d'"", '%', 'i', 'for', 'i', 'in', 'range', '(', '1', ',', 'len', '(', 'row', ')', '+', '1', ')', ']', 'elif', 'len', '(', 'names', ')', '<', 'len', '(', 'row', ')', ':', 'names', '.', 'extend', '(', ""'_%d'"", '%', 'i', 'for', 'i', 'in', 'range', '(', 'len', '(', 'names', ')', '+', '1', ',', 'len', '(', 'row', ')', '+', '1', ')', ')', 'items', '=', 'zip', '(', 'names', ',', 'row', ')', 'elif', 'hasattr', '(', 'row', ',', '""__dict__""', ')', ':', '# object', 'items', '=', 'sorted', '(', 'row', '.', '__dict__', '.', 'items', '(', ')', ')', 'else', ':', 'raise', 'TypeError', '(', '""Can not infer schema for type: %s""', '%', 'type', '(', 'row', ')', ')', 'fields', '=', '[', 'StructField', '(', 'k', ',', '_infer_type', '(', 'v', ')', ',', 'True', ')', 'for', 'k', ',', 'v', 'in', 'items', ']', 'return', 'StructType', '(', 'fields', ')']",Infer the schema from dict/namedtuple/object,"['Infer', 'the', 'schema', 'from', 'dict', '/', 'namedtuple', '/', 'object']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1041-L1065,train,Infer the schema from dict namedtuple or object
apache/spark,python/pyspark/sql/types.py,_has_nulltype,"def _has_nulltype(dt):
    """""" Return whether there is NullType in `dt` or not """"""
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)",python,"def _has_nulltype(dt):
    """""" Return whether there is NullType in `dt` or not """"""
    if isinstance(dt, StructType):
        return any(_has_nulltype(f.dataType) for f in dt.fields)
    elif isinstance(dt, ArrayType):
        return _has_nulltype((dt.elementType))
    elif isinstance(dt, MapType):
        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)
    else:
        return isinstance(dt, NullType)","['def', '_has_nulltype', '(', 'dt', ')', ':', 'if', 'isinstance', '(', 'dt', ',', 'StructType', ')', ':', 'return', 'any', '(', '_has_nulltype', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dt', '.', 'fields', ')', 'elif', 'isinstance', '(', 'dt', ',', 'ArrayType', ')', ':', 'return', '_has_nulltype', '(', '(', 'dt', '.', 'elementType', ')', ')', 'elif', 'isinstance', '(', 'dt', ',', 'MapType', ')', ':', 'return', '_has_nulltype', '(', 'dt', '.', 'keyType', ')', 'or', '_has_nulltype', '(', 'dt', '.', 'valueType', ')', 'else', ':', 'return', 'isinstance', '(', 'dt', ',', 'NullType', ')']",Return whether there is NullType in `dt` or not,"['Return', 'whether', 'there', 'is', 'NullType', 'in', 'dt', 'or', 'not']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1068-L1077,train,Return whether there is NullType in dt
apache/spark,python/pyspark/sql/types.py,_create_converter,"def _create_converter(dataType):
    """"""Create a converter to drop the names of fields in obj """"""
    if not _need_converter(dataType):
        return lambda x: x

    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]

    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())

    elif isinstance(dataType, NullType):
        return lambda x: None

    elif not isinstance(dataType, StructType):
        return lambda x: x

    # dataType must be StructType
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)

    def convert_struct(obj):
        if obj is None:
            return

        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)

        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, ""__dict__""):  # object
            d = obj.__dict__
        else:
            raise TypeError(""Unexpected obj type: %s"" % type(obj))

        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])

    return convert_struct",python,"def _create_converter(dataType):
    """"""Create a converter to drop the names of fields in obj """"""
    if not _need_converter(dataType):
        return lambda x: x

    if isinstance(dataType, ArrayType):
        conv = _create_converter(dataType.elementType)
        return lambda row: [conv(v) for v in row]

    elif isinstance(dataType, MapType):
        kconv = _create_converter(dataType.keyType)
        vconv = _create_converter(dataType.valueType)
        return lambda row: dict((kconv(k), vconv(v)) for k, v in row.items())

    elif isinstance(dataType, NullType):
        return lambda x: None

    elif not isinstance(dataType, StructType):
        return lambda x: x

    # dataType must be StructType
    names = [f.name for f in dataType.fields]
    converters = [_create_converter(f.dataType) for f in dataType.fields]
    convert_fields = any(_need_converter(f.dataType) for f in dataType.fields)

    def convert_struct(obj):
        if obj is None:
            return

        if isinstance(obj, (tuple, list)):
            if convert_fields:
                return tuple(conv(v) for v, conv in zip(obj, converters))
            else:
                return tuple(obj)

        if isinstance(obj, dict):
            d = obj
        elif hasattr(obj, ""__dict__""):  # object
            d = obj.__dict__
        else:
            raise TypeError(""Unexpected obj type: %s"" % type(obj))

        if convert_fields:
            return tuple([conv(d.get(name)) for name, conv in zip(names, converters)])
        else:
            return tuple([d.get(name) for name in names])

    return convert_struct","['def', '_create_converter', '(', 'dataType', ')', ':', 'if', 'not', '_need_converter', '(', 'dataType', ')', ':', 'return', 'lambda', 'x', ':', 'x', 'if', 'isinstance', '(', 'dataType', ',', 'ArrayType', ')', ':', 'conv', '=', '_create_converter', '(', 'dataType', '.', 'elementType', ')', 'return', 'lambda', 'row', ':', '[', 'conv', '(', 'v', ')', 'for', 'v', 'in', 'row', ']', 'elif', 'isinstance', '(', 'dataType', ',', 'MapType', ')', ':', 'kconv', '=', '_create_converter', '(', 'dataType', '.', 'keyType', ')', 'vconv', '=', '_create_converter', '(', 'dataType', '.', 'valueType', ')', 'return', 'lambda', 'row', ':', 'dict', '(', '(', 'kconv', '(', 'k', ')', ',', 'vconv', '(', 'v', ')', ')', 'for', 'k', ',', 'v', 'in', 'row', '.', 'items', '(', ')', ')', 'elif', 'isinstance', '(', 'dataType', ',', 'NullType', ')', ':', 'return', 'lambda', 'x', ':', 'None', 'elif', 'not', 'isinstance', '(', 'dataType', ',', 'StructType', ')', ':', 'return', 'lambda', 'x', ':', 'x', '# dataType must be StructType', 'names', '=', '[', 'f', '.', 'name', 'for', 'f', 'in', 'dataType', '.', 'fields', ']', 'converters', '=', '[', '_create_converter', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dataType', '.', 'fields', ']', 'convert_fields', '=', 'any', '(', '_need_converter', '(', 'f', '.', 'dataType', ')', 'for', 'f', 'in', 'dataType', '.', 'fields', ')', 'def', 'convert_struct', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'return', 'if', 'isinstance', '(', 'obj', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'convert_fields', ':', 'return', 'tuple', '(', 'conv', '(', 'v', ')', 'for', 'v', ',', 'conv', 'in', 'zip', '(', 'obj', ',', 'converters', ')', ')', 'else', ':', 'return', 'tuple', '(', 'obj', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'd', '=', 'obj', 'elif', 'hasattr', '(', 'obj', ',', '""__dict__""', ')', ':', '# object', 'd', '=', 'obj', '.', '__dict__', 'else', ':', 'raise', 'TypeError', '(', '""Unexpected obj type: %s""', '%', 'type', '(', 'obj', ')', ')', 'if', 'convert_fields', ':', 'return', 'tuple', '(', '[', 'conv', '(', 'd', '.', 'get', '(', 'name', ')', ')', 'for', 'name', ',', 'conv', 'in', 'zip', '(', 'names', ',', 'converters', ')', ']', ')', 'else', ':', 'return', 'tuple', '(', '[', 'd', '.', 'get', '(', 'name', ')', 'for', 'name', 'in', 'names', ']', ')', 'return', 'convert_struct']",Create a converter to drop the names of fields in obj,"['Create', 'a', 'converter', 'to', 'drop', 'the', 'names', 'of', 'fields', 'in', 'obj']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1133-L1180,train,Create a converter to drop the names of fields in obj
apache/spark,python/pyspark/sql/types.py,_make_type_verifier,"def _make_type_verifier(dataType, nullable=True, name=None):
    """"""
    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("""")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add(""a"", IntegerType()).add(""b"", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    """"""

    if name is None:
        new_msg = lambda msg: msg
        new_name = lambda n: ""field %s"" % n
    else:
        new_msg = lambda msg: ""%s: %s"" % (name, msg)
        new_name = lambda n: ""field %s in %s"" % (n, name)

    def verify_nullability(obj):
        if obj is None:
            if nullable:
                return True
            else:
                raise ValueError(new_msg(""This field is not nullable, but got None""))
        else:
            return False

    _type = type(dataType)

    def assert_acceptable_types(obj):
        assert _type in _acceptable_types, \
            new_msg(""unknown datatype: %s for object %r"" % (dataType, obj))

    def verify_acceptable_types(obj):
        # subclass of them can not be fromInternal in JVM
        if type(obj) not in _acceptable_types[_type]:
            raise TypeError(new_msg(""%s can not accept object %r in type %s""
                                    % (dataType, obj, type(obj))))

    if isinstance(dataType, StringType):
        # StringType can work with any types
        verify_value = lambda _: _

    elif isinstance(dataType, UserDefinedType):
        verifier = _make_type_verifier(dataType.sqlType(), name=name)

        def verify_udf(obj):
            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):
                raise ValueError(new_msg(""%r is not an instance of type %r"" % (obj, dataType)))
            verifier(dataType.toInternal(obj))

        verify_value = verify_udf

    elif isinstance(dataType, ByteType):
        def verify_byte(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -128 or obj > 127:
                raise ValueError(new_msg(""object of ByteType out of range, got: %s"" % obj))

        verify_value = verify_byte

    elif isinstance(dataType, ShortType):
        def verify_short(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -32768 or obj > 32767:
                raise ValueError(new_msg(""object of ShortType out of range, got: %s"" % obj))

        verify_value = verify_short

    elif isinstance(dataType, IntegerType):
        def verify_integer(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -2147483648 or obj > 2147483647:
                raise ValueError(
                    new_msg(""object of IntegerType out of range, got: %s"" % obj))

        verify_value = verify_integer

    elif isinstance(dataType, ArrayType):
        element_verifier = _make_type_verifier(
            dataType.elementType, dataType.containsNull, name=""element in array %s"" % name)

        def verify_array(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for i in obj:
                element_verifier(i)

        verify_value = verify_array

    elif isinstance(dataType, MapType):
        key_verifier = _make_type_verifier(dataType.keyType, False, name=""key of map %s"" % name)
        value_verifier = _make_type_verifier(
            dataType.valueType, dataType.valueContainsNull, name=""value of map %s"" % name)

        def verify_map(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for k, v in obj.items():
                key_verifier(k)
                value_verifier(v)

        verify_value = verify_map

    elif isinstance(dataType, StructType):
        verifiers = []
        for f in dataType.fields:
            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))
            verifiers.append((f.name, verifier))

        def verify_struct(obj):
            assert_acceptable_types(obj)

            if isinstance(obj, dict):
                for f, verifier in verifiers:
                    verifier(obj.get(f))
            elif isinstance(obj, Row) and getattr(obj, ""__from_dict__"", False):
                # the order in obj could be different than dataType.fields
                for f, verifier in verifiers:
                    verifier(obj[f])
            elif isinstance(obj, (tuple, list)):
                if len(obj) != len(verifiers):
                    raise ValueError(
                        new_msg(""Length of object (%d) does not match with ""
                                ""length of fields (%d)"" % (len(obj), len(verifiers))))
                for v, (_, verifier) in zip(obj, verifiers):
                    verifier(v)
            elif hasattr(obj, ""__dict__""):
                d = obj.__dict__
                for f, verifier in verifiers:
                    verifier(d.get(f))
            else:
                raise TypeError(new_msg(""StructType can not accept object %r in type %s""
                                        % (obj, type(obj))))
        verify_value = verify_struct

    else:
        def verify_default(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)

        verify_value = verify_default

    def verify(obj):
        if not verify_nullability(obj):
            verify_value(obj)

    return verify",python,"def _make_type_verifier(dataType, nullable=True, name=None):
    """"""
    Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("""")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add(""a"", IntegerType()).add(""b"", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    """"""

    if name is None:
        new_msg = lambda msg: msg
        new_name = lambda n: ""field %s"" % n
    else:
        new_msg = lambda msg: ""%s: %s"" % (name, msg)
        new_name = lambda n: ""field %s in %s"" % (n, name)

    def verify_nullability(obj):
        if obj is None:
            if nullable:
                return True
            else:
                raise ValueError(new_msg(""This field is not nullable, but got None""))
        else:
            return False

    _type = type(dataType)

    def assert_acceptable_types(obj):
        assert _type in _acceptable_types, \
            new_msg(""unknown datatype: %s for object %r"" % (dataType, obj))

    def verify_acceptable_types(obj):
        # subclass of them can not be fromInternal in JVM
        if type(obj) not in _acceptable_types[_type]:
            raise TypeError(new_msg(""%s can not accept object %r in type %s""
                                    % (dataType, obj, type(obj))))

    if isinstance(dataType, StringType):
        # StringType can work with any types
        verify_value = lambda _: _

    elif isinstance(dataType, UserDefinedType):
        verifier = _make_type_verifier(dataType.sqlType(), name=name)

        def verify_udf(obj):
            if not (hasattr(obj, '__UDT__') and obj.__UDT__ == dataType):
                raise ValueError(new_msg(""%r is not an instance of type %r"" % (obj, dataType)))
            verifier(dataType.toInternal(obj))

        verify_value = verify_udf

    elif isinstance(dataType, ByteType):
        def verify_byte(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -128 or obj > 127:
                raise ValueError(new_msg(""object of ByteType out of range, got: %s"" % obj))

        verify_value = verify_byte

    elif isinstance(dataType, ShortType):
        def verify_short(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -32768 or obj > 32767:
                raise ValueError(new_msg(""object of ShortType out of range, got: %s"" % obj))

        verify_value = verify_short

    elif isinstance(dataType, IntegerType):
        def verify_integer(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            if obj < -2147483648 or obj > 2147483647:
                raise ValueError(
                    new_msg(""object of IntegerType out of range, got: %s"" % obj))

        verify_value = verify_integer

    elif isinstance(dataType, ArrayType):
        element_verifier = _make_type_verifier(
            dataType.elementType, dataType.containsNull, name=""element in array %s"" % name)

        def verify_array(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for i in obj:
                element_verifier(i)

        verify_value = verify_array

    elif isinstance(dataType, MapType):
        key_verifier = _make_type_verifier(dataType.keyType, False, name=""key of map %s"" % name)
        value_verifier = _make_type_verifier(
            dataType.valueType, dataType.valueContainsNull, name=""value of map %s"" % name)

        def verify_map(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)
            for k, v in obj.items():
                key_verifier(k)
                value_verifier(v)

        verify_value = verify_map

    elif isinstance(dataType, StructType):
        verifiers = []
        for f in dataType.fields:
            verifier = _make_type_verifier(f.dataType, f.nullable, name=new_name(f.name))
            verifiers.append((f.name, verifier))

        def verify_struct(obj):
            assert_acceptable_types(obj)

            if isinstance(obj, dict):
                for f, verifier in verifiers:
                    verifier(obj.get(f))
            elif isinstance(obj, Row) and getattr(obj, ""__from_dict__"", False):
                # the order in obj could be different than dataType.fields
                for f, verifier in verifiers:
                    verifier(obj[f])
            elif isinstance(obj, (tuple, list)):
                if len(obj) != len(verifiers):
                    raise ValueError(
                        new_msg(""Length of object (%d) does not match with ""
                                ""length of fields (%d)"" % (len(obj), len(verifiers))))
                for v, (_, verifier) in zip(obj, verifiers):
                    verifier(v)
            elif hasattr(obj, ""__dict__""):
                d = obj.__dict__
                for f, verifier in verifiers:
                    verifier(d.get(f))
            else:
                raise TypeError(new_msg(""StructType can not accept object %r in type %s""
                                        % (obj, type(obj))))
        verify_value = verify_struct

    else:
        def verify_default(obj):
            assert_acceptable_types(obj)
            verify_acceptable_types(obj)

        verify_value = verify_default

    def verify(obj):
        if not verify_nullability(obj):
            verify_value(obj)

    return verify","['def', '_make_type_verifier', '(', 'dataType', ',', 'nullable', '=', 'True', ',', 'name', '=', 'None', ')', ':', 'if', 'name', 'is', 'None', ':', 'new_msg', '=', 'lambda', 'msg', ':', 'msg', 'new_name', '=', 'lambda', 'n', ':', '""field %s""', '%', 'n', 'else', ':', 'new_msg', '=', 'lambda', 'msg', ':', '""%s: %s""', '%', '(', 'name', ',', 'msg', ')', 'new_name', '=', 'lambda', 'n', ':', '""field %s in %s""', '%', '(', 'n', ',', 'name', ')', 'def', 'verify_nullability', '(', 'obj', ')', ':', 'if', 'obj', 'is', 'None', ':', 'if', 'nullable', ':', 'return', 'True', 'else', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""This field is not nullable, but got None""', ')', ')', 'else', ':', 'return', 'False', '_type', '=', 'type', '(', 'dataType', ')', 'def', 'assert_acceptable_types', '(', 'obj', ')', ':', 'assert', '_type', 'in', '_acceptable_types', ',', 'new_msg', '(', '""unknown datatype: %s for object %r""', '%', '(', 'dataType', ',', 'obj', ')', ')', 'def', 'verify_acceptable_types', '(', 'obj', ')', ':', '# subclass of them can not be fromInternal in JVM', 'if', 'type', '(', 'obj', ')', 'not', 'in', '_acceptable_types', '[', '_type', ']', ':', 'raise', 'TypeError', '(', 'new_msg', '(', '""%s can not accept object %r in type %s""', '%', '(', 'dataType', ',', 'obj', ',', 'type', '(', 'obj', ')', ')', ')', ')', 'if', 'isinstance', '(', 'dataType', ',', 'StringType', ')', ':', '# StringType can work with any types', 'verify_value', '=', 'lambda', '_', ':', '_', 'elif', 'isinstance', '(', 'dataType', ',', 'UserDefinedType', ')', ':', 'verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'sqlType', '(', ')', ',', 'name', '=', 'name', ')', 'def', 'verify_udf', '(', 'obj', ')', ':', 'if', 'not', '(', 'hasattr', '(', 'obj', ',', ""'__UDT__'"", ')', 'and', 'obj', '.', '__UDT__', '==', 'dataType', ')', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""%r is not an instance of type %r""', '%', '(', 'obj', ',', 'dataType', ')', ')', ')', 'verifier', '(', 'dataType', '.', 'toInternal', '(', 'obj', ')', ')', 'verify_value', '=', 'verify_udf', 'elif', 'isinstance', '(', 'dataType', ',', 'ByteType', ')', ':', 'def', 'verify_byte', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '128', 'or', 'obj', '>', '127', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""object of ByteType out of range, got: %s""', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_byte', 'elif', 'isinstance', '(', 'dataType', ',', 'ShortType', ')', ':', 'def', 'verify_short', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '32768', 'or', 'obj', '>', '32767', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""object of ShortType out of range, got: %s""', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_short', 'elif', 'isinstance', '(', 'dataType', ',', 'IntegerType', ')', ':', 'def', 'verify_integer', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'if', 'obj', '<', '-', '2147483648', 'or', 'obj', '>', '2147483647', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""object of IntegerType out of range, got: %s""', '%', 'obj', ')', ')', 'verify_value', '=', 'verify_integer', 'elif', 'isinstance', '(', 'dataType', ',', 'ArrayType', ')', ':', 'element_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'elementType', ',', 'dataType', '.', 'containsNull', ',', 'name', '=', '""element in array %s""', '%', 'name', ')', 'def', 'verify_array', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'for', 'i', 'in', 'obj', ':', 'element_verifier', '(', 'i', ')', 'verify_value', '=', 'verify_array', 'elif', 'isinstance', '(', 'dataType', ',', 'MapType', ')', ':', 'key_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'keyType', ',', 'False', ',', 'name', '=', '""key of map %s""', '%', 'name', ')', 'value_verifier', '=', '_make_type_verifier', '(', 'dataType', '.', 'valueType', ',', 'dataType', '.', 'valueContainsNull', ',', 'name', '=', '""value of map %s""', '%', 'name', ')', 'def', 'verify_map', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', ':', 'key_verifier', '(', 'k', ')', 'value_verifier', '(', 'v', ')', 'verify_value', '=', 'verify_map', 'elif', 'isinstance', '(', 'dataType', ',', 'StructType', ')', ':', 'verifiers', '=', '[', ']', 'for', 'f', 'in', 'dataType', '.', 'fields', ':', 'verifier', '=', '_make_type_verifier', '(', 'f', '.', 'dataType', ',', 'f', '.', 'nullable', ',', 'name', '=', 'new_name', '(', 'f', '.', 'name', ')', ')', 'verifiers', '.', 'append', '(', '(', 'f', '.', 'name', ',', 'verifier', ')', ')', 'def', 'verify_struct', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'if', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'obj', '.', 'get', '(', 'f', ')', ')', 'elif', 'isinstance', '(', 'obj', ',', 'Row', ')', 'and', 'getattr', '(', 'obj', ',', '""__from_dict__""', ',', 'False', ')', ':', '# the order in obj could be different than dataType.fields', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'obj', '[', 'f', ']', ')', 'elif', 'isinstance', '(', 'obj', ',', '(', 'tuple', ',', 'list', ')', ')', ':', 'if', 'len', '(', 'obj', ')', '!=', 'len', '(', 'verifiers', ')', ':', 'raise', 'ValueError', '(', 'new_msg', '(', '""Length of object (%d) does not match with ""', '""length of fields (%d)""', '%', '(', 'len', '(', 'obj', ')', ',', 'len', '(', 'verifiers', ')', ')', ')', ')', 'for', 'v', ',', '(', '_', ',', 'verifier', ')', 'in', 'zip', '(', 'obj', ',', 'verifiers', ')', ':', 'verifier', '(', 'v', ')', 'elif', 'hasattr', '(', 'obj', ',', '""__dict__""', ')', ':', 'd', '=', 'obj', '.', '__dict__', 'for', 'f', ',', 'verifier', 'in', 'verifiers', ':', 'verifier', '(', 'd', '.', 'get', '(', 'f', ')', ')', 'else', ':', 'raise', 'TypeError', '(', 'new_msg', '(', '""StructType can not accept object %r in type %s""', '%', '(', 'obj', ',', 'type', '(', 'obj', ')', ')', ')', ')', 'verify_value', '=', 'verify_struct', 'else', ':', 'def', 'verify_default', '(', 'obj', ')', ':', 'assert_acceptable_types', '(', 'obj', ')', 'verify_acceptable_types', '(', 'obj', ')', 'verify_value', '=', 'verify_default', 'def', 'verify', '(', 'obj', ')', ':', 'if', 'not', 'verify_nullability', '(', 'obj', ')', ':', 'verify_value', '(', 'obj', ')', 'return', 'verify']","Make a verifier that checks the type of obj against dataType and raises a TypeError if they do
    not match.

    This verifier also checks the value of obj against datatype and raises a ValueError if it's not
    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is
    not checked, so it will become infinity when cast to Java float if it overflows.

    >>> _make_type_verifier(StructType([]))(None)
    >>> _make_type_verifier(StringType())("""")
    >>> _make_type_verifier(LongType())(0)
    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))
    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    TypeError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})
    >>> _make_type_verifier(StructType([]))(())
    >>> _make_type_verifier(StructType([]))([])
    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> # Check if numeric values are within the allowed range.
    >>> _make_type_verifier(ByteType())(12)
    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(
    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})
    Traceback (most recent call last):
        ...
    ValueError:...
    >>> schema = StructType().add(""a"", IntegerType()).add(""b"", StringType(), False)
    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL
    Traceback (most recent call last):
        ...
    ValueError:...","['Make', 'a', 'verifier', 'that', 'checks', 'the', 'type', 'of', 'obj', 'against', 'dataType', 'and', 'raises', 'a', 'TypeError', 'if', 'they', 'do', 'not', 'match', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1202-L1391,train,Returns a verifier that checks the type of obj against dataType and raises a TypeError if they do not match.
apache/spark,python/pyspark/sql/types.py,to_arrow_type,"def to_arrow_type(dt):
    """""" Convert Spark data type to pyarrow type
    """"""
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError(""Unsupported type in conversion to Arrow: "" + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError(""Nested StructType not supported in conversion to Arrow"")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError(""Unsupported type in conversion to Arrow: "" + str(dt))
    return arrow_type",python,"def to_arrow_type(dt):
    """""" Convert Spark data type to pyarrow type
    """"""
    import pyarrow as pa
    if type(dt) == BooleanType:
        arrow_type = pa.bool_()
    elif type(dt) == ByteType:
        arrow_type = pa.int8()
    elif type(dt) == ShortType:
        arrow_type = pa.int16()
    elif type(dt) == IntegerType:
        arrow_type = pa.int32()
    elif type(dt) == LongType:
        arrow_type = pa.int64()
    elif type(dt) == FloatType:
        arrow_type = pa.float32()
    elif type(dt) == DoubleType:
        arrow_type = pa.float64()
    elif type(dt) == DecimalType:
        arrow_type = pa.decimal128(dt.precision, dt.scale)
    elif type(dt) == StringType:
        arrow_type = pa.string()
    elif type(dt) == BinaryType:
        arrow_type = pa.binary()
    elif type(dt) == DateType:
        arrow_type = pa.date32()
    elif type(dt) == TimestampType:
        # Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read
        arrow_type = pa.timestamp('us', tz='UTC')
    elif type(dt) == ArrayType:
        if type(dt.elementType) in [StructType, TimestampType]:
            raise TypeError(""Unsupported type in conversion to Arrow: "" + str(dt))
        arrow_type = pa.list_(to_arrow_type(dt.elementType))
    elif type(dt) == StructType:
        if any(type(field.dataType) == StructType for field in dt):
            raise TypeError(""Nested StructType not supported in conversion to Arrow"")
        fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
                  for field in dt]
        arrow_type = pa.struct(fields)
    else:
        raise TypeError(""Unsupported type in conversion to Arrow: "" + str(dt))
    return arrow_type","['def', 'to_arrow_type', '(', 'dt', ')', ':', 'import', 'pyarrow', 'as', 'pa', 'if', 'type', '(', 'dt', ')', '==', 'BooleanType', ':', 'arrow_type', '=', 'pa', '.', 'bool_', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'ByteType', ':', 'arrow_type', '=', 'pa', '.', 'int8', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'ShortType', ':', 'arrow_type', '=', 'pa', '.', 'int16', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'IntegerType', ':', 'arrow_type', '=', 'pa', '.', 'int32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'LongType', ':', 'arrow_type', '=', 'pa', '.', 'int64', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'FloatType', ':', 'arrow_type', '=', 'pa', '.', 'float32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DoubleType', ':', 'arrow_type', '=', 'pa', '.', 'float64', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DecimalType', ':', 'arrow_type', '=', 'pa', '.', 'decimal128', '(', 'dt', '.', 'precision', ',', 'dt', '.', 'scale', ')', 'elif', 'type', '(', 'dt', ')', '==', 'StringType', ':', 'arrow_type', '=', 'pa', '.', 'string', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'BinaryType', ':', 'arrow_type', '=', 'pa', '.', 'binary', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'DateType', ':', 'arrow_type', '=', 'pa', '.', 'date32', '(', ')', 'elif', 'type', '(', 'dt', ')', '==', 'TimestampType', ':', '# Timestamps should be in UTC, JVM Arrow timestamps require a timezone to be read', 'arrow_type', '=', 'pa', '.', 'timestamp', '(', ""'us'"", ',', 'tz', '=', ""'UTC'"", ')', 'elif', 'type', '(', 'dt', ')', '==', 'ArrayType', ':', 'if', 'type', '(', 'dt', '.', 'elementType', ')', 'in', '[', 'StructType', ',', 'TimestampType', ']', ':', 'raise', 'TypeError', '(', '""Unsupported type in conversion to Arrow: ""', '+', 'str', '(', 'dt', ')', ')', 'arrow_type', '=', 'pa', '.', 'list_', '(', 'to_arrow_type', '(', 'dt', '.', 'elementType', ')', ')', 'elif', 'type', '(', 'dt', ')', '==', 'StructType', ':', 'if', 'any', '(', 'type', '(', 'field', '.', 'dataType', ')', '==', 'StructType', 'for', 'field', 'in', 'dt', ')', ':', 'raise', 'TypeError', '(', '""Nested StructType not supported in conversion to Arrow""', ')', 'fields', '=', '[', 'pa', '.', 'field', '(', 'field', '.', 'name', ',', 'to_arrow_type', '(', 'field', '.', 'dataType', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'dt', ']', 'arrow_type', '=', 'pa', '.', 'struct', '(', 'fields', ')', 'else', ':', 'raise', 'TypeError', '(', '""Unsupported type in conversion to Arrow: ""', '+', 'str', '(', 'dt', ')', ')', 'return', 'arrow_type']",Convert Spark data type to pyarrow type,"['Convert', 'Spark', 'data', 'type', 'to', 'pyarrow', 'type']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1581-L1622,train,Convert Spark data type to Arrow type
apache/spark,python/pyspark/sql/types.py,to_arrow_schema,"def to_arrow_schema(schema):
    """""" Convert a schema from Spark to Arrow
    """"""
    import pyarrow as pa
    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
              for field in schema]
    return pa.schema(fields)",python,"def to_arrow_schema(schema):
    """""" Convert a schema from Spark to Arrow
    """"""
    import pyarrow as pa
    fields = [pa.field(field.name, to_arrow_type(field.dataType), nullable=field.nullable)
              for field in schema]
    return pa.schema(fields)","['def', 'to_arrow_schema', '(', 'schema', ')', ':', 'import', 'pyarrow', 'as', 'pa', 'fields', '=', '[', 'pa', '.', 'field', '(', 'field', '.', 'name', ',', 'to_arrow_type', '(', 'field', '.', 'dataType', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'schema', ']', 'return', 'pa', '.', 'schema', '(', 'fields', ')']",Convert a schema from Spark to Arrow,"['Convert', 'a', 'schema', 'from', 'Spark', 'to', 'Arrow']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1625-L1631,train,"Convert a Spark schema from Spark to Arrow
   "
apache/spark,python/pyspark/sql/types.py,from_arrow_type,"def from_arrow_type(at):
    """""" Convert pyarrow type to Spark data type.
    """"""
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError(""Unsupported type in conversion from Arrow: "" + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError(""Nested StructType not supported in conversion from Arrow: "" + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError(""Unsupported type in conversion from Arrow: "" + str(at))
    return spark_type",python,"def from_arrow_type(at):
    """""" Convert pyarrow type to Spark data type.
    """"""
    import pyarrow.types as types
    if types.is_boolean(at):
        spark_type = BooleanType()
    elif types.is_int8(at):
        spark_type = ByteType()
    elif types.is_int16(at):
        spark_type = ShortType()
    elif types.is_int32(at):
        spark_type = IntegerType()
    elif types.is_int64(at):
        spark_type = LongType()
    elif types.is_float32(at):
        spark_type = FloatType()
    elif types.is_float64(at):
        spark_type = DoubleType()
    elif types.is_decimal(at):
        spark_type = DecimalType(precision=at.precision, scale=at.scale)
    elif types.is_string(at):
        spark_type = StringType()
    elif types.is_binary(at):
        spark_type = BinaryType()
    elif types.is_date32(at):
        spark_type = DateType()
    elif types.is_timestamp(at):
        spark_type = TimestampType()
    elif types.is_list(at):
        if types.is_timestamp(at.value_type):
            raise TypeError(""Unsupported type in conversion from Arrow: "" + str(at))
        spark_type = ArrayType(from_arrow_type(at.value_type))
    elif types.is_struct(at):
        if any(types.is_struct(field.type) for field in at):
            raise TypeError(""Nested StructType not supported in conversion from Arrow: "" + str(at))
        return StructType(
            [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
             for field in at])
    else:
        raise TypeError(""Unsupported type in conversion from Arrow: "" + str(at))
    return spark_type","['def', 'from_arrow_type', '(', 'at', ')', ':', 'import', 'pyarrow', '.', 'types', 'as', 'types', 'if', 'types', '.', 'is_boolean', '(', 'at', ')', ':', 'spark_type', '=', 'BooleanType', '(', ')', 'elif', 'types', '.', 'is_int8', '(', 'at', ')', ':', 'spark_type', '=', 'ByteType', '(', ')', 'elif', 'types', '.', 'is_int16', '(', 'at', ')', ':', 'spark_type', '=', 'ShortType', '(', ')', 'elif', 'types', '.', 'is_int32', '(', 'at', ')', ':', 'spark_type', '=', 'IntegerType', '(', ')', 'elif', 'types', '.', 'is_int64', '(', 'at', ')', ':', 'spark_type', '=', 'LongType', '(', ')', 'elif', 'types', '.', 'is_float32', '(', 'at', ')', ':', 'spark_type', '=', 'FloatType', '(', ')', 'elif', 'types', '.', 'is_float64', '(', 'at', ')', ':', 'spark_type', '=', 'DoubleType', '(', ')', 'elif', 'types', '.', 'is_decimal', '(', 'at', ')', ':', 'spark_type', '=', 'DecimalType', '(', 'precision', '=', 'at', '.', 'precision', ',', 'scale', '=', 'at', '.', 'scale', ')', 'elif', 'types', '.', 'is_string', '(', 'at', ')', ':', 'spark_type', '=', 'StringType', '(', ')', 'elif', 'types', '.', 'is_binary', '(', 'at', ')', ':', 'spark_type', '=', 'BinaryType', '(', ')', 'elif', 'types', '.', 'is_date32', '(', 'at', ')', ':', 'spark_type', '=', 'DateType', '(', ')', 'elif', 'types', '.', 'is_timestamp', '(', 'at', ')', ':', 'spark_type', '=', 'TimestampType', '(', ')', 'elif', 'types', '.', 'is_list', '(', 'at', ')', ':', 'if', 'types', '.', 'is_timestamp', '(', 'at', '.', 'value_type', ')', ':', 'raise', 'TypeError', '(', '""Unsupported type in conversion from Arrow: ""', '+', 'str', '(', 'at', ')', ')', 'spark_type', '=', 'ArrayType', '(', 'from_arrow_type', '(', 'at', '.', 'value_type', ')', ')', 'elif', 'types', '.', 'is_struct', '(', 'at', ')', ':', 'if', 'any', '(', 'types', '.', 'is_struct', '(', 'field', '.', 'type', ')', 'for', 'field', 'in', 'at', ')', ':', 'raise', 'TypeError', '(', '""Nested StructType not supported in conversion from Arrow: ""', '+', 'str', '(', 'at', ')', ')', 'return', 'StructType', '(', '[', 'StructField', '(', 'field', '.', 'name', ',', 'from_arrow_type', '(', 'field', '.', 'type', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'at', ']', ')', 'else', ':', 'raise', 'TypeError', '(', '""Unsupported type in conversion from Arrow: ""', '+', 'str', '(', 'at', ')', ')', 'return', 'spark_type']",Convert pyarrow type to Spark data type.,"['Convert', 'pyarrow', 'type', 'to', 'Spark', 'data', 'type', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1634-L1674,train,Convert a pyarrow type to Spark data type.
apache/spark,python/pyspark/sql/types.py,from_arrow_schema,"def from_arrow_schema(arrow_schema):
    """""" Convert schema from Arrow to Spark.
    """"""
    return StructType(
        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
         for field in arrow_schema])",python,"def from_arrow_schema(arrow_schema):
    """""" Convert schema from Arrow to Spark.
    """"""
    return StructType(
        [StructField(field.name, from_arrow_type(field.type), nullable=field.nullable)
         for field in arrow_schema])","['def', 'from_arrow_schema', '(', 'arrow_schema', ')', ':', 'return', 'StructType', '(', '[', 'StructField', '(', 'field', '.', 'name', ',', 'from_arrow_type', '(', 'field', '.', 'type', ')', ',', 'nullable', '=', 'field', '.', 'nullable', ')', 'for', 'field', 'in', 'arrow_schema', ']', ')']",Convert schema from Arrow to Spark.,"['Convert', 'schema', 'from', 'Arrow', 'to', 'Spark', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1677-L1682,train,"Convert schema from Arrow to Spark.
   "
apache/spark,python/pyspark/sql/types.py,_check_series_localize_timestamps,"def _check_series_localize_timestamps(s, timezone):
    """"""
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s",python,"def _check_series_localize_timestamps(s, timezone):
    """"""
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64tz_dtype
    tz = timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(tz).dt.tz_localize(None)
    else:
        return s","['def', '_check_series_localize_timestamps', '(', 's', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64tz_dtype', 'tz', '=', 'timezone', 'or', '_get_local_timezone', '(', ')', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', 'tz', ')', '.', 'dt', '.', 'tz_localize', '(', 'None', ')', 'else', ':', 'return', 's']","Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.

    If the input series is not a timestamp series, then the same series is returned. If the input
    series is a timestamp series, then a converted series is returned.

    :param s: pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series that have been converted to tz-naive","['Convert', 'timezone', 'aware', 'timestamps', 'to', 'timezone', '-', 'naive', 'in', 'the', 'specified', 'timezone', 'or', 'local', 'timezone', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1700-L1720,train,Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone.
apache/spark,python/pyspark/sql/types.py,_check_dataframe_localize_timestamps,"def _check_dataframe_localize_timestamps(pdf, timezone):
    """"""
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    for column, series in pdf.iteritems():
        pdf[column] = _check_series_localize_timestamps(series, timezone)
    return pdf",python,"def _check_dataframe_localize_timestamps(pdf, timezone):
    """"""
    Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    for column, series in pdf.iteritems():
        pdf[column] = _check_series_localize_timestamps(series, timezone)
    return pdf","['def', '_check_dataframe_localize_timestamps', '(', 'pdf', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'for', 'column', ',', 'series', 'in', 'pdf', '.', 'iteritems', '(', ')', ':', 'pdf', '[', 'column', ']', '=', '_check_series_localize_timestamps', '(', 'series', ',', 'timezone', ')', 'return', 'pdf']","Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone

    :param pdf: pandas.DataFrame
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive","['Convert', 'timezone', 'aware', 'timestamps', 'to', 'timezone', '-', 'naive', 'in', 'the', 'specified', 'timezone', 'or', 'local', 'timezone']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1723-L1736,train,Convert timezone aware timestamps to timezone - naive in the specified timezone or local timezone - naive in the specified timezone or local timezone - naive in the specified timezone.
apache/spark,python/pyspark/sql/types.py,_check_series_convert_timestamps_internal,"def _check_series_convert_timestamps_internal(s, timezone):
    """"""
    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64_dtype(s.dtype):
        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive
        # timestamp is during the hour when the clock is adjusted backward during due to
        # daylight saving time (dst).
        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to
        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize
        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either
        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).
        #
        # Here we explicit choose to use standard time. This matches the default behavior of
        # pytz.
        #
        # Here are some code to help understand this behavior:
        # >>> import datetime
        # >>> import pandas as pd
        # >>> import pytz
        # >>>
        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)
        # >>> ts = pd.Series([t])
        # >>> tz = pytz.timezone('America/New_York')
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=True)
        # 0   2015-11-01 01:30:00-04:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=False)
        # 0   2015-11-01 01:30:00-05:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> str(tz.localize(t))
        # '2015-11-01 01:30:00-05:00'
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s",python,"def _check_series_convert_timestamps_internal(s, timezone):
    """"""
    Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    from pandas.api.types import is_datetime64_dtype, is_datetime64tz_dtype
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64_dtype(s.dtype):
        # When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive
        # timestamp is during the hour when the clock is adjusted backward during due to
        # daylight saving time (dst).
        # E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to
        # 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize
        # a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either
        # dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).
        #
        # Here we explicit choose to use standard time. This matches the default behavior of
        # pytz.
        #
        # Here are some code to help understand this behavior:
        # >>> import datetime
        # >>> import pandas as pd
        # >>> import pytz
        # >>>
        # >>> t = datetime.datetime(2015, 11, 1, 1, 30)
        # >>> ts = pd.Series([t])
        # >>> tz = pytz.timezone('America/New_York')
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=True)
        # 0   2015-11-01 01:30:00-04:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> ts.dt.tz_localize(tz, ambiguous=False)
        # 0   2015-11-01 01:30:00-05:00
        # dtype: datetime64[ns, America/New_York]
        # >>>
        # >>> str(tz.localize(t))
        # '2015-11-01 01:30:00-05:00'
        tz = timezone or _get_local_timezone()
        return s.dt.tz_localize(tz, ambiguous=False).dt.tz_convert('UTC')
    elif is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert('UTC')
    else:
        return s","['def', '_check_series_convert_timestamps_internal', '(', 's', ',', 'timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64_dtype', ',', 'is_datetime64tz_dtype', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64_dtype', '(', 's', '.', 'dtype', ')', ':', '# When tz_localize a tz-naive timestamp, the result is ambiguous if the tz-naive', '# timestamp is during the hour when the clock is adjusted backward during due to', '# daylight saving time (dst).', '# E.g., for America/New_York, the clock is adjusted backward on 2015-11-01 2:00 to', '# 2015-11-01 1:00 from dst-time to standard time, and therefore, when tz_localize', '# a tz-naive timestamp 2015-11-01 1:30 with America/New_York timezone, it can be either', '# dst time (2015-01-01 1:30-0400) or standard time (2015-11-01 1:30-0500).', '#', '# Here we explicit choose to use standard time. This matches the default behavior of', '# pytz.', '#', '# Here are some code to help understand this behavior:', '# >>> import datetime', '# >>> import pandas as pd', '# >>> import pytz', '# >>>', '# >>> t = datetime.datetime(2015, 11, 1, 1, 30)', '# >>> ts = pd.Series([t])', ""# >>> tz = pytz.timezone('America/New_York')"", '# >>>', '# >>> ts.dt.tz_localize(tz, ambiguous=True)', '# 0   2015-11-01 01:30:00-04:00', '# dtype: datetime64[ns, America/New_York]', '# >>>', '# >>> ts.dt.tz_localize(tz, ambiguous=False)', '# 0   2015-11-01 01:30:00-05:00', '# dtype: datetime64[ns, America/New_York]', '# >>>', '# >>> str(tz.localize(t))', ""# '2015-11-01 01:30:00-05:00'"", 'tz', '=', 'timezone', 'or', '_get_local_timezone', '(', ')', 'return', 's', '.', 'dt', '.', 'tz_localize', '(', 'tz', ',', 'ambiguous', '=', 'False', ')', '.', 'dt', '.', 'tz_convert', '(', ""'UTC'"", ')', 'elif', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', ""'UTC'"", ')', 'else', ':', 'return', 's']","Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for
    Spark internal storage

    :param s: a pandas.Series
    :param timezone: the timezone to convert. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone","['Convert', 'a', 'tz', '-', 'naive', 'timestamp', 'in', 'the', 'specified', 'timezone', 'or', 'local', 'timezone', 'to', 'UTC', 'normalized', 'for', 'Spark', 'internal', 'storage']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1739-L1789,train,Convert a tz - naive timestamp in the specified timezone or local timezone to UTC normalized for Spark internal storage.
apache/spark,python/pyspark/sql/types.py,_check_series_convert_timestamps_localize,"def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    """"""
    Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s",python,"def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone):
    """"""
    Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive
    """"""
    from pyspark.sql.utils import require_minimum_pandas_version
    require_minimum_pandas_version()

    import pandas as pd
    from pandas.api.types import is_datetime64tz_dtype, is_datetime64_dtype
    from_tz = from_timezone or _get_local_timezone()
    to_tz = to_timezone or _get_local_timezone()
    # TODO: handle nested timestamps, such as ArrayType(TimestampType())?
    if is_datetime64tz_dtype(s.dtype):
        return s.dt.tz_convert(to_tz).dt.tz_localize(None)
    elif is_datetime64_dtype(s.dtype) and from_tz != to_tz:
        # `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT.
        return s.apply(
            lambda ts: ts.tz_localize(from_tz, ambiguous=False).tz_convert(to_tz).tz_localize(None)
            if ts is not pd.NaT else pd.NaT)
    else:
        return s","['def', '_check_series_convert_timestamps_localize', '(', 's', ',', 'from_timezone', ',', 'to_timezone', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'utils', 'import', 'require_minimum_pandas_version', 'require_minimum_pandas_version', '(', ')', 'import', 'pandas', 'as', 'pd', 'from', 'pandas', '.', 'api', '.', 'types', 'import', 'is_datetime64tz_dtype', ',', 'is_datetime64_dtype', 'from_tz', '=', 'from_timezone', 'or', '_get_local_timezone', '(', ')', 'to_tz', '=', 'to_timezone', 'or', '_get_local_timezone', '(', ')', '# TODO: handle nested timestamps, such as ArrayType(TimestampType())?', 'if', 'is_datetime64tz_dtype', '(', 's', '.', 'dtype', ')', ':', 'return', 's', '.', 'dt', '.', 'tz_convert', '(', 'to_tz', ')', '.', 'dt', '.', 'tz_localize', '(', 'None', ')', 'elif', 'is_datetime64_dtype', '(', 's', '.', 'dtype', ')', 'and', 'from_tz', '!=', 'to_tz', ':', ""# `s.dt.tz_localize('tzlocal()')` doesn't work properly when including NaT."", 'return', 's', '.', 'apply', '(', 'lambda', 'ts', ':', 'ts', '.', 'tz_localize', '(', 'from_tz', ',', 'ambiguous', '=', 'False', ')', '.', 'tz_convert', '(', 'to_tz', ')', '.', 'tz_localize', '(', 'None', ')', 'if', 'ts', 'is', 'not', 'pd', '.', 'NaT', 'else', 'pd', '.', 'NaT', ')', 'else', ':', 'return', 's']","Convert timestamp to timezone-naive in the specified timezone or local timezone

    :param s: a pandas.Series
    :param from_timezone: the timezone to convert from. if None then use local timezone
    :param to_timezone: the timezone to convert to. if None then use local timezone
    :return pandas.Series where if it is a timestamp, has been converted to tz-naive","['Convert', 'timestamp', 'to', 'timezone', '-', 'naive', 'in', 'the', 'specified', 'timezone', 'or', 'local', 'timezone']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1792-L1817,train,Convert timestamp to timezone - naive in the specified timezone or local timezone.
apache/spark,python/pyspark/sql/types.py,StructType.add,"def add(self, field, data_type=None, nullable=True, metadata=None):
        """"""
        Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add(""f1"", StringType(), True).add(""f2"", StringType(), True, None)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True), \\
        ...     StructField(""f2"", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField(""f1"", StringType(), True))
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(""f1"", ""string"", True)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
        """"""
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError(""Must specify DataType if passing name of struct_field to create."")

            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        # Precalculated list of fields that need conversion with fromInternal/toInternal functions
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self",python,"def add(self, field, data_type=None, nullable=True, metadata=None):
        """"""
        Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add(""f1"", StringType(), True).add(""f2"", StringType(), True, None)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True), \\
        ...     StructField(""f2"", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField(""f1"", StringType(), True))
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(""f1"", ""string"", True)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType
        """"""
        if isinstance(field, StructField):
            self.fields.append(field)
            self.names.append(field.name)
        else:
            if isinstance(field, str) and data_type is None:
                raise ValueError(""Must specify DataType if passing name of struct_field to create."")

            if isinstance(data_type, str):
                data_type_f = _parse_datatype_json_value(data_type)
            else:
                data_type_f = data_type
            self.fields.append(StructField(field, data_type_f, nullable, metadata))
            self.names.append(field)
        # Precalculated list of fields that need conversion with fromInternal/toInternal functions
        self._needConversion = [f.needConversion() for f in self]
        self._needSerializeAnyField = any(self._needConversion)
        return self","['def', 'add', '(', 'self', ',', 'field', ',', 'data_type', '=', 'None', ',', 'nullable', '=', 'True', ',', 'metadata', '=', 'None', ')', ':', 'if', 'isinstance', '(', 'field', ',', 'StructField', ')', ':', 'self', '.', 'fields', '.', 'append', '(', 'field', ')', 'self', '.', 'names', '.', 'append', '(', 'field', '.', 'name', ')', 'else', ':', 'if', 'isinstance', '(', 'field', ',', 'str', ')', 'and', 'data_type', 'is', 'None', ':', 'raise', 'ValueError', '(', '""Must specify DataType if passing name of struct_field to create.""', ')', 'if', 'isinstance', '(', 'data_type', ',', 'str', ')', ':', 'data_type_f', '=', '_parse_datatype_json_value', '(', 'data_type', ')', 'else', ':', 'data_type_f', '=', 'data_type', 'self', '.', 'fields', '.', 'append', '(', 'StructField', '(', 'field', ',', 'data_type_f', ',', 'nullable', ',', 'metadata', ')', ')', 'self', '.', 'names', '.', 'append', '(', 'field', ')', '# Precalculated list of fields that need conversion with fromInternal/toInternal functions', 'self', '.', '_needConversion', '=', '[', 'f', '.', 'needConversion', '(', ')', 'for', 'f', 'in', 'self', ']', 'self', '.', '_needSerializeAnyField', '=', 'any', '(', 'self', '.', '_needConversion', ')', 'return', 'self']","Construct a StructType by adding new elements to it to define the schema. The method accepts
        either:

            a) A single parameter which is a StructField object.
            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),
               metadata(optional). The data_type parameter may be either a String or a
               DataType object.

        >>> struct1 = StructType().add(""f1"", StringType(), True).add(""f2"", StringType(), True, None)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True), \\
        ...     StructField(""f2"", StringType(), True, None)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(StructField(""f1"", StringType(), True))
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True
        >>> struct1 = StructType().add(""f1"", ""string"", True)
        >>> struct2 = StructType([StructField(""f1"", StringType(), True)])
        >>> struct1 == struct2
        True

        :param field: Either the name of the field or a StructField object
        :param data_type: If present, the DataType of the StructField to create
        :param nullable: Whether the field to add should be nullable (default True)
        :param metadata: Any additional metadata (default None)
        :return: a new updated StructType","['Construct', 'a', 'StructType', 'by', 'adding', 'new', 'elements', 'to', 'it', 'to', 'define', 'the', 'schema', '.', 'The', 'method', 'accepts', 'either', ':']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L491-L537,train,Constructs a new StructType object by adding new elements to the list of fields.
apache/spark,python/pyspark/sql/types.py,UserDefinedType._cachedSqlType,"def _cachedSqlType(cls):
        """"""
        Cache the sqlType() into class, because it's heavy used in `toInternal`.
        """"""
        if not hasattr(cls, ""_cached_sql_type""):
            cls._cached_sql_type = cls.sqlType()
        return cls._cached_sql_type",python,"def _cachedSqlType(cls):
        """"""
        Cache the sqlType() into class, because it's heavy used in `toInternal`.
        """"""
        if not hasattr(cls, ""_cached_sql_type""):
            cls._cached_sql_type = cls.sqlType()
        return cls._cached_sql_type","['def', '_cachedSqlType', '(', 'cls', ')', ':', 'if', 'not', 'hasattr', '(', 'cls', ',', '""_cached_sql_type""', ')', ':', 'cls', '.', '_cached_sql_type', '=', 'cls', '.', 'sqlType', '(', ')', 'return', 'cls', '.', '_cached_sql_type']","Cache the sqlType() into class, because it's heavy used in `toInternal`.","['Cache', 'the', 'sqlType', '()', 'into', 'class', 'because', 'it', 's', 'heavy', 'used', 'in', 'toInternal', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L675-L681,train,Cache the sqlType into class because it s heavy used in toInternal.
apache/spark,python/pyspark/sql/types.py,Row.asDict,"def asDict(self, recursive=False):
        """"""
        Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name=""Alice"", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
        """"""
        if not hasattr(self, ""__fields__""):
            raise TypeError(""Cannot convert a Row class into dict"")

        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))",python,"def asDict(self, recursive=False):
        """"""
        Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name=""Alice"", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True
        """"""
        if not hasattr(self, ""__fields__""):
            raise TypeError(""Cannot convert a Row class into dict"")

        if recursive:
            def conv(obj):
                if isinstance(obj, Row):
                    return obj.asDict(True)
                elif isinstance(obj, list):
                    return [conv(o) for o in obj]
                elif isinstance(obj, dict):
                    return dict((k, conv(v)) for k, v in obj.items())
                else:
                    return obj
            return dict(zip(self.__fields__, (conv(o) for o in self)))
        else:
            return dict(zip(self.__fields__, self))","['def', 'asDict', '(', 'self', ',', 'recursive', '=', 'False', ')', ':', 'if', 'not', 'hasattr', '(', 'self', ',', '""__fields__""', ')', ':', 'raise', 'TypeError', '(', '""Cannot convert a Row class into dict""', ')', 'if', 'recursive', ':', 'def', 'conv', '(', 'obj', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'Row', ')', ':', 'return', 'obj', '.', 'asDict', '(', 'True', ')', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'return', '[', 'conv', '(', 'o', ')', 'for', 'o', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'dict', ')', ':', 'return', 'dict', '(', '(', 'k', ',', 'conv', '(', 'v', ')', ')', 'for', 'k', ',', 'v', 'in', 'obj', '.', 'items', '(', ')', ')', 'else', ':', 'return', 'obj', 'return', 'dict', '(', 'zip', '(', 'self', '.', '__fields__', ',', '(', 'conv', '(', 'o', ')', 'for', 'o', 'in', 'self', ')', ')', ')', 'else', ':', 'return', 'dict', '(', 'zip', '(', 'self', '.', '__fields__', ',', 'self', ')', ')']","Return as an dict

        :param recursive: turns the nested Row as dict (default: False).

        >>> Row(name=""Alice"", age=11).asDict() == {'name': 'Alice', 'age': 11}
        True
        >>> row = Row(key=1, value=Row(name='a', age=2))
        >>> row.asDict() == {'key': 1, 'value': Row(age=2, name='a')}
        True
        >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}
        True","['Return', 'as', 'an', 'dict']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/types.py#L1463-L1492,train,Converts the table into a dict.
apache/spark,python/pyspark/ml/regression.py,LinearRegressionModel.summary,"def summary(self):
        """"""
        Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """"""
        if self.hasSummary:
            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)
        else:
            raise RuntimeError(""No training summary available for this %s"" %
                               self.__class__.__name__)",python,"def summary(self):
        """"""
        Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """"""
        if self.hasSummary:
            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)
        else:
            raise RuntimeError(""No training summary available for this %s"" %
                               self.__class__.__name__)","['def', 'summary', '(', 'self', ')', ':', 'if', 'self', '.', 'hasSummary', ':', 'return', 'LinearRegressionTrainingSummary', '(', 'super', '(', 'LinearRegressionModel', ',', 'self', ')', '.', 'summary', ')', 'else', ':', 'raise', 'RuntimeError', '(', '""No training summary available for this %s""', '%', 'self', '.', '__class__', '.', '__name__', ')']","Gets summary (e.g. residuals, mse, r-squared ) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.","['Gets', 'summary', '(', 'e', '.', 'g', '.', 'residuals', 'mse', 'r', '-', 'squared', ')', 'of', 'model', 'on', 'training', 'set', '.', 'An', 'exception', 'is', 'thrown', 'if', 'trainingSummary', 'is', 'None', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L198-L208,train,Returns the summary of the LinearRegressionModel.
apache/spark,python/pyspark/ml/regression.py,LinearRegressionModel.evaluate,"def evaluate(self, dataset):
        """"""
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """"""
        if not isinstance(dataset, DataFrame):
            raise ValueError(""dataset must be a DataFrame but got %s."" % type(dataset))
        java_lr_summary = self._call_java(""evaluate"", dataset)
        return LinearRegressionSummary(java_lr_summary)",python,"def evaluate(self, dataset):
        """"""
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """"""
        if not isinstance(dataset, DataFrame):
            raise ValueError(""dataset must be a DataFrame but got %s."" % type(dataset))
        java_lr_summary = self._call_java(""evaluate"", dataset)
        return LinearRegressionSummary(java_lr_summary)","['def', 'evaluate', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'DataFrame', ')', ':', 'raise', 'ValueError', '(', '""dataset must be a DataFrame but got %s.""', '%', 'type', '(', 'dataset', ')', ')', 'java_lr_summary', '=', 'self', '.', '_call_java', '(', '""evaluate""', ',', 'dataset', ')', 'return', 'LinearRegressionSummary', '(', 'java_lr_summary', ')']","Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`","['Evaluates', 'the', 'model', 'on', 'a', 'test', 'dataset', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L211-L222,train,Evaluates the model on a test dataset.
apache/spark,python/pyspark/ml/regression.py,GeneralizedLinearRegressionModel.summary,"def summary(self):
        """"""
        Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """"""
        if self.hasSummary:
            return GeneralizedLinearRegressionTrainingSummary(
                super(GeneralizedLinearRegressionModel, self).summary)
        else:
            raise RuntimeError(""No training summary available for this %s"" %
                               self.__class__.__name__)",python,"def summary(self):
        """"""
        Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.
        """"""
        if self.hasSummary:
            return GeneralizedLinearRegressionTrainingSummary(
                super(GeneralizedLinearRegressionModel, self).summary)
        else:
            raise RuntimeError(""No training summary available for this %s"" %
                               self.__class__.__name__)","['def', 'summary', '(', 'self', ')', ':', 'if', 'self', '.', 'hasSummary', ':', 'return', 'GeneralizedLinearRegressionTrainingSummary', '(', 'super', '(', 'GeneralizedLinearRegressionModel', ',', 'self', ')', '.', 'summary', ')', 'else', ':', 'raise', 'RuntimeError', '(', '""No training summary available for this %s""', '%', 'self', '.', '__class__', '.', '__name__', ')']","Gets summary (e.g. residuals, deviance, pValues) of model on
        training set. An exception is thrown if
        `trainingSummary is None`.","['Gets', 'summary', '(', 'e', '.', 'g', '.', 'residuals', 'deviance', 'pValues', ')', 'of', 'model', 'on', 'training', 'set', '.', 'An', 'exception', 'is', 'thrown', 'if', 'trainingSummary', 'is', 'None', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1679-L1690,train,Returns a GeneralizedLinearRegressionTrainingSummary object for this training set.
apache/spark,python/pyspark/ml/regression.py,GeneralizedLinearRegressionModel.evaluate,"def evaluate(self, dataset):
        """"""
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """"""
        if not isinstance(dataset, DataFrame):
            raise ValueError(""dataset must be a DataFrame but got %s."" % type(dataset))
        java_glr_summary = self._call_java(""evaluate"", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)",python,"def evaluate(self, dataset):
        """"""
        Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`
        """"""
        if not isinstance(dataset, DataFrame):
            raise ValueError(""dataset must be a DataFrame but got %s."" % type(dataset))
        java_glr_summary = self._call_java(""evaluate"", dataset)
        return GeneralizedLinearRegressionSummary(java_glr_summary)","['def', 'evaluate', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'DataFrame', ')', ':', 'raise', 'ValueError', '(', '""dataset must be a DataFrame but got %s.""', '%', 'type', '(', 'dataset', ')', ')', 'java_glr_summary', '=', 'self', '.', '_call_java', '(', '""evaluate""', ',', 'dataset', ')', 'return', 'GeneralizedLinearRegressionSummary', '(', 'java_glr_summary', ')']","Evaluates the model on a test dataset.

        :param dataset:
          Test dataset to evaluate model on, where dataset is an
          instance of :py:class:`pyspark.sql.DataFrame`","['Evaluates', 'the', 'model', 'on', 'a', 'test', 'dataset', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/regression.py#L1693-L1704,train,Evaluates the model on a test dataset.
apache/spark,python/pyspark/shuffle.py,_get_local_dirs,"def _get_local_dirs(sub):
    """""" Get all the directories """"""
    path = os.environ.get(""SPARK_LOCAL_DIRS"", ""/tmp"")
    dirs = path.split("","")
    if len(dirs) > 1:
        # different order in different processes and instances
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, ""python"", str(os.getpid()), sub) for d in dirs]",python,"def _get_local_dirs(sub):
    """""" Get all the directories """"""
    path = os.environ.get(""SPARK_LOCAL_DIRS"", ""/tmp"")
    dirs = path.split("","")
    if len(dirs) > 1:
        # different order in different processes and instances
        rnd = random.Random(os.getpid() + id(dirs))
        random.shuffle(dirs, rnd.random)
    return [os.path.join(d, ""python"", str(os.getpid()), sub) for d in dirs]","['def', '_get_local_dirs', '(', 'sub', ')', ':', 'path', '=', 'os', '.', 'environ', '.', 'get', '(', '""SPARK_LOCAL_DIRS""', ',', '""/tmp""', ')', 'dirs', '=', 'path', '.', 'split', '(', '"",""', ')', 'if', 'len', '(', 'dirs', ')', '>', '1', ':', '# different order in different processes and instances', 'rnd', '=', 'random', '.', 'Random', '(', 'os', '.', 'getpid', '(', ')', '+', 'id', '(', 'dirs', ')', ')', 'random', '.', 'shuffle', '(', 'dirs', ',', 'rnd', '.', 'random', ')', 'return', '[', 'os', '.', 'path', '.', 'join', '(', 'd', ',', '""python""', ',', 'str', '(', 'os', '.', 'getpid', '(', ')', ')', ',', 'sub', ')', 'for', 'd', 'in', 'dirs', ']']",Get all the directories,"['Get', 'all', 'the', 'directories']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L71-L79,train,Get all the directories that are local
apache/spark,python/pyspark/shuffle.py,ExternalMerger._get_spill_dir,"def _get_spill_dir(self, n):
        """""" Choose one directory for spill by number n """"""
        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))",python,"def _get_spill_dir(self, n):
        """""" Choose one directory for spill by number n """"""
        return os.path.join(self.localdirs[n % len(self.localdirs)], str(n))","['def', '_get_spill_dir', '(', 'self', ',', 'n', ')', ':', 'return', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'localdirs', '[', 'n', '%', 'len', '(', 'self', '.', 'localdirs', ')', ']', ',', 'str', '(', 'n', ')', ')']",Choose one directory for spill by number n,"['Choose', 'one', 'directory', 'for', 'spill', 'by', 'number', 'n']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L219-L221,train,Choose one directory for spill by number n
apache/spark,python/pyspark/shuffle.py,ExternalMerger.mergeValues,"def mergeValues(self, iterator):
        """""" Combine the items by creator and combiner """"""
        # speedup attribute lookup
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit

        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)

            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if get_used_memory() >= limit:
            self._spill()",python,"def mergeValues(self, iterator):
        """""" Combine the items by creator and combiner """"""
        # speedup attribute lookup
        creator, comb = self.agg.createCombiner, self.agg.mergeValue
        c, data, pdata, hfun, batch = 0, self.data, self.pdata, self._partition, self.batch
        limit = self.memory_limit

        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else creator(v)

            c += 1
            if c >= batch:
                if get_used_memory() >= limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if get_used_memory() >= limit:
            self._spill()","['def', 'mergeValues', '(', 'self', ',', 'iterator', ')', ':', '# speedup attribute lookup', 'creator', ',', 'comb', '=', 'self', '.', 'agg', '.', 'createCombiner', ',', 'self', '.', 'agg', '.', 'mergeValue', 'c', ',', 'data', ',', 'pdata', ',', 'hfun', ',', 'batch', '=', '0', ',', 'self', '.', 'data', ',', 'self', '.', 'pdata', ',', 'self', '.', '_partition', ',', 'self', '.', 'batch', 'limit', '=', 'self', '.', 'memory_limit', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'd', '=', 'pdata', '[', 'hfun', '(', 'k', ')', ']', 'if', 'pdata', 'else', 'data', 'd', '[', 'k', ']', '=', 'comb', '(', 'd', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'd', 'else', 'creator', '(', 'v', ')', 'c', '+=', '1', 'if', 'c', '>=', 'batch', ':', 'if', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'batch', '/=', '2', 'c', '=', '0', 'else', ':', 'batch', '*=', '1.5', 'if', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')']",Combine the items by creator and combiner,"['Combine', 'the', 'items', 'by', 'creator', 'and', 'combiner']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L231-L253,train,Combine the items by creator and combiner
apache/spark,python/pyspark/shuffle.py,ExternalMerger.mergeCombiners,"def mergeCombiners(self, iterator, limit=None):
        """""" Merge (K,V) pair by mergeCombiner """"""
        if limit is None:
            limit = self.memory_limit
        # speedup attribute lookup
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue

            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if limit and get_used_memory() >= limit:
            self._spill()",python,"def mergeCombiners(self, iterator, limit=None):
        """""" Merge (K,V) pair by mergeCombiner """"""
        if limit is None:
            limit = self.memory_limit
        # speedup attribute lookup
        comb, hfun, objsize = self.agg.mergeCombiners, self._partition, self._object_size
        c, data, pdata, batch = 0, self.data, self.pdata, self.batch
        for k, v in iterator:
            d = pdata[hfun(k)] if pdata else data
            d[k] = comb(d[k], v) if k in d else v
            if not limit:
                continue

            c += objsize(v)
            if c > batch:
                if get_used_memory() > limit:
                    self._spill()
                    limit = self._next_limit()
                    batch /= 2
                    c = 0
                else:
                    batch *= 1.5

        if limit and get_used_memory() >= limit:
            self._spill()","['def', 'mergeCombiners', '(', 'self', ',', 'iterator', ',', 'limit', '=', 'None', ')', ':', 'if', 'limit', 'is', 'None', ':', 'limit', '=', 'self', '.', 'memory_limit', '# speedup attribute lookup', 'comb', ',', 'hfun', ',', 'objsize', '=', 'self', '.', 'agg', '.', 'mergeCombiners', ',', 'self', '.', '_partition', ',', 'self', '.', '_object_size', 'c', ',', 'data', ',', 'pdata', ',', 'batch', '=', '0', ',', 'self', '.', 'data', ',', 'self', '.', 'pdata', ',', 'self', '.', 'batch', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'd', '=', 'pdata', '[', 'hfun', '(', 'k', ')', ']', 'if', 'pdata', 'else', 'data', 'd', '[', 'k', ']', '=', 'comb', '(', 'd', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'd', 'else', 'v', 'if', 'not', 'limit', ':', 'continue', 'c', '+=', 'objsize', '(', 'v', ')', 'if', 'c', '>', 'batch', ':', 'if', 'get_used_memory', '(', ')', '>', 'limit', ':', 'self', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'batch', '/=', '2', 'c', '=', '0', 'else', ':', 'batch', '*=', '1.5', 'if', 'limit', 'and', 'get_used_memory', '(', ')', '>=', 'limit', ':', 'self', '.', '_spill', '(', ')']","Merge (K,V) pair by mergeCombiner","['Merge', '(', 'K', 'V', ')', 'pair', 'by', 'mergeCombiner']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L265-L289,train,Merge a set of keys and values by merging them into a single object.
apache/spark,python/pyspark/shuffle.py,ExternalMerger._spill,"def _spill(self):
        """"""
        dump already partitioned data into disks.

        It will dump the data in batch for better performance.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # dataset once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            for k, v in self.data.items():
                h = self._partition(k)
                # put one item in batch, make it compatible with load_stream
                # it will increase the memory if dump them in batch
                self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, ""wb"") as f:
                    # dump items in batch
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20",python,"def _spill(self):
        """"""
        dump already partitioned data into disks.

        It will dump the data in batch for better performance.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # dataset once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            for k, v in self.data.items():
                h = self._partition(k)
                # put one item in batch, make it compatible with load_stream
                # it will increase the memory if dump them in batch
                self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, ""wb"") as f:
                    # dump items in batch
                    self.serializer.dump_stream(iter(self.pdata[i].items()), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20","['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'self', '.', 'spills', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'path', ')', ':', 'os', '.', 'makedirs', '(', 'path', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'not', 'self', '.', 'pdata', ':', '# The data has not been partitioned, it will iterator the', '# dataset once, write them into different files, has no', '# additional memory. It only called when the memory goes', '# above limit at the first time.', '# open all the files for writing', 'streams', '=', '[', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ',', ""'wb'"", ')', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', 'for', 'k', ',', 'v', 'in', 'self', '.', 'data', '.', 'items', '(', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', '# put one item in batch, make it compatible with load_stream', '# it will increase the memory if dump them in batch', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'v', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'for', 's', 'in', 'streams', ':', 'DiskBytesSpilled', '+=', 's', '.', 'tell', '(', ')', 's', '.', 'close', '(', ')', 'self', '.', 'data', '.', 'clear', '(', ')', 'self', '.', 'pdata', '.', 'extend', '(', '[', '{', '}', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', 'with', 'open', '(', 'p', ',', '""wb""', ')', 'as', 'f', ':', '# dump items in batch', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'iter', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ')', ',', 'f', ')', 'self', '.', 'pdata', '[', 'i', ']', '.', 'clear', '(', ')', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'p', ')', 'self', '.', 'spills', '+=', '1', 'gc', '.', 'collect', '(', ')', '# release the memory as much as possible', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']","dump already partitioned data into disks.

        It will dump the data in batch for better performance.","['dump', 'already', 'partitioned', 'data', 'into', 'disks', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L291-L337,train,This function will dump already partitioned data into disks. It will dump the data into the disks and the memory used by the memory.
apache/spark,python/pyspark/shuffle.py,ExternalMerger.items,"def items(self):
        """""" Return all merged items as iterator """"""
        if not self.pdata and not self.spills:
            return iter(self.data.items())
        return self._external_items()",python,"def items(self):
        """""" Return all merged items as iterator """"""
        if not self.pdata and not self.spills:
            return iter(self.data.items())
        return self._external_items()","['def', 'items', '(', 'self', ')', ':', 'if', 'not', 'self', '.', 'pdata', 'and', 'not', 'self', '.', 'spills', ':', 'return', 'iter', '(', 'self', '.', 'data', '.', 'items', '(', ')', ')', 'return', 'self', '.', '_external_items', '(', ')']",Return all merged items as iterator,"['Return', 'all', 'merged', 'items', 'as', 'iterator']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L339-L343,train,Return all items as iterator
apache/spark,python/pyspark/shuffle.py,ExternalMerger._external_items,"def _external_items(self):
        """""" Return all partitioned items as iterator """"""
        assert not self.data
        if any(self.pdata):
            self._spill()
        # disable partitioning and spilling when merge combiners from disk
        self.pdata = []

        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()

                # remove the merged partition
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()",python,"def _external_items(self):
        """""" Return all partitioned items as iterator """"""
        assert not self.data
        if any(self.pdata):
            self._spill()
        # disable partitioning and spilling when merge combiners from disk
        self.pdata = []

        try:
            for i in range(self.partitions):
                for v in self._merged_items(i):
                    yield v
                self.data.clear()

                # remove the merged partition
                for j in range(self.spills):
                    path = self._get_spill_dir(j)
                    os.remove(os.path.join(path, str(i)))
        finally:
            self._cleanup()","['def', '_external_items', '(', 'self', ')', ':', 'assert', 'not', 'self', '.', 'data', 'if', 'any', '(', 'self', '.', 'pdata', ')', ':', 'self', '.', '_spill', '(', ')', '# disable partitioning and spilling when merge combiners from disk', 'self', '.', 'pdata', '=', '[', ']', 'try', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'for', 'v', 'in', 'self', '.', '_merged_items', '(', 'i', ')', ':', 'yield', 'v', 'self', '.', 'data', '.', 'clear', '(', ')', '# remove the merged partition', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'os', '.', 'remove', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ')', 'finally', ':', 'self', '.', '_cleanup', '(', ')']",Return all partitioned items as iterator,"['Return', 'all', 'partitioned', 'items', 'as', 'iterator']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L345-L364,train,Return all partitioned items as iterator
apache/spark,python/pyspark/shuffle.py,ExternalMerger._recursive_merged_items,"def _recursive_merged_items(self, index):
        """"""
        merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
        """"""
        subdirs = [os.path.join(d, ""parts"", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()

        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)

            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()

        return m._external_items()",python,"def _recursive_merged_items(self, index):
        """"""
        merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.
        """"""
        subdirs = [os.path.join(d, ""parts"", str(index)) for d in self.localdirs]
        m = ExternalMerger(self.agg, self.memory_limit, self.serializer, subdirs,
                           self.scale * self.partitions, self.partitions, self.batch)
        m.pdata = [{} for _ in range(self.partitions)]
        limit = self._next_limit()

        for j in range(self.spills):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb') as f:
                m.mergeCombiners(self.serializer.load_stream(f), 0)

            if get_used_memory() > limit:
                m._spill()
                limit = self._next_limit()

        return m._external_items()","['def', '_recursive_merged_items', '(', 'self', ',', 'index', ')', ':', 'subdirs', '=', '[', 'os', '.', 'path', '.', 'join', '(', 'd', ',', '""parts""', ',', 'str', '(', 'index', ')', ')', 'for', 'd', 'in', 'self', '.', 'localdirs', ']', 'm', '=', 'ExternalMerger', '(', 'self', '.', 'agg', ',', 'self', '.', 'memory_limit', ',', 'self', '.', 'serializer', ',', 'subdirs', ',', 'self', '.', 'scale', '*', 'self', '.', 'partitions', ',', 'self', '.', 'partitions', ',', 'self', '.', 'batch', ')', 'm', '.', 'pdata', '=', '[', '{', '}', 'for', '_', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'index', ')', ')', 'with', 'open', '(', 'p', ',', ""'rb'"", ')', 'as', 'f', ':', 'm', '.', 'mergeCombiners', '(', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ',', '0', ')', 'if', 'get_used_memory', '(', ')', '>', 'limit', ':', 'm', '.', '_spill', '(', ')', 'limit', '=', 'self', '.', '_next_limit', '(', ')', 'return', 'm', '.', '_external_items', '(', ')']","merge the partitioned items and return the as iterator

        If one partition can not be fit in memory, then them will be
        partitioned and merged recursively.","['merge', 'the', 'partitioned', 'items', 'and', 'return', 'the', 'as', 'iterator']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L386-L409,train,"Merge the partitioned items and return the as iterator
       "
apache/spark,python/pyspark/shuffle.py,ExternalSorter._get_path,"def _get_path(self, n):
        """""" Choose one directory for spill by number n """"""
        d = self.local_dirs[n % len(self.local_dirs)]
        if not os.path.exists(d):
            os.makedirs(d)
        return os.path.join(d, str(n))",python,"def _get_path(self, n):
        """""" Choose one directory for spill by number n """"""
        d = self.local_dirs[n % len(self.local_dirs)]
        if not os.path.exists(d):
            os.makedirs(d)
        return os.path.join(d, str(n))","['def', '_get_path', '(', 'self', ',', 'n', ')', ':', 'd', '=', 'self', '.', 'local_dirs', '[', 'n', '%', 'len', '(', 'self', '.', 'local_dirs', ')', ']', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'd', ')', ':', 'os', '.', 'makedirs', '(', 'd', ')', 'return', 'os', '.', 'path', '.', 'join', '(', 'd', ',', 'str', '(', 'n', ')', ')']",Choose one directory for spill by number n,"['Choose', 'one', 'directory', 'for', 'spill', 'by', 'number', 'n']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L440-L445,train,Choose one directory for spill by number n
apache/spark,python/pyspark/shuffle.py,ExternalSorter.sorted,"def sorted(self, iterator, key=None, reverse=False):
        """"""
        Sort the elements in iterator, do external sort when the memory
        goes above the limit.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            # pick elements in batch
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break

            used_memory = get_used_memory()
            if used_memory > limit:
                # sort them inplace will save memory
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)

                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    # close the file explicit once we consume all the items
                    # to avoid ResourceWarning in Python3
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  # data will be deleted after close

            elif not chunks:
                batch = min(int(batch * 1.5), 10000)

        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk

        if current_chunk:
            chunks.append(iter(current_chunk))

        return heapq.merge(chunks, key=key, reverse=reverse)",python,"def sorted(self, iterator, key=None, reverse=False):
        """"""
        Sort the elements in iterator, do external sort when the memory
        goes above the limit.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        batch, limit = 100, self._next_limit()
        chunks, current_chunk = [], []
        iterator = iter(iterator)
        while True:
            # pick elements in batch
            chunk = list(itertools.islice(iterator, batch))
            current_chunk.extend(chunk)
            if len(chunk) < batch:
                break

            used_memory = get_used_memory()
            if used_memory > limit:
                # sort them inplace will save memory
                current_chunk.sort(key=key, reverse=reverse)
                path = self._get_path(len(chunks))
                with open(path, 'wb') as f:
                    self.serializer.dump_stream(current_chunk, f)

                def load(f):
                    for v in self.serializer.load_stream(f):
                        yield v
                    # close the file explicit once we consume all the items
                    # to avoid ResourceWarning in Python3
                    f.close()
                chunks.append(load(open(path, 'rb')))
                current_chunk = []
                MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20
                DiskBytesSpilled += os.path.getsize(path)
                os.unlink(path)  # data will be deleted after close

            elif not chunks:
                batch = min(int(batch * 1.5), 10000)

        current_chunk.sort(key=key, reverse=reverse)
        if not chunks:
            return current_chunk

        if current_chunk:
            chunks.append(iter(current_chunk))

        return heapq.merge(chunks, key=key, reverse=reverse)","['def', 'sorted', '(', 'self', ',', 'iterator', ',', 'key', '=', 'None', ',', 'reverse', '=', 'False', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'batch', ',', 'limit', '=', '100', ',', 'self', '.', '_next_limit', '(', ')', 'chunks', ',', 'current_chunk', '=', '[', ']', ',', '[', ']', 'iterator', '=', 'iter', '(', 'iterator', ')', 'while', 'True', ':', '# pick elements in batch', 'chunk', '=', 'list', '(', 'itertools', '.', 'islice', '(', 'iterator', ',', 'batch', ')', ')', 'current_chunk', '.', 'extend', '(', 'chunk', ')', 'if', 'len', '(', 'chunk', ')', '<', 'batch', ':', 'break', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'used_memory', '>', 'limit', ':', '# sort them inplace will save memory', 'current_chunk', '.', 'sort', '(', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')', 'path', '=', 'self', '.', '_get_path', '(', 'len', '(', 'chunks', ')', ')', 'with', 'open', '(', 'path', ',', ""'wb'"", ')', 'as', 'f', ':', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'current_chunk', ',', 'f', ')', 'def', 'load', '(', 'f', ')', ':', 'for', 'v', 'in', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ':', 'yield', 'v', '# close the file explicit once we consume all the items', '# to avoid ResourceWarning in Python3', 'f', '.', 'close', '(', ')', 'chunks', '.', 'append', '(', 'load', '(', 'open', '(', 'path', ',', ""'rb'"", ')', ')', ')', 'current_chunk', '=', '[', ']', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'path', ')', 'os', '.', 'unlink', '(', 'path', ')', '# data will be deleted after close', 'elif', 'not', 'chunks', ':', 'batch', '=', 'min', '(', 'int', '(', 'batch', '*', '1.5', ')', ',', '10000', ')', 'current_chunk', '.', 'sort', '(', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')', 'if', 'not', 'chunks', ':', 'return', 'current_chunk', 'if', 'current_chunk', ':', 'chunks', '.', 'append', '(', 'iter', '(', 'current_chunk', ')', ')', 'return', 'heapq', '.', 'merge', '(', 'chunks', ',', 'key', '=', 'key', ',', 'reverse', '=', 'reverse', ')']","Sort the elements in iterator, do external sort when the memory
        goes above the limit.","['Sort', 'the', 'elements', 'in', 'iterator', 'do', 'external', 'sort', 'when', 'the', 'memory', 'goes', 'above', 'the', 'limit', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L455-L501,train,Sort the elements in iterator do external sort when the memory is below the limit.
apache/spark,python/pyspark/shuffle.py,ExternalList._spill,"def _spill(self):
        """""" dump the values into disk """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()

        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20",python,"def _spill(self):
        """""" dump the values into disk """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        if self._file is None:
            self._open_file()

        used_memory = get_used_memory()
        pos = self._file.tell()
        self._ser.dump_stream(self.values, self._file)
        self.values = []
        gc.collect()
        DiskBytesSpilled += self._file.tell() - pos
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20","['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'if', 'self', '.', '_file', 'is', 'None', ':', 'self', '.', '_open_file', '(', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'pos', '=', 'self', '.', '_file', '.', 'tell', '(', ')', 'self', '.', '_ser', '.', 'dump_stream', '(', 'self', '.', 'values', ',', 'self', '.', '_file', ')', 'self', '.', 'values', '=', '[', ']', 'gc', '.', 'collect', '(', ')', 'DiskBytesSpilled', '+=', 'self', '.', '_file', '.', 'tell', '(', ')', '-', 'pos', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']",dump the values into disk,"['dump', 'the', 'values', 'into', 'disk']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L590-L602,train,dump the values into disk
apache/spark,python/pyspark/shuffle.py,ExternalGroupBy._spill,"def _spill(self):
        """"""
        dump already partitioned data into disks.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # data once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            # If the number of keys is small, then the overhead of sort is small
            # sort them before dumping into disks
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            # self.pdata is cached in `mergeValues` and `mergeCombiners`
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, ""wb"") as f:
                    # dump items in batch
                    if self._sorted:
                        # sort by key only (stable)
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20",python,"def _spill(self):
        """"""
        dump already partitioned data into disks.
        """"""
        global MemoryBytesSpilled, DiskBytesSpilled
        path = self._get_spill_dir(self.spills)
        if not os.path.exists(path):
            os.makedirs(path)

        used_memory = get_used_memory()
        if not self.pdata:
            # The data has not been partitioned, it will iterator the
            # data once, write them into different files, has no
            # additional memory. It only called when the memory goes
            # above limit at the first time.

            # open all the files for writing
            streams = [open(os.path.join(path, str(i)), 'wb')
                       for i in range(self.partitions)]

            # If the number of keys is small, then the overhead of sort is small
            # sort them before dumping into disks
            self._sorted = len(self.data) < self.SORT_KEY_LIMIT
            if self._sorted:
                self.serializer = self.flattened_serializer()
                for k in sorted(self.data.keys()):
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, self.data[k])], streams[h])
            else:
                for k, v in self.data.items():
                    h = self._partition(k)
                    self.serializer.dump_stream([(k, v)], streams[h])

            for s in streams:
                DiskBytesSpilled += s.tell()
                s.close()

            self.data.clear()
            # self.pdata is cached in `mergeValues` and `mergeCombiners`
            self.pdata.extend([{} for i in range(self.partitions)])

        else:
            for i in range(self.partitions):
                p = os.path.join(path, str(i))
                with open(p, ""wb"") as f:
                    # dump items in batch
                    if self._sorted:
                        # sort by key only (stable)
                        sorted_items = sorted(self.pdata[i].items(), key=operator.itemgetter(0))
                        self.serializer.dump_stream(sorted_items, f)
                    else:
                        self.serializer.dump_stream(self.pdata[i].items(), f)
                self.pdata[i].clear()
                DiskBytesSpilled += os.path.getsize(p)

        self.spills += 1
        gc.collect()  # release the memory as much as possible
        MemoryBytesSpilled += max(used_memory - get_used_memory(), 0) << 20","['def', '_spill', '(', 'self', ')', ':', 'global', 'MemoryBytesSpilled', ',', 'DiskBytesSpilled', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'self', '.', 'spills', ')', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'path', ')', ':', 'os', '.', 'makedirs', '(', 'path', ')', 'used_memory', '=', 'get_used_memory', '(', ')', 'if', 'not', 'self', '.', 'pdata', ':', '# The data has not been partitioned, it will iterator the', '# data once, write them into different files, has no', '# additional memory. It only called when the memory goes', '# above limit at the first time.', '# open all the files for writing', 'streams', '=', '[', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', ',', ""'wb'"", ')', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', '# If the number of keys is small, then the overhead of sort is small', '# sort them before dumping into disks', 'self', '.', '_sorted', '=', 'len', '(', 'self', '.', 'data', ')', '<', 'self', '.', 'SORT_KEY_LIMIT', 'if', 'self', '.', '_sorted', ':', 'self', '.', 'serializer', '=', 'self', '.', 'flattened_serializer', '(', ')', 'for', 'k', 'in', 'sorted', '(', 'self', '.', 'data', '.', 'keys', '(', ')', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'self', '.', 'data', '[', 'k', ']', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'else', ':', 'for', 'k', ',', 'v', 'in', 'self', '.', 'data', '.', 'items', '(', ')', ':', 'h', '=', 'self', '.', '_partition', '(', 'k', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', '[', '(', 'k', ',', 'v', ')', ']', ',', 'streams', '[', 'h', ']', ')', 'for', 's', 'in', 'streams', ':', 'DiskBytesSpilled', '+=', 's', '.', 'tell', '(', ')', 's', '.', 'close', '(', ')', 'self', '.', 'data', '.', 'clear', '(', ')', '# self.pdata is cached in `mergeValues` and `mergeCombiners`', 'self', '.', 'pdata', '.', 'extend', '(', '[', '{', '}', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ']', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', 'self', '.', 'partitions', ')', ':', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'i', ')', ')', 'with', 'open', '(', 'p', ',', '""wb""', ')', 'as', 'f', ':', '# dump items in batch', 'if', 'self', '.', '_sorted', ':', '# sort by key only (stable)', 'sorted_items', '=', 'sorted', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'sorted_items', ',', 'f', ')', 'else', ':', 'self', '.', 'serializer', '.', 'dump_stream', '(', 'self', '.', 'pdata', '[', 'i', ']', '.', 'items', '(', ')', ',', 'f', ')', 'self', '.', 'pdata', '[', 'i', ']', '.', 'clear', '(', ')', 'DiskBytesSpilled', '+=', 'os', '.', 'path', '.', 'getsize', '(', 'p', ')', 'self', '.', 'spills', '+=', '1', 'gc', '.', 'collect', '(', ')', '# release the memory as much as possible', 'MemoryBytesSpilled', '+=', 'max', '(', 'used_memory', '-', 'get_used_memory', '(', ')', ',', '0', ')', '<<', '20']",dump already partitioned data into disks.,"['dump', 'already', 'partitioned', 'data', 'into', 'disks', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L709-L766,train,Dump already partitioned data into disks.
apache/spark,python/pyspark/shuffle.py,ExternalGroupBy._merge_sorted_items,"def _merge_sorted_items(self, index):
        """""" load a partition from disk, then sort and group by key """"""
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v

        disk_items = [load_partition(j) for j in range(self.spills)]

        if self._sorted:
            # all the partitions are already sorted
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))

        else:
            # Flatten the combined values, so it will not consume huge
            # memory during merging sort.
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))",python,"def _merge_sorted_items(self, index):
        """""" load a partition from disk, then sort and group by key """"""
        def load_partition(j):
            path = self._get_spill_dir(j)
            p = os.path.join(path, str(index))
            with open(p, 'rb', 65536) as f:
                for v in self.serializer.load_stream(f):
                    yield v

        disk_items = [load_partition(j) for j in range(self.spills)]

        if self._sorted:
            # all the partitions are already sorted
            sorted_items = heapq.merge(disk_items, key=operator.itemgetter(0))

        else:
            # Flatten the combined values, so it will not consume huge
            # memory during merging sort.
            ser = self.flattened_serializer()
            sorter = ExternalSorter(self.memory_limit, ser)
            sorted_items = sorter.sorted(itertools.chain(*disk_items),
                                         key=operator.itemgetter(0))
        return ((k, vs) for k, vs in GroupByKey(sorted_items))","['def', '_merge_sorted_items', '(', 'self', ',', 'index', ')', ':', 'def', 'load_partition', '(', 'j', ')', ':', 'path', '=', 'self', '.', '_get_spill_dir', '(', 'j', ')', 'p', '=', 'os', '.', 'path', '.', 'join', '(', 'path', ',', 'str', '(', 'index', ')', ')', 'with', 'open', '(', 'p', ',', ""'rb'"", ',', '65536', ')', 'as', 'f', ':', 'for', 'v', 'in', 'self', '.', 'serializer', '.', 'load_stream', '(', 'f', ')', ':', 'yield', 'v', 'disk_items', '=', '[', 'load_partition', '(', 'j', ')', 'for', 'j', 'in', 'range', '(', 'self', '.', 'spills', ')', ']', 'if', 'self', '.', '_sorted', ':', '# all the partitions are already sorted', 'sorted_items', '=', 'heapq', '.', 'merge', '(', 'disk_items', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'else', ':', '# Flatten the combined values, so it will not consume huge', '# memory during merging sort.', 'ser', '=', 'self', '.', 'flattened_serializer', '(', ')', 'sorter', '=', 'ExternalSorter', '(', 'self', '.', 'memory_limit', ',', 'ser', ')', 'sorted_items', '=', 'sorter', '.', 'sorted', '(', 'itertools', '.', 'chain', '(', '*', 'disk_items', ')', ',', 'key', '=', 'operator', '.', 'itemgetter', '(', '0', ')', ')', 'return', '(', '(', 'k', ',', 'vs', ')', 'for', 'k', ',', 'vs', 'in', 'GroupByKey', '(', 'sorted_items', ')', ')']","load a partition from disk, then sort and group by key","['load', 'a', 'partition', 'from', 'disk', 'then', 'sort', 'and', 'group', 'by', 'key']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/shuffle.py#L786-L808,train,Load a partition from disk then sort and group by key
apache/spark,python/pyspark/daemon.py,worker,"def worker(sock, authenticated):
    """"""
    Called by a worker process after the fork().
    """"""
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    # restore the handler for SIGINT,
    # it's useful for debugging (show the stacktrace before exit)
    signal.signal(SIGINT, signal.default_int_handler)

    # Read the socket using fdopen instead of socket.makefile() because the latter
    # seems to be very slow; note that we need to dup() the file descriptor because
    # otherwise writes also cause a seek that makes us miss data on the read side.
    infile = os.fdopen(os.dup(sock.fileno()), ""rb"", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), ""wb"", 65536)

    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ[""PYTHON_WORKER_FACTORY_SECRET""] == client_secret:
            write_with_length(""ok"".encode(""utf-8""), outfile)
            outfile.flush()
        else:
            write_with_length(""err"".encode(""utf-8""), outfile)
            outfile.flush()
            sock.close()
            return 1

    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code",python,"def worker(sock, authenticated):
    """"""
    Called by a worker process after the fork().
    """"""
    signal.signal(SIGHUP, SIG_DFL)
    signal.signal(SIGCHLD, SIG_DFL)
    signal.signal(SIGTERM, SIG_DFL)
    # restore the handler for SIGINT,
    # it's useful for debugging (show the stacktrace before exit)
    signal.signal(SIGINT, signal.default_int_handler)

    # Read the socket using fdopen instead of socket.makefile() because the latter
    # seems to be very slow; note that we need to dup() the file descriptor because
    # otherwise writes also cause a seek that makes us miss data on the read side.
    infile = os.fdopen(os.dup(sock.fileno()), ""rb"", 65536)
    outfile = os.fdopen(os.dup(sock.fileno()), ""wb"", 65536)

    if not authenticated:
        client_secret = UTF8Deserializer().loads(infile)
        if os.environ[""PYTHON_WORKER_FACTORY_SECRET""] == client_secret:
            write_with_length(""ok"".encode(""utf-8""), outfile)
            outfile.flush()
        else:
            write_with_length(""err"".encode(""utf-8""), outfile)
            outfile.flush()
            sock.close()
            return 1

    exit_code = 0
    try:
        worker_main(infile, outfile)
    except SystemExit as exc:
        exit_code = compute_real_exit_code(exc.code)
    finally:
        try:
            outfile.flush()
        except Exception:
            pass
    return exit_code","['def', 'worker', '(', 'sock', ',', 'authenticated', ')', ':', 'signal', '.', 'signal', '(', 'SIGHUP', ',', 'SIG_DFL', ')', 'signal', '.', 'signal', '(', 'SIGCHLD', ',', 'SIG_DFL', ')', 'signal', '.', 'signal', '(', 'SIGTERM', ',', 'SIG_DFL', ')', '# restore the handler for SIGINT,', ""# it's useful for debugging (show the stacktrace before exit)"", 'signal', '.', 'signal', '(', 'SIGINT', ',', 'signal', '.', 'default_int_handler', ')', '# Read the socket using fdopen instead of socket.makefile() because the latter', '# seems to be very slow; note that we need to dup() the file descriptor because', '# otherwise writes also cause a seek that makes us miss data on the read side.', 'infile', '=', 'os', '.', 'fdopen', '(', 'os', '.', 'dup', '(', 'sock', '.', 'fileno', '(', ')', ')', ',', '""rb""', ',', '65536', ')', 'outfile', '=', 'os', '.', 'fdopen', '(', 'os', '.', 'dup', '(', 'sock', '.', 'fileno', '(', ')', ')', ',', '""wb""', ',', '65536', ')', 'if', 'not', 'authenticated', ':', 'client_secret', '=', 'UTF8Deserializer', '(', ')', '.', 'loads', '(', 'infile', ')', 'if', 'os', '.', 'environ', '[', '""PYTHON_WORKER_FACTORY_SECRET""', ']', '==', 'client_secret', ':', 'write_with_length', '(', '""ok""', '.', 'encode', '(', '""utf-8""', ')', ',', 'outfile', ')', 'outfile', '.', 'flush', '(', ')', 'else', ':', 'write_with_length', '(', '""err""', '.', 'encode', '(', '""utf-8""', ')', ',', 'outfile', ')', 'outfile', '.', 'flush', '(', ')', 'sock', '.', 'close', '(', ')', 'return', '1', 'exit_code', '=', '0', 'try', ':', 'worker_main', '(', 'infile', ',', 'outfile', ')', 'except', 'SystemExit', 'as', 'exc', ':', 'exit_code', '=', 'compute_real_exit_code', '(', 'exc', '.', 'code', ')', 'finally', ':', 'try', ':', 'outfile', '.', 'flush', '(', ')', 'except', 'Exception', ':', 'pass', 'return', 'exit_code']",Called by a worker process after the fork().,"['Called', 'by', 'a', 'worker', 'process', 'after', 'the', 'fork', '()', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/daemon.py#L43-L81,train,This function is called by the worker process.
apache/spark,python/pyspark/rdd.py,portable_hash,"def portable_hash(x):
    """"""
    This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
    """"""

    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception(""Randomness of hash of string should be disabled via PYTHONHASHSEED"")

    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)",python,"def portable_hash(x):
    """"""
    This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521
    """"""

    if sys.version_info >= (3, 2, 3) and 'PYTHONHASHSEED' not in os.environ:
        raise Exception(""Randomness of hash of string should be disabled via PYTHONHASHSEED"")

    if x is None:
        return 0
    if isinstance(x, tuple):
        h = 0x345678
        for i in x:
            h ^= portable_hash(i)
            h *= 1000003
            h &= sys.maxsize
        h ^= len(x)
        if h == -1:
            h = -2
        return int(h)
    return hash(x)","['def', 'portable_hash', '(', 'x', ')', ':', 'if', 'sys', '.', 'version_info', '>=', '(', '3', ',', '2', ',', '3', ')', 'and', ""'PYTHONHASHSEED'"", 'not', 'in', 'os', '.', 'environ', ':', 'raise', 'Exception', '(', '""Randomness of hash of string should be disabled via PYTHONHASHSEED""', ')', 'if', 'x', 'is', 'None', ':', 'return', '0', 'if', 'isinstance', '(', 'x', ',', 'tuple', ')', ':', 'h', '=', '0x345678', 'for', 'i', 'in', 'x', ':', 'h', '^=', 'portable_hash', '(', 'i', ')', 'h', '*=', '1000003', 'h', '&=', 'sys', '.', 'maxsize', 'h', '^=', 'len', '(', 'x', ')', 'if', 'h', '==', '-', '1', ':', 'h', '=', '-', '2', 'return', 'int', '(', 'h', ')', 'return', 'hash', '(', 'x', ')']","This function returns consistent hash code for builtin types, especially
    for None and tuple with None.

    The algorithm is similar to that one used by CPython 2.7

    >>> portable_hash(None)
    0
    >>> portable_hash((None, 1)) & 0xffffffff
    219750521","['This', 'function', 'returns', 'consistent', 'hash', 'code', 'for', 'builtin', 'types', 'especially', 'for', 'None', 'and', 'tuple', 'with', 'None', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L78-L106,train,This function returns consistent hash code for builtin types and tuple with None.
apache/spark,python/pyspark/rdd.py,_parse_memory,"def _parse_memory(s):
    """"""
    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory(""256m"")
    256
    >>> _parse_memory(""2g"")
    2048
    """"""
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError(""invalid format: "" + s)
    return int(float(s[:-1]) * units[s[-1].lower()])",python,"def _parse_memory(s):
    """"""
    Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory(""256m"")
    256
    >>> _parse_memory(""2g"")
    2048
    """"""
    units = {'g': 1024, 'm': 1, 't': 1 << 20, 'k': 1.0 / 1024}
    if s[-1].lower() not in units:
        raise ValueError(""invalid format: "" + s)
    return int(float(s[:-1]) * units[s[-1].lower()])","['def', '_parse_memory', '(', 's', ')', ':', 'units', '=', '{', ""'g'"", ':', '1024', ',', ""'m'"", ':', '1', ',', ""'t'"", ':', '1', '<<', '20', ',', ""'k'"", ':', '1.0', '/', '1024', '}', 'if', 's', '[', '-', '1', ']', '.', 'lower', '(', ')', 'not', 'in', 'units', ':', 'raise', 'ValueError', '(', '""invalid format: ""', '+', 's', ')', 'return', 'int', '(', 'float', '(', 's', '[', ':', '-', '1', ']', ')', '*', 'units', '[', 's', '[', '-', '1', ']', '.', 'lower', '(', ')', ']', ')']","Parse a memory string in the format supported by Java (e.g. 1g, 200m) and
    return the value in MiB

    >>> _parse_memory(""256m"")
    256
    >>> _parse_memory(""2g"")
    2048","['Parse', 'a', 'memory', 'string', 'in', 'the', 'format', 'supported', 'by', 'Java', '(', 'e', '.', 'g', '.', '1g', '200m', ')', 'and', 'return', 'the', 'value', 'in', 'MiB']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L125-L138,train,Parse a memory string in the format supported by Java and return the value in MiB.
apache/spark,python/pyspark/rdd.py,ignore_unicode_prefix,"def ignore_unicode_prefix(f):
    """"""
    Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
    """"""
    if sys.version >= '3':
        # the representation of unicode string in Python 3 does not have prefix 'u',
        # so remove the prefix 'u' for doc tests
        literal_re = re.compile(r""(\W|^)[uU](['])"", re.UNICODE)
        f.__doc__ = literal_re.sub(r'\1\2', f.__doc__)
    return f",python,"def ignore_unicode_prefix(f):
    """"""
    Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3
    """"""
    if sys.version >= '3':
        # the representation of unicode string in Python 3 does not have prefix 'u',
        # so remove the prefix 'u' for doc tests
        literal_re = re.compile(r""(\W|^)[uU](['])"", re.UNICODE)
        f.__doc__ = literal_re.sub(r'\1\2', f.__doc__)
    return f","['def', 'ignore_unicode_prefix', '(', 'f', ')', ':', 'if', 'sys', '.', 'version', '>=', ""'3'"", ':', ""# the representation of unicode string in Python 3 does not have prefix 'u',"", ""# so remove the prefix 'u' for doc tests"", 'literal_re', '=', 're', '.', 'compile', '(', 'r""(\\W|^)[uU]([\'])""', ',', 're', '.', 'UNICODE', ')', 'f', '.', '__doc__', '=', 'literal_re', '.', 'sub', '(', ""r'\\1\\2'"", ',', 'f', '.', '__doc__', ')', 'return', 'f']","Ignore the 'u' prefix of string in doc tests, to make it works
    in both python 2 and 3","['Ignore', 'the', 'u', 'prefix', 'of', 'string', 'in', 'doc', 'tests', 'to', 'make', 'it', 'works', 'in', 'both', 'python', '2', 'and', '3']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L150-L160,train,Ignore the u prefix of string in doc tests
apache/spark,python/pyspark/rdd.py,RDD.cache,"def cache(self):
        """"""
        Persist this RDD with the default storage level (C{MEMORY_ONLY}).
        """"""
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self",python,"def cache(self):
        """"""
        Persist this RDD with the default storage level (C{MEMORY_ONLY}).
        """"""
        self.is_cached = True
        self.persist(StorageLevel.MEMORY_ONLY)
        return self","['def', 'cache', '(', 'self', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'self', '.', 'persist', '(', 'StorageLevel', '.', 'MEMORY_ONLY', ')', 'return', 'self']",Persist this RDD with the default storage level (C{MEMORY_ONLY}).,"['Persist', 'this', 'RDD', 'with', 'the', 'default', 'storage', 'level', '(', 'C', '{', 'MEMORY_ONLY', '}', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L223-L229,train,Persist this RDD with the default storage level.
apache/spark,python/pyspark/rdd.py,RDD.persist,"def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):
        """"""
        Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> rdd.persist().is_cached
        True
        """"""
        self.is_cached = True
        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
        self._jrdd.persist(javaStorageLevel)
        return self",python,"def persist(self, storageLevel=StorageLevel.MEMORY_ONLY):
        """"""
        Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> rdd.persist().is_cached
        True
        """"""
        self.is_cached = True
        javaStorageLevel = self.ctx._getJavaStorageLevel(storageLevel)
        self._jrdd.persist(javaStorageLevel)
        return self","['def', 'persist', '(', 'self', ',', 'storageLevel', '=', 'StorageLevel', '.', 'MEMORY_ONLY', ')', ':', 'self', '.', 'is_cached', '=', 'True', 'javaStorageLevel', '=', 'self', '.', 'ctx', '.', '_getJavaStorageLevel', '(', 'storageLevel', ')', 'self', '.', '_jrdd', '.', 'persist', '(', 'javaStorageLevel', ')', 'return', 'self']","Set this RDD's storage level to persist its values across operations
        after the first time it is computed. This can only be used to assign
        a new storage level if the RDD does not have a storage level set yet.
        If no storage level is specified defaults to (C{MEMORY_ONLY}).

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> rdd.persist().is_cached
        True","['Set', 'this', 'RDD', 's', 'storage', 'level', 'to', 'persist', 'its', 'values', 'across', 'operations', 'after', 'the', 'first', 'time', 'it', 'is', 'computed', '.', 'This', 'can', 'only', 'be', 'used', 'to', 'assign', 'a', 'new', 'storage', 'level', 'if', 'the', 'RDD', 'does', 'not', 'have', 'a', 'storage', 'level', 'set', 'yet', '.', 'If', 'no', 'storage', 'level', 'is', 'specified', 'defaults', 'to', '(', 'C', '{', 'MEMORY_ONLY', '}', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L231-L245,train,"Set this RDD s storage level to persist its values across operations
       ."
apache/spark,python/pyspark/rdd.py,RDD.unpersist,"def unpersist(self, blocking=False):
        """"""
        Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
        """"""
        self.is_cached = False
        self._jrdd.unpersist(blocking)
        return self",python,"def unpersist(self, blocking=False):
        """"""
        Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.
        """"""
        self.is_cached = False
        self._jrdd.unpersist(blocking)
        return self","['def', 'unpersist', '(', 'self', ',', 'blocking', '=', 'False', ')', ':', 'self', '.', 'is_cached', '=', 'False', 'self', '.', '_jrdd', '.', 'unpersist', '(', 'blocking', ')', 'return', 'self']","Mark the RDD as non-persistent, and remove all blocks for it from
        memory and disk.

        .. versionchanged:: 3.0.0
           Added optional argument `blocking` to specify whether to block until all
           blocks are deleted.","['Mark', 'the', 'RDD', 'as', 'non', '-', 'persistent', 'and', 'remove', 'all', 'blocks', 'for', 'it', 'from', 'memory', 'and', 'disk', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L247-L258,train,Mark the RDD as non - persistent and remove all blocks for the current entry from memory and disk.
apache/spark,python/pyspark/rdd.py,RDD.getCheckpointFile,"def getCheckpointFile(self):
        """"""
        Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
        """"""
        checkpointFile = self._jrdd.rdd().getCheckpointFile()
        if checkpointFile.isDefined():
            return checkpointFile.get()",python,"def getCheckpointFile(self):
        """"""
        Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.
        """"""
        checkpointFile = self._jrdd.rdd().getCheckpointFile()
        if checkpointFile.isDefined():
            return checkpointFile.get()","['def', 'getCheckpointFile', '(', 'self', ')', ':', 'checkpointFile', '=', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', '.', 'getCheckpointFile', '(', ')', 'if', 'checkpointFile', '.', 'isDefined', '(', ')', ':', 'return', 'checkpointFile', '.', 'get', '(', ')']","Gets the name of the file to which this RDD was checkpointed

        Not defined if RDD is checkpointed locally.","['Gets', 'the', 'name', 'of', 'the', 'file', 'to', 'which', 'this', 'RDD', 'was', 'checkpointed']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L307-L315,train,Gets the name of the file to which this RDD was checkpointed.
apache/spark,python/pyspark/rdd.py,RDD.map,"def map(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
        """"""
        def func(_, iterator):
            return map(fail_on_stopiteration(f), iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)",python,"def map(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]
        """"""
        def func(_, iterator):
            return map(fail_on_stopiteration(f), iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)","['def', 'map', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', '_', ',', 'iterator', ')', ':', 'return', 'map', '(', 'fail_on_stopiteration', '(', 'f', ')', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']","Return a new RDD by applying a function to each element of this RDD.

        >>> rdd = sc.parallelize([""b"", ""a"", ""c""])
        >>> sorted(rdd.map(lambda x: (x, 1)).collect())
        [('a', 1), ('b', 1), ('c', 1)]","['Return', 'a', 'new', 'RDD', 'by', 'applying', 'a', 'function', 'to', 'each', 'element', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L317-L327,train,Return a new RDD by applying a function to each element of this RDD.
apache/spark,python/pyspark/rdd.py,RDD.flatMap,"def flatMap(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
        """"""
        def func(s, iterator):
            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))
        return self.mapPartitionsWithIndex(func, preservesPartitioning)",python,"def flatMap(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]
        """"""
        def func(s, iterator):
            return chain.from_iterable(map(fail_on_stopiteration(f), iterator))
        return self.mapPartitionsWithIndex(func, preservesPartitioning)","['def', 'flatMap', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'chain', '.', 'from_iterable', '(', 'map', '(', 'fail_on_stopiteration', '(', 'f', ')', ',', 'iterator', ')', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']","Return a new RDD by first applying a function to all elements of this
        RDD, and then flattening the results.

        >>> rdd = sc.parallelize([2, 3, 4])
        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())
        [1, 1, 1, 2, 2, 3]
        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())
        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]","['Return', 'a', 'new', 'RDD', 'by', 'first', 'applying', 'a', 'function', 'to', 'all', 'elements', 'of', 'this', 'RDD', 'and', 'then', 'flattening', 'the', 'results', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L329-L342,train,Return a new RDD by first applying a function to all elements of this RDD and then flattening the results.
apache/spark,python/pyspark/rdd.py,RDD.mapPartitions,"def mapPartitions(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
        """"""
        def func(s, iterator):
            return f(iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)",python,"def mapPartitions(self, f, preservesPartitioning=False):
        """"""
        Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]
        """"""
        def func(s, iterator):
            return f(iterator)
        return self.mapPartitionsWithIndex(func, preservesPartitioning)","['def', 'mapPartitions', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'f', '(', 'iterator', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ',', 'preservesPartitioning', ')']","Return a new RDD by applying a function to each partition of this RDD.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)
        >>> def f(iterator): yield sum(iterator)
        >>> rdd.mapPartitions(f).collect()
        [3, 7]","['Return', 'a', 'new', 'RDD', 'by', 'applying', 'a', 'function', 'to', 'each', 'partition', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L344-L355,train,Return a new RDD by applying a function to each partition of this RDD.
apache/spark,python/pyspark/rdd.py,RDD.mapPartitionsWithSplit,"def mapPartitionsWithSplit(self, f, preservesPartitioning=False):
        """"""
        Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
        """"""
        warnings.warn(""mapPartitionsWithSplit is deprecated; ""
                      ""use mapPartitionsWithIndex instead"", DeprecationWarning, stacklevel=2)
        return self.mapPartitionsWithIndex(f, preservesPartitioning)",python,"def mapPartitionsWithSplit(self, f, preservesPartitioning=False):
        """"""
        Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6
        """"""
        warnings.warn(""mapPartitionsWithSplit is deprecated; ""
                      ""use mapPartitionsWithIndex instead"", DeprecationWarning, stacklevel=2)
        return self.mapPartitionsWithIndex(f, preservesPartitioning)","['def', 'mapPartitionsWithSplit', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'warnings', '.', 'warn', '(', '""mapPartitionsWithSplit is deprecated; ""', '""use mapPartitionsWithIndex instead""', ',', 'DeprecationWarning', ',', 'stacklevel', '=', '2', ')', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'f', ',', 'preservesPartitioning', ')']","Deprecated: use mapPartitionsWithIndex instead.

        Return a new RDD by applying a function to each partition of this RDD,
        while tracking the index of the original partition.

        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)
        >>> def f(splitIndex, iterator): yield splitIndex
        >>> rdd.mapPartitionsWithSplit(f).sum()
        6","['Deprecated', ':', 'use', 'mapPartitionsWithIndex', 'instead', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L369-L383,train,Return a new RDD by applying a function to each partition of this RDD while tracking the index of the original partition.
apache/spark,python/pyspark/rdd.py,RDD.distinct,"def distinct(self, numPartitions=None):
        """"""
        Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
        """"""
        return self.map(lambda x: (x, None)) \
                   .reduceByKey(lambda x, _: x, numPartitions) \
                   .map(lambda x: x[0])",python,"def distinct(self, numPartitions=None):
        """"""
        Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]
        """"""
        return self.map(lambda x: (x, None)) \
                   .reduceByKey(lambda x, _: x, numPartitions) \
                   .map(lambda x: x[0])","['def', 'distinct', '(', 'self', ',', 'numPartitions', '=', 'None', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'None', ')', ')', '.', 'reduceByKey', '(', 'lambda', 'x', ',', '_', ':', 'x', ',', 'numPartitions', ')', '.', 'map', '(', 'lambda', 'x', ':', 'x', '[', '0', ']', ')']","Return a new RDD containing the distinct elements in this RDD.

        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())
        [1, 2, 3]","['Return', 'a', 'new', 'RDD', 'containing', 'the', 'distinct', 'elements', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L407-L416,train,Return an RDD containing the distinct elements in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.sample,"def sample(self, withReplacement, fraction, seed=None):
        """"""
        Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
        """"""
        assert fraction >= 0.0, ""Negative fraction value: %s"" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)",python,"def sample(self, withReplacement, fraction, seed=None):
        """"""
        Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True
        """"""
        assert fraction >= 0.0, ""Negative fraction value: %s"" % fraction
        return self.mapPartitionsWithIndex(RDDSampler(withReplacement, fraction, seed).func, True)","['def', 'sample', '(', 'self', ',', 'withReplacement', ',', 'fraction', ',', 'seed', '=', 'None', ')', ':', 'assert', 'fraction', '>=', '0.0', ',', '""Negative fraction value: %s""', '%', 'fraction', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDSampler', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'func', ',', 'True', ')']","Return a sampled subset of this RDD.

        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)
        :param fraction: expected size of the sample as a fraction of this RDD's size
            without replacement: probability that each element is chosen; fraction must be [0, 1]
            with replacement: expected number of times each element is chosen; fraction must be >= 0
        :param seed: seed for the random number generator

        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
            count of the given :class:`DataFrame`.

        >>> rdd = sc.parallelize(range(100), 4)
        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14
        True","['Return', 'a', 'sampled', 'subset', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L418-L436,train,Return a new RDD with the specified fraction of the total number of elements in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.randomSplit,"def randomSplit(self, weights, seed=None):
        """"""
        Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
        """"""
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]",python,"def randomSplit(self, weights, seed=None):
        """"""
        Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True
        """"""
        s = float(sum(weights))
        cweights = [0.0]
        for w in weights:
            cweights.append(cweights[-1] + w / s)
        if seed is None:
            seed = random.randint(0, 2 ** 32 - 1)
        return [self.mapPartitionsWithIndex(RDDRangeSampler(lb, ub, seed).func, True)
                for lb, ub in zip(cweights, cweights[1:])]","['def', 'randomSplit', '(', 'self', ',', 'weights', ',', 'seed', '=', 'None', ')', ':', 's', '=', 'float', '(', 'sum', '(', 'weights', ')', ')', 'cweights', '=', '[', '0.0', ']', 'for', 'w', 'in', 'weights', ':', 'cweights', '.', 'append', '(', 'cweights', '[', '-', '1', ']', '+', 'w', '/', 's', ')', 'if', 'seed', 'is', 'None', ':', 'seed', '=', 'random', '.', 'randint', '(', '0', ',', '2', '**', '32', '-', '1', ')', 'return', '[', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDRangeSampler', '(', 'lb', ',', 'ub', ',', 'seed', ')', '.', 'func', ',', 'True', ')', 'for', 'lb', ',', 'ub', 'in', 'zip', '(', 'cweights', ',', 'cweights', '[', '1', ':', ']', ')', ']']","Randomly splits this RDD with the provided weights.

        :param weights: weights for splits, will be normalized if they don't sum to 1
        :param seed: random seed
        :return: split RDDs in a list

        >>> rdd = sc.parallelize(range(500), 1)
        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)
        >>> len(rdd1.collect() + rdd2.collect())
        500
        >>> 150 < rdd1.count() < 250
        True
        >>> 250 < rdd2.count() < 350
        True","['Randomly', 'splits', 'this', 'RDD', 'with', 'the', 'provided', 'weights', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L438-L462,train,Randomly splits this RDD with the provided weights.
apache/spark,python/pyspark/rdd.py,RDD.takeSample,"def takeSample(self, withReplacement, num, seed=None):
        """"""
        Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
        """"""
        numStDev = 10.0

        if num < 0:
            raise ValueError(""Sample size cannot be negative."")
        elif num == 0:
            return []

        initialCount = self.count()
        if initialCount == 0:
            return []

        rand = random.Random(seed)

        if (not withReplacement) and num >= initialCount:
            # shuffle current RDD and return
            samples = self.collect()
            rand.shuffle(samples)
            return samples

        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                ""Sample size cannot be greater than %d."" % maxSampleSize)

        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()

        # If the first sample didn't turn out large enough, keep trying to take samples;
        # this shouldn't happen often because we use a big multiplier for their initial size.
        # See: scala/spark/RDD.scala
        while len(samples) < num:
            # TODO: add log warning for when more than one iteration was run
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()

        rand.shuffle(samples)

        return samples[0:num]",python,"def takeSample(self, withReplacement, num, seed=None):
        """"""
        Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10
        """"""
        numStDev = 10.0

        if num < 0:
            raise ValueError(""Sample size cannot be negative."")
        elif num == 0:
            return []

        initialCount = self.count()
        if initialCount == 0:
            return []

        rand = random.Random(seed)

        if (not withReplacement) and num >= initialCount:
            # shuffle current RDD and return
            samples = self.collect()
            rand.shuffle(samples)
            return samples

        maxSampleSize = sys.maxsize - int(numStDev * sqrt(sys.maxsize))
        if num > maxSampleSize:
            raise ValueError(
                ""Sample size cannot be greater than %d."" % maxSampleSize)

        fraction = RDD._computeFractionForSampleSize(
            num, initialCount, withReplacement)
        samples = self.sample(withReplacement, fraction, seed).collect()

        # If the first sample didn't turn out large enough, keep trying to take samples;
        # this shouldn't happen often because we use a big multiplier for their initial size.
        # See: scala/spark/RDD.scala
        while len(samples) < num:
            # TODO: add log warning for when more than one iteration was run
            seed = rand.randint(0, sys.maxsize)
            samples = self.sample(withReplacement, fraction, seed).collect()

        rand.shuffle(samples)

        return samples[0:num]","['def', 'takeSample', '(', 'self', ',', 'withReplacement', ',', 'num', ',', 'seed', '=', 'None', ')', ':', 'numStDev', '=', '10.0', 'if', 'num', '<', '0', ':', 'raise', 'ValueError', '(', '""Sample size cannot be negative.""', ')', 'elif', 'num', '==', '0', ':', 'return', '[', ']', 'initialCount', '=', 'self', '.', 'count', '(', ')', 'if', 'initialCount', '==', '0', ':', 'return', '[', ']', 'rand', '=', 'random', '.', 'Random', '(', 'seed', ')', 'if', '(', 'not', 'withReplacement', ')', 'and', 'num', '>=', 'initialCount', ':', '# shuffle current RDD and return', 'samples', '=', 'self', '.', 'collect', '(', ')', 'rand', '.', 'shuffle', '(', 'samples', ')', 'return', 'samples', 'maxSampleSize', '=', 'sys', '.', 'maxsize', '-', 'int', '(', 'numStDev', '*', 'sqrt', '(', 'sys', '.', 'maxsize', ')', ')', 'if', 'num', '>', 'maxSampleSize', ':', 'raise', 'ValueError', '(', '""Sample size cannot be greater than %d.""', '%', 'maxSampleSize', ')', 'fraction', '=', 'RDD', '.', '_computeFractionForSampleSize', '(', 'num', ',', 'initialCount', ',', 'withReplacement', ')', 'samples', '=', 'self', '.', 'sample', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'collect', '(', ')', ""# If the first sample didn't turn out large enough, keep trying to take samples;"", ""# this shouldn't happen often because we use a big multiplier for their initial size."", '# See: scala/spark/RDD.scala', 'while', 'len', '(', 'samples', ')', '<', 'num', ':', '# TODO: add log warning for when more than one iteration was run', 'seed', '=', 'rand', '.', 'randint', '(', '0', ',', 'sys', '.', 'maxsize', ')', 'samples', '=', 'self', '.', 'sample', '(', 'withReplacement', ',', 'fraction', ',', 'seed', ')', '.', 'collect', '(', ')', 'rand', '.', 'shuffle', '(', 'samples', ')', 'return', 'samples', '[', '0', ':', 'num', ']']","Return a fixed-size sampled subset of this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> rdd = sc.parallelize(range(0, 10))
        >>> len(rdd.takeSample(True, 20, 1))
        20
        >>> len(rdd.takeSample(False, 5, 2))
        5
        >>> len(rdd.takeSample(False, 15, 3))
        10","['Return', 'a', 'fixed', '-', 'size', 'sampled', 'subset', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L465-L518,train,Return a fixed - size sampled subset of this RDD.
apache/spark,python/pyspark/rdd.py,RDD._computeFractionForSampleSize,"def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        """"""
        Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
        """"""
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))",python,"def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement):
        """"""
        Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.
        """"""
        fraction = float(sampleSizeLowerBound) / total
        if withReplacement:
            numStDev = 5
            if (sampleSizeLowerBound < 12):
                numStDev = 9
            return fraction + numStDev * sqrt(fraction / total)
        else:
            delta = 0.00005
            gamma = - log(delta) / total
            return min(1, fraction + gamma + sqrt(gamma * gamma + 2 * gamma * fraction))","['def', '_computeFractionForSampleSize', '(', 'sampleSizeLowerBound', ',', 'total', ',', 'withReplacement', ')', ':', 'fraction', '=', 'float', '(', 'sampleSizeLowerBound', ')', '/', 'total', 'if', 'withReplacement', ':', 'numStDev', '=', '5', 'if', '(', 'sampleSizeLowerBound', '<', '12', ')', ':', 'numStDev', '=', '9', 'return', 'fraction', '+', 'numStDev', '*', 'sqrt', '(', 'fraction', '/', 'total', ')', 'else', ':', 'delta', '=', '0.00005', 'gamma', '=', '-', 'log', '(', 'delta', ')', '/', 'total', 'return', 'min', '(', '1', ',', 'fraction', '+', 'gamma', '+', 'sqrt', '(', 'gamma', '*', 'gamma', '+', '2', '*', 'gamma', '*', 'fraction', ')', ')']","Returns a sampling rate that guarantees a sample of
        size >= sampleSizeLowerBound 99.99% of the time.

        How the sampling rate is determined:
        Let p = num / total, where num is the sample size and total is the
        total number of data points in the RDD. We're trying to compute
        q > p such that
          - when sampling with replacement, we're drawing each data point
            with prob_i ~ Pois(q), where we want to guarantee
            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to
            total), i.e. the failure rate of not having a sufficiently large
            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient
            to guarantee 0.9999 success rate for num > 12, but we need a
            slightly larger q (9 empirically determined).
          - when sampling without replacement, we're drawing each data point
            with prob_i ~ Binomial(total, fraction) and our choice of q
            guarantees 1-delta, or 0.9999 success rate, where success rate is
            defined the same as in sampling with replacement.","['Returns', 'a', 'sampling', 'rate', 'that', 'guarantees', 'a', 'sample', 'of', 'size', '>', '=', 'sampleSizeLowerBound', '99', '.', '99%', 'of', 'the', 'time', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L521-L551,train,Compute the sampling rate for a specific sample size.
apache/spark,python/pyspark/rdd.py,RDD.union,"def union(self, other):
        """"""
        Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
        """"""
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            # These RDDs contain data in different serialized formats, so we
            # must normalize them to the default serializer.
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd",python,"def union(self, other):
        """"""
        Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]
        """"""
        if self._jrdd_deserializer == other._jrdd_deserializer:
            rdd = RDD(self._jrdd.union(other._jrdd), self.ctx,
                      self._jrdd_deserializer)
        else:
            # These RDDs contain data in different serialized formats, so we
            # must normalize them to the default serializer.
            self_copy = self._reserialize()
            other_copy = other._reserialize()
            rdd = RDD(self_copy._jrdd.union(other_copy._jrdd), self.ctx,
                      self.ctx.serializer)
        if (self.partitioner == other.partitioner and
                self.getNumPartitions() == rdd.getNumPartitions()):
            rdd.partitioner = self.partitioner
        return rdd","['def', 'union', '(', 'self', ',', 'other', ')', ':', 'if', 'self', '.', '_jrdd_deserializer', '==', 'other', '.', '_jrdd_deserializer', ':', 'rdd', '=', 'RDD', '(', 'self', '.', '_jrdd', '.', 'union', '(', 'other', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'self', '.', '_jrdd_deserializer', ')', 'else', ':', '# These RDDs contain data in different serialized formats, so we', '# must normalize them to the default serializer.', 'self_copy', '=', 'self', '.', '_reserialize', '(', ')', 'other_copy', '=', 'other', '.', '_reserialize', '(', ')', 'rdd', '=', 'RDD', '(', 'self_copy', '.', '_jrdd', '.', 'union', '(', 'other_copy', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'self', '.', 'ctx', '.', 'serializer', ')', 'if', '(', 'self', '.', 'partitioner', '==', 'other', '.', 'partitioner', 'and', 'self', '.', 'getNumPartitions', '(', ')', '==', 'rdd', '.', 'getNumPartitions', '(', ')', ')', ':', 'rdd', '.', 'partitioner', '=', 'self', '.', 'partitioner', 'return', 'rdd']","Return the union of this RDD and another one.

        >>> rdd = sc.parallelize([1, 1, 2, 3])
        >>> rdd.union(rdd).collect()
        [1, 1, 2, 3, 1, 1, 2, 3]","['Return', 'the', 'union', 'of', 'this', 'RDD', 'and', 'another', 'one', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L553-L574,train,Return the union of this RDD and another RDD.
apache/spark,python/pyspark/rdd.py,RDD.intersection,"def intersection(self, other):
        """"""
        Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
        """"""
        return self.map(lambda v: (v, None)) \
            .cogroup(other.map(lambda v: (v, None))) \
            .filter(lambda k_vs: all(k_vs[1])) \
            .keys()",python,"def intersection(self, other):
        """"""
        Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]
        """"""
        return self.map(lambda v: (v, None)) \
            .cogroup(other.map(lambda v: (v, None))) \
            .filter(lambda k_vs: all(k_vs[1])) \
            .keys()","['def', 'intersection', '(', 'self', ',', 'other', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'v', ':', '(', 'v', ',', 'None', ')', ')', '.', 'cogroup', '(', 'other', '.', 'map', '(', 'lambda', 'v', ':', '(', 'v', ',', 'None', ')', ')', ')', '.', 'filter', '(', 'lambda', 'k_vs', ':', 'all', '(', 'k_vs', '[', '1', ']', ')', ')', '.', 'keys', '(', ')']","Return the intersection of this RDD and another one. The output will
        not contain any duplicate elements, even if the input RDDs did.

        .. note:: This method performs a shuffle internally.

        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
        >>> rdd1.intersection(rdd2).collect()
        [1, 2, 3]","['Return', 'the', 'intersection', 'of', 'this', 'RDD', 'and', 'another', 'one', '.', 'The', 'output', 'will', 'not', 'contain', 'any', 'duplicate', 'elements', 'even', 'if', 'the', 'input', 'RDDs', 'did', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L576-L591,train,Return the intersection of this RDD and another RDD.
apache/spark,python/pyspark/rdd.py,RDD.repartitionAndSortWithinPartitions,"def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        """"""
        Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = _parse_memory(self.ctx._conf.get(""spark.python.worker.memory"", ""512m""))
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))

        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)",python,"def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,
                                           ascending=True, keyfunc=lambda x: x):
        """"""
        Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = _parse_memory(self.ctx._conf.get(""spark.python.worker.memory"", ""512m""))
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda k_v: keyfunc(k_v[0]), reverse=(not ascending)))

        return self.partitionBy(numPartitions, partitionFunc).mapPartitions(sortPartition, True)","['def', 'repartitionAndSortWithinPartitions', '(', 'self', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ',', 'ascending', '=', 'True', ',', 'keyfunc', '=', 'lambda', 'x', ':', 'x', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'memory', '=', '_parse_memory', '(', 'self', '.', 'ctx', '.', '_conf', '.', 'get', '(', '""spark.python.worker.memory""', ',', '""512m""', ')', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'def', 'sortPartition', '(', 'iterator', ')', ':', 'sort', '=', 'ExternalSorter', '(', 'memory', '*', '0.9', ',', 'serializer', ')', '.', 'sorted', 'return', 'iter', '(', 'sort', '(', 'iterator', ',', 'key', '=', 'lambda', 'k_v', ':', 'keyfunc', '(', 'k_v', '[', '0', ']', ')', ',', 'reverse', '=', '(', 'not', 'ascending', ')', ')', ')', 'return', 'self', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')']","Repartition the RDD according to the given partitioner and, within each resulting partition,
        sort records by their keys.

        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])
        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)
        >>> rdd2.glom().collect()
        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]","['Repartition', 'the', 'RDD', 'according', 'to', 'the', 'given', 'partitioner', 'and', 'within', 'each', 'resulting', 'partition', 'sort', 'records', 'by', 'their', 'keys', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L612-L633,train,Repartition the RDD according to the given partitioner and within each resulting partition sort records by their keys.
apache/spark,python/pyspark/rdd.py,RDD.sortByKey,"def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        """"""
        Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))

        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)

        # first compute the boundary of each part via sampling: we want to partition
        # the key-space into bins such that the bins have roughly the same
        # number of (key, value) pairs falling into them
        rddSize = self.count()
        if not rddSize:
            return self  # empty RDD
        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)

        # we have numPartitions many parts but one of the them has
        # an implicit boundary
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]

        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p

        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)",python,"def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x):
        """"""
        Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer

        def sortPartition(iterator):
            sort = ExternalSorter(memory * 0.9, serializer).sorted
            return iter(sort(iterator, key=lambda kv: keyfunc(kv[0]), reverse=(not ascending)))

        if numPartitions == 1:
            if self.getNumPartitions() > 1:
                self = self.coalesce(1)
            return self.mapPartitions(sortPartition, True)

        # first compute the boundary of each part via sampling: we want to partition
        # the key-space into bins such that the bins have roughly the same
        # number of (key, value) pairs falling into them
        rddSize = self.count()
        if not rddSize:
            return self  # empty RDD
        maxSampleSize = numPartitions * 20.0  # constant from Spark's RangePartitioner
        fraction = min(maxSampleSize / max(rddSize, 1), 1.0)
        samples = self.sample(False, fraction, 1).map(lambda kv: kv[0]).collect()
        samples = sorted(samples, key=keyfunc)

        # we have numPartitions many parts but one of the them has
        # an implicit boundary
        bounds = [samples[int(len(samples) * (i + 1) / numPartitions)]
                  for i in range(0, numPartitions - 1)]

        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p

        return self.partitionBy(numPartitions, rangePartitioner).mapPartitions(sortPartition, True)","['def', 'sortByKey', '(', 'self', ',', 'ascending', '=', 'True', ',', 'numPartitions', '=', 'None', ',', 'keyfunc', '=', 'lambda', 'x', ':', 'x', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'def', 'sortPartition', '(', 'iterator', ')', ':', 'sort', '=', 'ExternalSorter', '(', 'memory', '*', '0.9', ',', 'serializer', ')', '.', 'sorted', 'return', 'iter', '(', 'sort', '(', 'iterator', ',', 'key', '=', 'lambda', 'kv', ':', 'keyfunc', '(', 'kv', '[', '0', ']', ')', ',', 'reverse', '=', '(', 'not', 'ascending', ')', ')', ')', 'if', 'numPartitions', '==', '1', ':', 'if', 'self', '.', 'getNumPartitions', '(', ')', '>', '1', ':', 'self', '=', 'self', '.', 'coalesce', '(', '1', ')', 'return', 'self', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')', '# first compute the boundary of each part via sampling: we want to partition', '# the key-space into bins such that the bins have roughly the same', '# number of (key, value) pairs falling into them', 'rddSize', '=', 'self', '.', 'count', '(', ')', 'if', 'not', 'rddSize', ':', 'return', 'self', '# empty RDD', 'maxSampleSize', '=', 'numPartitions', '*', '20.0', ""# constant from Spark's RangePartitioner"", 'fraction', '=', 'min', '(', 'maxSampleSize', '/', 'max', '(', 'rddSize', ',', '1', ')', ',', '1.0', ')', 'samples', '=', 'self', '.', 'sample', '(', 'False', ',', 'fraction', ',', '1', ')', '.', 'map', '(', 'lambda', 'kv', ':', 'kv', '[', '0', ']', ')', '.', 'collect', '(', ')', 'samples', '=', 'sorted', '(', 'samples', ',', 'key', '=', 'keyfunc', ')', '# we have numPartitions many parts but one of the them has', '# an implicit boundary', 'bounds', '=', '[', 'samples', '[', 'int', '(', 'len', '(', 'samples', ')', '*', '(', 'i', '+', '1', ')', '/', 'numPartitions', ')', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'numPartitions', '-', '1', ')', ']', 'def', 'rangePartitioner', '(', 'k', ')', ':', 'p', '=', 'bisect', '.', 'bisect_left', '(', 'bounds', ',', 'keyfunc', '(', 'k', ')', ')', 'if', 'ascending', ':', 'return', 'p', 'else', ':', 'return', 'numPartitions', '-', '1', '-', 'p', 'return', 'self', '.', 'partitionBy', '(', 'numPartitions', ',', 'rangePartitioner', ')', '.', 'mapPartitions', '(', 'sortPartition', ',', 'True', ')']","Sorts this RDD, which is assumed to consist of (key, value) pairs.

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortByKey().first()
        ('1', 3)
        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]
        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])
        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()
        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]","['Sorts', 'this', 'RDD', 'which', 'is', 'assumed', 'to', 'consist', 'of', '(', 'key', 'value', ')', 'pairs', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L635-L689,train,Sorts this RDD by key.
apache/spark,python/pyspark/rdd.py,RDD.sortBy,"def sortBy(self, keyfunc, ascending=True, numPartitions=None):
        """"""
        Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        """"""
        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()",python,"def sortBy(self, keyfunc, ascending=True, numPartitions=None):
        """"""
        Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        """"""
        return self.keyBy(keyfunc).sortByKey(ascending, numPartitions).values()","['def', 'sortBy', '(', 'self', ',', 'keyfunc', ',', 'ascending', '=', 'True', ',', 'numPartitions', '=', 'None', ')', ':', 'return', 'self', '.', 'keyBy', '(', 'keyfunc', ')', '.', 'sortByKey', '(', 'ascending', ',', 'numPartitions', ')', '.', 'values', '(', ')']","Sorts this RDD by the given keyfunc

        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()
        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]
        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()
        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]","['Sorts', 'this', 'RDD', 'by', 'the', 'given', 'keyfunc']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L691-L701,train,Sorts this RDD by the given keyfunc.
apache/spark,python/pyspark/rdd.py,RDD.cartesian,"def cartesian(self, other):
        """"""
        Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
        """"""
        # Due to batching, we can't use the Java cartesian method.
        deserializer = CartesianDeserializer(self._jrdd_deserializer,
                                             other._jrdd_deserializer)
        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)",python,"def cartesian(self, other):
        """"""
        Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]
        """"""
        # Due to batching, we can't use the Java cartesian method.
        deserializer = CartesianDeserializer(self._jrdd_deserializer,
                                             other._jrdd_deserializer)
        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)","['def', 'cartesian', '(', 'self', ',', 'other', ')', ':', ""# Due to batching, we can't use the Java cartesian method."", 'deserializer', '=', 'CartesianDeserializer', '(', 'self', '.', '_jrdd_deserializer', ',', 'other', '.', '_jrdd_deserializer', ')', 'return', 'RDD', '(', 'self', '.', '_jrdd', '.', 'cartesian', '(', 'other', '.', '_jrdd', ')', ',', 'self', '.', 'ctx', ',', 'deserializer', ')']","Return the Cartesian product of this RDD and another one, that is, the
        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and
        C{b} is in C{other}.

        >>> rdd = sc.parallelize([1, 2])
        >>> sorted(rdd.cartesian(rdd).collect())
        [(1, 1), (1, 2), (2, 1), (2, 2)]","['Return', 'the', 'Cartesian', 'product', 'of', 'this', 'RDD', 'and', 'another', 'one', 'that', 'is', 'the', 'RDD', 'of', 'all', 'pairs', 'of', 'elements', 'C', '{', '(', 'a', 'b', ')', '}', 'where', 'C', '{', 'a', '}', 'is', 'in', 'C', '{', 'self', '}', 'and', 'C', '{', 'b', '}', 'is', 'in', 'C', '{', 'other', '}', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L716-L729,train,Return the Cartesian product of this RDD and another RDD.
apache/spark,python/pyspark/rdd.py,RDD.groupBy,"def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
        """"""
        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)",python,"def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]
        """"""
        return self.map(lambda x: (f(x), x)).groupByKey(numPartitions, partitionFunc)","['def', 'groupBy', '(', 'self', ',', 'f', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'f', '(', 'x', ')', ',', 'x', ')', ')', '.', 'groupByKey', '(', 'numPartitions', ',', 'partitionFunc', ')']","Return an RDD of grouped items.

        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])
        >>> result = rdd.groupBy(lambda x: x % 2).collect()
        >>> sorted([(x, sorted(y)) for (x, y) in result])
        [(0, [2, 8]), (1, [1, 1, 3, 5])]","['Return', 'an', 'RDD', 'of', 'grouped', 'items', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L731-L740,train,Return an RDD of grouped items by a function.
apache/spark,python/pyspark/rdd.py,RDD.pipe,"def pipe(self, command, env=None, checkCode=False):
        """"""
        Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
        """"""
        if env is None:
            env = dict()

        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)

            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()

            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception(""Pipe function `%s' exited ""
                                    ""with error code %d"" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)",python,"def pipe(self, command, env=None, checkCode=False):
        """"""
        Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.
        """"""
        if env is None:
            env = dict()

        def func(iterator):
            pipe = Popen(
                shlex.split(command), env=env, stdin=PIPE, stdout=PIPE)

            def pipe_objs(out):
                for obj in iterator:
                    s = unicode(obj).rstrip('\n') + '\n'
                    out.write(s.encode('utf-8'))
                out.close()
            Thread(target=pipe_objs, args=[pipe.stdin]).start()

            def check_return_code():
                pipe.wait()
                if checkCode and pipe.returncode:
                    raise Exception(""Pipe function `%s' exited ""
                                    ""with error code %d"" % (command, pipe.returncode))
                else:
                    for i in range(0):
                        yield i
            return (x.rstrip(b'\n').decode('utf-8') for x in
                    chain(iter(pipe.stdout.readline, b''), check_return_code()))
        return self.mapPartitions(func)","['def', 'pipe', '(', 'self', ',', 'command', ',', 'env', '=', 'None', ',', 'checkCode', '=', 'False', ')', ':', 'if', 'env', 'is', 'None', ':', 'env', '=', 'dict', '(', ')', 'def', 'func', '(', 'iterator', ')', ':', 'pipe', '=', 'Popen', '(', 'shlex', '.', 'split', '(', 'command', ')', ',', 'env', '=', 'env', ',', 'stdin', '=', 'PIPE', ',', 'stdout', '=', 'PIPE', ')', 'def', 'pipe_objs', '(', 'out', ')', ':', 'for', 'obj', 'in', 'iterator', ':', 's', '=', 'unicode', '(', 'obj', ')', '.', 'rstrip', '(', ""'\\n'"", ')', '+', ""'\\n'"", 'out', '.', 'write', '(', 's', '.', 'encode', '(', ""'utf-8'"", ')', ')', 'out', '.', 'close', '(', ')', 'Thread', '(', 'target', '=', 'pipe_objs', ',', 'args', '=', '[', 'pipe', '.', 'stdin', ']', ')', '.', 'start', '(', ')', 'def', 'check_return_code', '(', ')', ':', 'pipe', '.', 'wait', '(', ')', 'if', 'checkCode', 'and', 'pipe', '.', 'returncode', ':', 'raise', 'Exception', '(', '""Pipe function `%s\' exited ""', '""with error code %d""', '%', '(', 'command', ',', 'pipe', '.', 'returncode', ')', ')', 'else', ':', 'for', 'i', 'in', 'range', '(', '0', ')', ':', 'yield', 'i', 'return', '(', 'x', '.', 'rstrip', '(', ""b'\\n'"", ')', '.', 'decode', '(', ""'utf-8'"", ')', 'for', 'x', 'in', 'chain', '(', 'iter', '(', 'pipe', '.', 'stdout', '.', 'readline', ',', ""b''"", ')', ',', 'check_return_code', '(', ')', ')', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ')']","Return an RDD created by piping elements to a forked external process.

        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()
        [u'1', u'2', u'', u'3']

        :param checkCode: whether or not to check the return value of the shell command.","['Return', 'an', 'RDD', 'created', 'by', 'piping', 'elements', 'to', 'a', 'forked', 'external', 'process', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L743-L776,train,Return an RDD of strings from a shell command.
apache/spark,python/pyspark/rdd.py,RDD.foreach,"def foreach(self, f):
        """"""
        Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
        """"""
        f = fail_on_stopiteration(f)

        def processPartition(iterator):
            for x in iterator:
                f(x)
            return iter([])
        self.mapPartitions(processPartition).count()",python,"def foreach(self, f):
        """"""
        Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)
        """"""
        f = fail_on_stopiteration(f)

        def processPartition(iterator):
            for x in iterator:
                f(x)
            return iter([])
        self.mapPartitions(processPartition).count()","['def', 'foreach', '(', 'self', ',', 'f', ')', ':', 'f', '=', 'fail_on_stopiteration', '(', 'f', ')', 'def', 'processPartition', '(', 'iterator', ')', ':', 'for', 'x', 'in', 'iterator', ':', 'f', '(', 'x', ')', 'return', 'iter', '(', '[', ']', ')', 'self', '.', 'mapPartitions', '(', 'processPartition', ')', '.', 'count', '(', ')']","Applies a function to all elements of this RDD.

        >>> def f(x): print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)","['Applies', 'a', 'function', 'to', 'all', 'elements', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L778-L791,train,Applies a function to all elements of this RDD.
apache/spark,python/pyspark/rdd.py,RDD.foreachPartition,"def foreachPartition(self, f):
        """"""
        Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
        """"""
        def func(it):
            r = f(it)
            try:
                return iter(r)
            except TypeError:
                return iter([])
        self.mapPartitions(func).count()",python,"def foreachPartition(self, f):
        """"""
        Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)
        """"""
        def func(it):
            r = f(it)
            try:
                return iter(r)
            except TypeError:
                return iter([])
        self.mapPartitions(func).count()","['def', 'foreachPartition', '(', 'self', ',', 'f', ')', ':', 'def', 'func', '(', 'it', ')', ':', 'r', '=', 'f', '(', 'it', ')', 'try', ':', 'return', 'iter', '(', 'r', ')', 'except', 'TypeError', ':', 'return', 'iter', '(', '[', ']', ')', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'count', '(', ')']","Applies a function to each partition of this RDD.

        >>> def f(iterator):
        ...     for x in iterator:
        ...          print(x)
        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)","['Applies', 'a', 'function', 'to', 'each', 'partition', 'of', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L793-L808,train,Applies a function to each partition of this RDD.
apache/spark,python/pyspark/rdd.py,RDD.collect,"def collect(self):
        """"""
        Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
        """"""
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))",python,"def collect(self):
        """"""
        Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.
        """"""
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
        return list(_load_from_socket(sock_info, self._jrdd_deserializer))","['def', 'collect', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'collectAndServe', '(', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', ')', 'return', 'list', '(', '_load_from_socket', '(', 'sock_info', ',', 'self', '.', '_jrdd_deserializer', ')', ')']","Return a list that contains all of the elements in this RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.","['Return', 'a', 'list', 'that', 'contains', 'all', 'of', 'the', 'elements', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L810-L819,train,Returns a list containing all of the elements in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.reduce,"def reduce(self, f):
        """"""
        Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
        """"""
        f = fail_on_stopiteration(f)

        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)

        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError(""Can not reduce() empty RDD"")",python,"def reduce(self, f):
        """"""
        Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD
        """"""
        f = fail_on_stopiteration(f)

        def func(iterator):
            iterator = iter(iterator)
            try:
                initial = next(iterator)
            except StopIteration:
                return
            yield reduce(f, iterator, initial)

        vals = self.mapPartitions(func).collect()
        if vals:
            return reduce(f, vals)
        raise ValueError(""Can not reduce() empty RDD"")","['def', 'reduce', '(', 'self', ',', 'f', ')', ':', 'f', '=', 'fail_on_stopiteration', '(', 'f', ')', 'def', 'func', '(', 'iterator', ')', ':', 'iterator', '=', 'iter', '(', 'iterator', ')', 'try', ':', 'initial', '=', 'next', '(', 'iterator', ')', 'except', 'StopIteration', ':', 'return', 'yield', 'reduce', '(', 'f', ',', 'iterator', ',', 'initial', ')', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'if', 'vals', ':', 'return', 'reduce', '(', 'f', ',', 'vals', ')', 'raise', 'ValueError', '(', '""Can not reduce() empty RDD""', ')']","Reduces the elements of this RDD using the specified commutative and
        associative binary operator. Currently reduces partitions locally.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)
        15
        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)
        10
        >>> sc.parallelize([]).reduce(add)
        Traceback (most recent call last):
            ...
        ValueError: Can not reduce() empty RDD","['Reduces', 'the', 'elements', 'of', 'this', 'RDD', 'using', 'the', 'specified', 'commutative', 'and', 'associative', 'binary', 'operator', '.', 'Currently', 'reduces', 'partitions', 'locally', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L821-L849,train,Reduces the elements of this RDD using the specified commutative and an associative binary operator. Currently reduces partitions locally.
apache/spark,python/pyspark/rdd.py,RDD.treeReduce,"def treeReduce(self, f, depth=2):
        """"""
        Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
        """"""
        if depth < 1:
            raise ValueError(""Depth cannot be smaller than 1 but got %d."" % depth)

        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.

        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False

        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError(""Cannot reduce empty RDD."")
        return reduced[0]",python,"def treeReduce(self, f, depth=2):
        """"""
        Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5
        """"""
        if depth < 1:
            raise ValueError(""Depth cannot be smaller than 1 but got %d."" % depth)

        zeroValue = None, True  # Use the second entry to indicate whether this is a dummy value.

        def op(x, y):
            if x[1]:
                return y
            elif y[1]:
                return x
            else:
                return f(x[0], y[0]), False

        reduced = self.map(lambda x: (x, False)).treeAggregate(zeroValue, op, op, depth)
        if reduced[1]:
            raise ValueError(""Cannot reduce empty RDD."")
        return reduced[0]","['def', 'treeReduce', '(', 'self', ',', 'f', ',', 'depth', '=', '2', ')', ':', 'if', 'depth', '<', '1', ':', 'raise', 'ValueError', '(', '""Depth cannot be smaller than 1 but got %d.""', '%', 'depth', ')', 'zeroValue', '=', 'None', ',', 'True', '# Use the second entry to indicate whether this is a dummy value.', 'def', 'op', '(', 'x', ',', 'y', ')', ':', 'if', 'x', '[', '1', ']', ':', 'return', 'y', 'elif', 'y', '[', '1', ']', ':', 'return', 'x', 'else', ':', 'return', 'f', '(', 'x', '[', '0', ']', ',', 'y', '[', '0', ']', ')', ',', 'False', 'reduced', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'False', ')', ')', '.', 'treeAggregate', '(', 'zeroValue', ',', 'op', ',', 'op', ',', 'depth', ')', 'if', 'reduced', '[', '1', ']', ':', 'raise', 'ValueError', '(', '""Cannot reduce empty RDD.""', ')', 'return', 'reduced', '[', '0', ']']","Reduces the elements of this RDD in a multi-level tree pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeReduce(add)
        -5
        >>> rdd.treeReduce(add, 1)
        -5
        >>> rdd.treeReduce(add, 2)
        -5
        >>> rdd.treeReduce(add, 5)
        -5
        >>> rdd.treeReduce(add, 10)
        -5","['Reduces', 'the', 'elements', 'of', 'this', 'RDD', 'in', 'a', 'multi', '-', 'level', 'tree', 'pattern', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L851-L886,train,Reduces the elements of this RDD in a multi - level tree pattern.
apache/spark,python/pyspark/rdd.py,RDD.fold,"def fold(self, zeroValue, op):
        """"""
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral ""zero value.""

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
        """"""
        op = fail_on_stopiteration(op)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)",python,"def fold(self, zeroValue, op):
        """"""
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral ""zero value.""

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
        """"""
        op = fail_on_stopiteration(op)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)","['def', 'fold', '(', 'self', ',', 'zeroValue', ',', 'op', ')', ':', 'op', '=', 'fail_on_stopiteration', '(', 'op', ')', 'def', 'func', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'op', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', '# collecting result of mapPartitions here ensures that the copy of', '# zeroValue provided to each partition is unique from the one provided', '# to the final reduce call', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'return', 'reduce', '(', 'op', ',', 'vals', ',', 'zeroValue', ')']","Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral ""zero value.""

        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        This behaves somewhat differently from fold operations implemented
        for non-distributed collections in functional languages like Scala.
        This fold operation may be applied to partitions individually, and then
        fold those results into the final result, rather than apply the fold
        to each element sequentially in some defined ordering. For functions
        that are not commutative, the result may differ from that of a fold
        applied to a non-distributed collection.

        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15","['Aggregate', 'the', 'elements', 'of', 'each', 'partition', 'and', 'then', 'the', 'results', 'for', 'all', 'the', 'partitions', 'using', 'a', 'given', 'associative', 'function', 'and', 'a', 'neutral', 'zero', 'value', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L888-L920,train,Folds the elements of each partition into a single value.
apache/spark,python/pyspark/rdd.py,RDD.aggregate,"def aggregate(self, zeroValue, seqOp, combOp):
        """"""
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral ""zero
        value.""

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
        """"""
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)",python,"def aggregate(self, zeroValue, seqOp, combOp):
        """"""
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral ""zero
        value.""

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)
        """"""
        seqOp = fail_on_stopiteration(seqOp)
        combOp = fail_on_stopiteration(combOp)

        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc
        # collecting result of mapPartitions here ensures that the copy of
        # zeroValue provided to each partition is unique from the one provided
        # to the final reduce call
        vals = self.mapPartitions(func).collect()
        return reduce(combOp, vals, zeroValue)","['def', 'aggregate', '(', 'self', ',', 'zeroValue', ',', 'seqOp', ',', 'combOp', ')', ':', 'seqOp', '=', 'fail_on_stopiteration', '(', 'seqOp', ')', 'combOp', '=', 'fail_on_stopiteration', '(', 'combOp', ')', 'def', 'func', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'seqOp', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', '# collecting result of mapPartitions here ensures that the copy of', '# zeroValue provided to each partition is unique from the one provided', '# to the final reduce call', 'vals', '=', 'self', '.', 'mapPartitions', '(', 'func', ')', '.', 'collect', '(', ')', 'return', 'reduce', '(', 'combOp', ',', 'vals', ',', 'zeroValue', ')']","Aggregate the elements of each partition, and then the results for all
        the partitions, using a given combine functions and a neutral ""zero
        value.""

        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.

        The first function (seqOp) can return a different result type, U, than
        the type of this RDD. Thus, we need one operation for merging a T into
        an U and one operation for merging two U

        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))
        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))
        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)
        (10, 4)
        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)
        (0, 0)","['Aggregate', 'the', 'elements', 'of', 'each', 'partition', 'and', 'then', 'the', 'results', 'for', 'all', 'the', 'partitions', 'using', 'a', 'given', 'combine', 'functions', 'and', 'a', 'neutral', 'zero', 'value', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L922-L955,train,Aggregate the elements of each partition and then the results for all the partitions using a given combine functions and a neutral zeroValue value.
apache/spark,python/pyspark/rdd.py,RDD.treeAggregate,"def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        """"""
        Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
        """"""
        if depth < 1:
            raise ValueError(""Depth cannot be smaller than 1 but got %d."" % depth)

        if self.getNumPartitions() == 0:
            return zeroValue

        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc

        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree
        # aggregation.
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)

            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)

            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()

        return partiallyAggregated.reduce(combOp)",python,"def treeAggregate(self, zeroValue, seqOp, combOp, depth=2):
        """"""
        Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5
        """"""
        if depth < 1:
            raise ValueError(""Depth cannot be smaller than 1 but got %d."" % depth)

        if self.getNumPartitions() == 0:
            return zeroValue

        def aggregatePartition(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = seqOp(acc, obj)
            yield acc

        partiallyAggregated = self.mapPartitions(aggregatePartition)
        numPartitions = partiallyAggregated.getNumPartitions()
        scale = max(int(ceil(pow(numPartitions, 1.0 / depth))), 2)
        # If creating an extra level doesn't help reduce the wall-clock time, we stop the tree
        # aggregation.
        while numPartitions > scale + numPartitions / scale:
            numPartitions /= scale
            curNumPartitions = int(numPartitions)

            def mapPartition(i, iterator):
                for obj in iterator:
                    yield (i % curNumPartitions, obj)

            partiallyAggregated = partiallyAggregated \
                .mapPartitionsWithIndex(mapPartition) \
                .reduceByKey(combOp, curNumPartitions) \
                .values()

        return partiallyAggregated.reduce(combOp)","['def', 'treeAggregate', '(', 'self', ',', 'zeroValue', ',', 'seqOp', ',', 'combOp', ',', 'depth', '=', '2', ')', ':', 'if', 'depth', '<', '1', ':', 'raise', 'ValueError', '(', '""Depth cannot be smaller than 1 but got %d.""', '%', 'depth', ')', 'if', 'self', '.', 'getNumPartitions', '(', ')', '==', '0', ':', 'return', 'zeroValue', 'def', 'aggregatePartition', '(', 'iterator', ')', ':', 'acc', '=', 'zeroValue', 'for', 'obj', 'in', 'iterator', ':', 'acc', '=', 'seqOp', '(', 'acc', ',', 'obj', ')', 'yield', 'acc', 'partiallyAggregated', '=', 'self', '.', 'mapPartitions', '(', 'aggregatePartition', ')', 'numPartitions', '=', 'partiallyAggregated', '.', 'getNumPartitions', '(', ')', 'scale', '=', 'max', '(', 'int', '(', 'ceil', '(', 'pow', '(', 'numPartitions', ',', '1.0', '/', 'depth', ')', ')', ')', ',', '2', ')', ""# If creating an extra level doesn't help reduce the wall-clock time, we stop the tree"", '# aggregation.', 'while', 'numPartitions', '>', 'scale', '+', 'numPartitions', '/', 'scale', ':', 'numPartitions', '/=', 'scale', 'curNumPartitions', '=', 'int', '(', 'numPartitions', ')', 'def', 'mapPartition', '(', 'i', ',', 'iterator', ')', ':', 'for', 'obj', 'in', 'iterator', ':', 'yield', '(', 'i', '%', 'curNumPartitions', ',', 'obj', ')', 'partiallyAggregated', '=', 'partiallyAggregated', '.', 'mapPartitionsWithIndex', '(', 'mapPartition', ')', '.', 'reduceByKey', '(', 'combOp', ',', 'curNumPartitions', ')', '.', 'values', '(', ')', 'return', 'partiallyAggregated', '.', 'reduce', '(', 'combOp', ')']","Aggregates the elements of this RDD in a multi-level tree
        pattern.

        :param depth: suggested depth of the tree (default: 2)

        >>> add = lambda x, y: x + y
        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)
        >>> rdd.treeAggregate(0, add, add)
        -5
        >>> rdd.treeAggregate(0, add, add, 1)
        -5
        >>> rdd.treeAggregate(0, add, add, 2)
        -5
        >>> rdd.treeAggregate(0, add, add, 5)
        -5
        >>> rdd.treeAggregate(0, add, add, 10)
        -5","['Aggregates', 'the', 'elements', 'of', 'this', 'RDD', 'in', 'a', 'multi', '-', 'level', 'tree', 'pattern', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L957-L1007,train,This function aggregates the elements of this RDD in a multi - level tree.
apache/spark,python/pyspark/rdd.py,RDD.max,"def max(self, key=None):
        """"""
        Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
        """"""
        if key is None:
            return self.reduce(max)
        return self.reduce(lambda a, b: max(a, b, key=key))",python,"def max(self, key=None):
        """"""
        Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0
        """"""
        if key is None:
            return self.reduce(max)
        return self.reduce(lambda a, b: max(a, b, key=key))","['def', 'max', '(', 'self', ',', 'key', '=', 'None', ')', ':', 'if', 'key', 'is', 'None', ':', 'return', 'self', '.', 'reduce', '(', 'max', ')', 'return', 'self', '.', 'reduce', '(', 'lambda', 'a', ',', 'b', ':', 'max', '(', 'a', ',', 'b', ',', 'key', '=', 'key', ')', ')']","Find the maximum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])
        >>> rdd.max()
        43.0
        >>> rdd.max(key=str)
        5.0","['Find', 'the', 'maximum', 'item', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1009-L1023,train,Find the maximum item in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.min,"def min(self, key=None):
        """"""
        Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
        """"""
        if key is None:
            return self.reduce(min)
        return self.reduce(lambda a, b: min(a, b, key=key))",python,"def min(self, key=None):
        """"""
        Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0
        """"""
        if key is None:
            return self.reduce(min)
        return self.reduce(lambda a, b: min(a, b, key=key))","['def', 'min', '(', 'self', ',', 'key', '=', 'None', ')', ':', 'if', 'key', 'is', 'None', ':', 'return', 'self', '.', 'reduce', '(', 'min', ')', 'return', 'self', '.', 'reduce', '(', 'lambda', 'a', ',', 'b', ':', 'min', '(', 'a', ',', 'b', ',', 'key', '=', 'key', ')', ')']","Find the minimum item in this RDD.

        :param key: A function used to generate key for comparing

        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])
        >>> rdd.min()
        2.0
        >>> rdd.min(key=str)
        10.0","['Find', 'the', 'minimum', 'item', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1025-L1039,train,Find the minimum item in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.sum,"def sum(self):
        """"""
        Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
        """"""
        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)",python,"def sum(self):
        """"""
        Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0
        """"""
        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)","['def', 'sum', '(', 'self', ')', ':', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'x', ':', '[', 'sum', '(', 'x', ')', ']', ')', '.', 'fold', '(', '0', ',', 'operator', '.', 'add', ')']","Add up the elements in this RDD.

        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()
        6.0","['Add', 'up', 'the', 'elements', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1041-L1048,train,Return the sum of the elements in this RDD.
apache/spark,python/pyspark/rdd.py,RDD.stats,"def stats(self):
        """"""
        Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
        """"""
        def redFunc(left_counter, right_counter):
            return left_counter.mergeStats(right_counter)

        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)",python,"def stats(self):
        """"""
        Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.
        """"""
        def redFunc(left_counter, right_counter):
            return left_counter.mergeStats(right_counter)

        return self.mapPartitions(lambda i: [StatCounter(i)]).reduce(redFunc)","['def', 'stats', '(', 'self', ')', ':', 'def', 'redFunc', '(', 'left_counter', ',', 'right_counter', ')', ':', 'return', 'left_counter', '.', 'mergeStats', '(', 'right_counter', ')', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'i', ':', '[', 'StatCounter', '(', 'i', ')', ']', ')', '.', 'reduce', '(', 'redFunc', ')']","Return a L{StatCounter} object that captures the mean, variance
        and count of the RDD's elements in one operation.","['Return', 'a', 'L', '{', 'StatCounter', '}', 'object', 'that', 'captures', 'the', 'mean', 'variance', 'and', 'count', 'of', 'the', 'RDD', 's', 'elements', 'in', 'one', 'operation', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1059-L1067,train,"Return a new RDD with the mean variance
        and count of the elements in one operation."
apache/spark,python/pyspark/rdd.py,RDD.histogram,"def histogram(self, buckets):
        """"""
        Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize([""ab"", ""ac"", ""b"", ""bd"", ""ef""])
        >>> rdd.histogram((""a"", ""b"", ""c""))
        (('a', 'b', 'c'), [2, 2])
        """"""

        if isinstance(buckets, int):
            if buckets < 1:
                raise ValueError(""number of buckets must be >= 1"")

            # filter out non-comparable elements
            def comparable(x):
                if x is None:
                    return False
                if type(x) is float and isnan(x):
                    return False
                return True

            filtered = self.filter(comparable)

            # faster than stats()
            def minmax(a, b):
                return min(a[0], b[0]), max(a[1], b[1])
            try:
                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)
            except TypeError as e:
                if "" empty "" in str(e):
                    raise ValueError(""can not generate buckets from empty RDD"")
                raise

            if minv == maxv or buckets == 1:
                return [minv, maxv], [filtered.count()]

            try:
                inc = (maxv - minv) / buckets
            except TypeError:
                raise TypeError(""Can not generate buckets with non-number in RDD"")

            if isinf(inc):
                raise ValueError(""Can not generate buckets with infinite value"")

            # keep them as integer if possible
            inc = int(inc)
            if inc * buckets != maxv - minv:
                inc = (maxv - minv) * 1.0 / buckets

            buckets = [i * inc + minv for i in range(buckets)]
            buckets.append(maxv)  # fix accumulated error
            even = True

        elif isinstance(buckets, (list, tuple)):
            if len(buckets) < 2:
                raise ValueError(""buckets should have more than one value"")

            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):
                raise ValueError(""can not have None or NaN in buckets"")

            if sorted(buckets) != list(buckets):
                raise ValueError(""buckets should be sorted"")

            if len(set(buckets)) != len(buckets):
                raise ValueError(""buckets should not contain duplicated values"")

            minv = buckets[0]
            maxv = buckets[-1]
            even = False
            inc = None
            try:
                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]
            except TypeError:
                pass  # objects in buckets do not support '-'
            else:
                if max(steps) - min(steps) < 1e-10:  # handle precision errors
                    even = True
                    inc = (maxv - minv) / (len(buckets) - 1)

        else:
            raise TypeError(""buckets should be a list or tuple or number(int or long)"")

        def histogram(iterator):
            counters = [0] * len(buckets)
            for i in iterator:
                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:
                    continue
                t = (int((i - minv) / inc) if even
                     else bisect.bisect_right(buckets, i) - 1)
                counters[t] += 1
            # add last two together
            last = counters.pop()
            counters[-1] += last
            return [counters]

        def mergeCounters(a, b):
            return [i + j for i, j in zip(a, b)]

        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)",python,"def histogram(self, buckets):
        """"""
        Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize([""ab"", ""ac"", ""b"", ""bd"", ""ef""])
        >>> rdd.histogram((""a"", ""b"", ""c""))
        (('a', 'b', 'c'), [2, 2])
        """"""

        if isinstance(buckets, int):
            if buckets < 1:
                raise ValueError(""number of buckets must be >= 1"")

            # filter out non-comparable elements
            def comparable(x):
                if x is None:
                    return False
                if type(x) is float and isnan(x):
                    return False
                return True

            filtered = self.filter(comparable)

            # faster than stats()
            def minmax(a, b):
                return min(a[0], b[0]), max(a[1], b[1])
            try:
                minv, maxv = filtered.map(lambda x: (x, x)).reduce(minmax)
            except TypeError as e:
                if "" empty "" in str(e):
                    raise ValueError(""can not generate buckets from empty RDD"")
                raise

            if minv == maxv or buckets == 1:
                return [minv, maxv], [filtered.count()]

            try:
                inc = (maxv - minv) / buckets
            except TypeError:
                raise TypeError(""Can not generate buckets with non-number in RDD"")

            if isinf(inc):
                raise ValueError(""Can not generate buckets with infinite value"")

            # keep them as integer if possible
            inc = int(inc)
            if inc * buckets != maxv - minv:
                inc = (maxv - minv) * 1.0 / buckets

            buckets = [i * inc + minv for i in range(buckets)]
            buckets.append(maxv)  # fix accumulated error
            even = True

        elif isinstance(buckets, (list, tuple)):
            if len(buckets) < 2:
                raise ValueError(""buckets should have more than one value"")

            if any(i is None or isinstance(i, float) and isnan(i) for i in buckets):
                raise ValueError(""can not have None or NaN in buckets"")

            if sorted(buckets) != list(buckets):
                raise ValueError(""buckets should be sorted"")

            if len(set(buckets)) != len(buckets):
                raise ValueError(""buckets should not contain duplicated values"")

            minv = buckets[0]
            maxv = buckets[-1]
            even = False
            inc = None
            try:
                steps = [buckets[i + 1] - buckets[i] for i in range(len(buckets) - 1)]
            except TypeError:
                pass  # objects in buckets do not support '-'
            else:
                if max(steps) - min(steps) < 1e-10:  # handle precision errors
                    even = True
                    inc = (maxv - minv) / (len(buckets) - 1)

        else:
            raise TypeError(""buckets should be a list or tuple or number(int or long)"")

        def histogram(iterator):
            counters = [0] * len(buckets)
            for i in iterator:
                if i is None or (type(i) is float and isnan(i)) or i > maxv or i < minv:
                    continue
                t = (int((i - minv) / inc) if even
                     else bisect.bisect_right(buckets, i) - 1)
                counters[t] += 1
            # add last two together
            last = counters.pop()
            counters[-1] += last
            return [counters]

        def mergeCounters(a, b):
            return [i + j for i, j in zip(a, b)]

        return buckets, self.mapPartitions(histogram).reduce(mergeCounters)","['def', 'histogram', '(', 'self', ',', 'buckets', ')', ':', 'if', 'isinstance', '(', 'buckets', ',', 'int', ')', ':', 'if', 'buckets', '<', '1', ':', 'raise', 'ValueError', '(', '""number of buckets must be >= 1""', ')', '# filter out non-comparable elements', 'def', 'comparable', '(', 'x', ')', ':', 'if', 'x', 'is', 'None', ':', 'return', 'False', 'if', 'type', '(', 'x', ')', 'is', 'float', 'and', 'isnan', '(', 'x', ')', ':', 'return', 'False', 'return', 'True', 'filtered', '=', 'self', '.', 'filter', '(', 'comparable', ')', '# faster than stats()', 'def', 'minmax', '(', 'a', ',', 'b', ')', ':', 'return', 'min', '(', 'a', '[', '0', ']', ',', 'b', '[', '0', ']', ')', ',', 'max', '(', 'a', '[', '1', ']', ',', 'b', '[', '1', ']', ')', 'try', ':', 'minv', ',', 'maxv', '=', 'filtered', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'x', ')', ')', '.', 'reduce', '(', 'minmax', ')', 'except', 'TypeError', 'as', 'e', ':', 'if', '"" empty ""', 'in', 'str', '(', 'e', ')', ':', 'raise', 'ValueError', '(', '""can not generate buckets from empty RDD""', ')', 'raise', 'if', 'minv', '==', 'maxv', 'or', 'buckets', '==', '1', ':', 'return', '[', 'minv', ',', 'maxv', ']', ',', '[', 'filtered', '.', 'count', '(', ')', ']', 'try', ':', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '/', 'buckets', 'except', 'TypeError', ':', 'raise', 'TypeError', '(', '""Can not generate buckets with non-number in RDD""', ')', 'if', 'isinf', '(', 'inc', ')', ':', 'raise', 'ValueError', '(', '""Can not generate buckets with infinite value""', ')', '# keep them as integer if possible', 'inc', '=', 'int', '(', 'inc', ')', 'if', 'inc', '*', 'buckets', '!=', 'maxv', '-', 'minv', ':', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '*', '1.0', '/', 'buckets', 'buckets', '=', '[', 'i', '*', 'inc', '+', 'minv', 'for', 'i', 'in', 'range', '(', 'buckets', ')', ']', 'buckets', '.', 'append', '(', 'maxv', ')', '# fix accumulated error', 'even', '=', 'True', 'elif', 'isinstance', '(', 'buckets', ',', '(', 'list', ',', 'tuple', ')', ')', ':', 'if', 'len', '(', 'buckets', ')', '<', '2', ':', 'raise', 'ValueError', '(', '""buckets should have more than one value""', ')', 'if', 'any', '(', 'i', 'is', 'None', 'or', 'isinstance', '(', 'i', ',', 'float', ')', 'and', 'isnan', '(', 'i', ')', 'for', 'i', 'in', 'buckets', ')', ':', 'raise', 'ValueError', '(', '""can not have None or NaN in buckets""', ')', 'if', 'sorted', '(', 'buckets', ')', '!=', 'list', '(', 'buckets', ')', ':', 'raise', 'ValueError', '(', '""buckets should be sorted""', ')', 'if', 'len', '(', 'set', '(', 'buckets', ')', ')', '!=', 'len', '(', 'buckets', ')', ':', 'raise', 'ValueError', '(', '""buckets should not contain duplicated values""', ')', 'minv', '=', 'buckets', '[', '0', ']', 'maxv', '=', 'buckets', '[', '-', '1', ']', 'even', '=', 'False', 'inc', '=', 'None', 'try', ':', 'steps', '=', '[', 'buckets', '[', 'i', '+', '1', ']', '-', 'buckets', '[', 'i', ']', 'for', 'i', 'in', 'range', '(', 'len', '(', 'buckets', ')', '-', '1', ')', ']', 'except', 'TypeError', ':', 'pass', ""# objects in buckets do not support '-'"", 'else', ':', 'if', 'max', '(', 'steps', ')', '-', 'min', '(', 'steps', ')', '<', '1e-10', ':', '# handle precision errors', 'even', '=', 'True', 'inc', '=', '(', 'maxv', '-', 'minv', ')', '/', '(', 'len', '(', 'buckets', ')', '-', '1', ')', 'else', ':', 'raise', 'TypeError', '(', '""buckets should be a list or tuple or number(int or long)""', ')', 'def', 'histogram', '(', 'iterator', ')', ':', 'counters', '=', '[', '0', ']', '*', 'len', '(', 'buckets', ')', 'for', 'i', 'in', 'iterator', ':', 'if', 'i', 'is', 'None', 'or', '(', 'type', '(', 'i', ')', 'is', 'float', 'and', 'isnan', '(', 'i', ')', ')', 'or', 'i', '>', 'maxv', 'or', 'i', '<', 'minv', ':', 'continue', 't', '=', '(', 'int', '(', '(', 'i', '-', 'minv', ')', '/', 'inc', ')', 'if', 'even', 'else', 'bisect', '.', 'bisect_right', '(', 'buckets', ',', 'i', ')', '-', '1', ')', 'counters', '[', 't', ']', '+=', '1', '# add last two together', 'last', '=', 'counters', '.', 'pop', '(', ')', 'counters', '[', '-', '1', ']', '+=', 'last', 'return', '[', 'counters', ']', 'def', 'mergeCounters', '(', 'a', ',', 'b', ')', ':', 'return', '[', 'i', '+', 'j', 'for', 'i', ',', 'j', 'in', 'zip', '(', 'a', ',', 'b', ')', ']', 'return', 'buckets', ',', 'self', '.', 'mapPartitions', '(', 'histogram', ')', '.', 'reduce', '(', 'mergeCounters', ')']","Compute a histogram using the provided buckets. The buckets
        are all open to the right except for the last which is closed.
        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],
        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1
        and 50 we would have a histogram of 1,0,1.

        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),
        this can be switched from an O(log n) inseration to O(1) per
        element (where n is the number of buckets).

        Buckets must be sorted, not contain any duplicates, and have
        at least two elements.

        If `buckets` is a number, it will generate buckets which are
        evenly spaced between the minimum and maximum of the RDD. For
        example, if the min value is 0 and the max is 100, given `buckets`
        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must
        be at least 1. An exception is raised if the RDD contains infinity.
        If the elements in the RDD do not vary (max == min), a single bucket
        will be used.

        The return value is a tuple of buckets and histogram.

        >>> rdd = sc.parallelize(range(51))
        >>> rdd.histogram(2)
        ([0, 25, 50], [25, 26])
        >>> rdd.histogram([0, 5, 25, 50])
        ([0, 5, 25, 50], [5, 20, 26])
        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets
        ([0, 15, 30, 45, 60], [15, 15, 15, 6])
        >>> rdd = sc.parallelize([""ab"", ""ac"", ""b"", ""bd"", ""ef""])
        >>> rdd.histogram((""a"", ""b"", ""c""))
        (('a', 'b', 'c'), [2, 2])","['Compute', 'a', 'histogram', 'using', 'the', 'provided', 'buckets', '.', 'The', 'buckets', 'are', 'all', 'open', 'to', 'the', 'right', 'except', 'for', 'the', 'last', 'which', 'is', 'closed', '.', 'e', '.', 'g', '.', '[', '1', '10', '20', '50', ']', 'means', 'the', 'buckets', 'are', '[', '1', '10', ')', '[', '10', '20', ')', '[', '20', '50', ']', 'which', 'means', '1<', '=', 'x<10', '10<', '=', 'x<20', '20<', '=', 'x<', '=', '50', '.', 'And', 'on', 'the', 'input', 'of', '1', 'and', '50', 'we', 'would', 'have', 'a', 'histogram', 'of', '1', '0', '1', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1069-L1195,train,Compute a histogram of the given buckets.
apache/spark,python/pyspark/rdd.py,RDD.countByValue,"def countByValue(self):
        """"""
        Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
        """"""
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)",python,"def countByValue(self):
        """"""
        Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]
        """"""
        def countPartition(iterator):
            counts = defaultdict(int)
            for obj in iterator:
                counts[obj] += 1
            yield counts

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] += v
            return m1
        return self.mapPartitions(countPartition).reduce(mergeMaps)","['def', 'countByValue', '(', 'self', ')', ':', 'def', 'countPartition', '(', 'iterator', ')', ':', 'counts', '=', 'defaultdict', '(', 'int', ')', 'for', 'obj', 'in', 'iterator', ':', 'counts', '[', 'obj', ']', '+=', '1', 'yield', 'counts', 'def', 'mergeMaps', '(', 'm1', ',', 'm2', ')', ':', 'for', 'k', ',', 'v', 'in', 'm2', '.', 'items', '(', ')', ':', 'm1', '[', 'k', ']', '+=', 'v', 'return', 'm1', 'return', 'self', '.', 'mapPartitions', '(', 'countPartition', ')', '.', 'reduce', '(', 'mergeMaps', ')']","Return the count of each unique value in this RDD as a dictionary of
        (value, count) pairs.

        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())
        [(1, 2), (2, 3)]","['Return', 'the', 'count', 'of', 'each', 'unique', 'value', 'in', 'this', 'RDD', 'as', 'a', 'dictionary', 'of', '(', 'value', 'count', ')', 'pairs', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1245-L1263,train,"Return the count of each unique value in this RDD as a dictionary of
        = > count"
apache/spark,python/pyspark/rdd.py,RDD.top,"def top(self, num, key=None):
        """"""
        Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
        """"""
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)

        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)

        return self.mapPartitions(topIterator).reduce(merge)",python,"def top(self, num, key=None):
        """"""
        Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]
        """"""
        def topIterator(iterator):
            yield heapq.nlargest(num, iterator, key=key)

        def merge(a, b):
            return heapq.nlargest(num, a + b, key=key)

        return self.mapPartitions(topIterator).reduce(merge)","['def', 'top', '(', 'self', ',', 'num', ',', 'key', '=', 'None', ')', ':', 'def', 'topIterator', '(', 'iterator', ')', ':', 'yield', 'heapq', '.', 'nlargest', '(', 'num', ',', 'iterator', ',', 'key', '=', 'key', ')', 'def', 'merge', '(', 'a', ',', 'b', ')', ':', 'return', 'heapq', '.', 'nlargest', '(', 'num', ',', 'a', '+', 'b', ',', 'key', '=', 'key', ')', 'return', 'self', '.', 'mapPartitions', '(', 'topIterator', ')', '.', 'reduce', '(', 'merge', ')']","Get the top N elements from an RDD.

        .. note:: This method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        .. note:: It returns the list sorted in descending order.

        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)
        [12]
        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)
        [6, 5]
        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)
        [4, 3, 2]","['Get', 'the', 'top', 'N', 'elements', 'from', 'an', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1265-L1287,train,Return the top N elements from an RDD.
apache/spark,python/pyspark/rdd.py,RDD.takeOrdered,"def takeOrdered(self, num, key=None):
        """"""
        Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
        """"""

        def merge(a, b):
            return heapq.nsmallest(num, a + b, key)

        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)",python,"def takeOrdered(self, num, key=None):
        """"""
        Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]
        """"""

        def merge(a, b):
            return heapq.nsmallest(num, a + b, key)

        return self.mapPartitions(lambda it: [heapq.nsmallest(num, it, key)]).reduce(merge)","['def', 'takeOrdered', '(', 'self', ',', 'num', ',', 'key', '=', 'None', ')', ':', 'def', 'merge', '(', 'a', ',', 'b', ')', ':', 'return', 'heapq', '.', 'nsmallest', '(', 'num', ',', 'a', '+', 'b', ',', 'key', ')', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'heapq', '.', 'nsmallest', '(', 'num', ',', 'it', ',', 'key', ')', ']', ')', '.', 'reduce', '(', 'merge', ')']","Get the N elements from an RDD ordered in ascending order or as
        specified by the optional key function.

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
        [1, 2, 3, 4, 5, 6]
        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)
        [10, 9, 7, 6, 5, 4]","['Get', 'the', 'N', 'elements', 'from', 'an', 'RDD', 'ordered', 'in', 'ascending', 'order', 'or', 'as', 'specified', 'by', 'the', 'optional', 'key', 'function', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1289-L1306,train,"Take the N elements from an RDD ordered in ascending order or as
            is specified by the optional key function."
apache/spark,python/pyspark/rdd.py,RDD.take,"def take(self, num):
        """"""
        Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
        """"""
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0

        while len(items) < num and partsScanned < totalParts:
            # The number of partitions to try in this iteration.
            # It is ok for this number to be greater than totalParts because
            # we actually cap it at totalParts in runJob.
            numPartsToTry = 1
            if partsScanned > 0:
                # If we didn't find any rows after the previous iteration,
                # quadruple and retry.  Otherwise, interpolate the number of
                # partitions we need to try, but overestimate it by 50%.
                # We also cap the estimation in the end.
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    # the first parameter of max is >=1 whenever partsScanned >= 2
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)

            left = num - len(items)

            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1

            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)

            items += res
            partsScanned += numPartsToTry

        return items[:num]",python,"def take(self, num):
        """"""
        Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]
        """"""
        items = []
        totalParts = self.getNumPartitions()
        partsScanned = 0

        while len(items) < num and partsScanned < totalParts:
            # The number of partitions to try in this iteration.
            # It is ok for this number to be greater than totalParts because
            # we actually cap it at totalParts in runJob.
            numPartsToTry = 1
            if partsScanned > 0:
                # If we didn't find any rows after the previous iteration,
                # quadruple and retry.  Otherwise, interpolate the number of
                # partitions we need to try, but overestimate it by 50%.
                # We also cap the estimation in the end.
                if len(items) == 0:
                    numPartsToTry = partsScanned * 4
                else:
                    # the first parameter of max is >=1 whenever partsScanned >= 2
                    numPartsToTry = int(1.5 * num * partsScanned / len(items)) - partsScanned
                    numPartsToTry = min(max(numPartsToTry, 1), partsScanned * 4)

            left = num - len(items)

            def takeUpToNumLeft(iterator):
                iterator = iter(iterator)
                taken = 0
                while taken < left:
                    try:
                        yield next(iterator)
                    except StopIteration:
                        return
                    taken += 1

            p = range(partsScanned, min(partsScanned + numPartsToTry, totalParts))
            res = self.context.runJob(self, takeUpToNumLeft, p)

            items += res
            partsScanned += numPartsToTry

        return items[:num]","['def', 'take', '(', 'self', ',', 'num', ')', ':', 'items', '=', '[', ']', 'totalParts', '=', 'self', '.', 'getNumPartitions', '(', ')', 'partsScanned', '=', '0', 'while', 'len', '(', 'items', ')', '<', 'num', 'and', 'partsScanned', '<', 'totalParts', ':', '# The number of partitions to try in this iteration.', '# It is ok for this number to be greater than totalParts because', '# we actually cap it at totalParts in runJob.', 'numPartsToTry', '=', '1', 'if', 'partsScanned', '>', '0', ':', ""# If we didn't find any rows after the previous iteration,"", '# quadruple and retry.  Otherwise, interpolate the number of', '# partitions we need to try, but overestimate it by 50%.', '# We also cap the estimation in the end.', 'if', 'len', '(', 'items', ')', '==', '0', ':', 'numPartsToTry', '=', 'partsScanned', '*', '4', 'else', ':', '# the first parameter of max is >=1 whenever partsScanned >= 2', 'numPartsToTry', '=', 'int', '(', '1.5', '*', 'num', '*', 'partsScanned', '/', 'len', '(', 'items', ')', ')', '-', 'partsScanned', 'numPartsToTry', '=', 'min', '(', 'max', '(', 'numPartsToTry', ',', '1', ')', ',', 'partsScanned', '*', '4', ')', 'left', '=', 'num', '-', 'len', '(', 'items', ')', 'def', 'takeUpToNumLeft', '(', 'iterator', ')', ':', 'iterator', '=', 'iter', '(', 'iterator', ')', 'taken', '=', '0', 'while', 'taken', '<', 'left', ':', 'try', ':', 'yield', 'next', '(', 'iterator', ')', 'except', 'StopIteration', ':', 'return', 'taken', '+=', '1', 'p', '=', 'range', '(', 'partsScanned', ',', 'min', '(', 'partsScanned', '+', 'numPartsToTry', ',', 'totalParts', ')', ')', 'res', '=', 'self', '.', 'context', '.', 'runJob', '(', 'self', ',', 'takeUpToNumLeft', ',', 'p', ')', 'items', '+=', 'res', 'partsScanned', '+=', 'numPartsToTry', 'return', 'items', '[', ':', 'num', ']']","Take the first num elements of the RDD.

        It works by first scanning one partition, and use the results from
        that partition to estimate the number of additional partitions needed
        to satisfy the limit.

        Translated from the Scala implementation in RDD#take().

        .. note:: this method should only be used if the resulting array is expected
            to be small, as all the data is loaded into the driver's memory.

        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
        [2, 3]
        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)
        [2, 3, 4, 5, 6]
        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)
        [91, 92, 93]","['Take', 'the', 'first', 'num', 'elements', 'of', 'the', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1308-L1367,train,Take the first num elements of the RDD.
apache/spark,python/pyspark/rdd.py,RDD.saveAsNewAPIHadoopDataset,"def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        """"""
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,
                                                    keyConverter, valueConverter, True)",python,"def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        """"""
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsHadoopDataset(pickledRDD._jrdd, True, jconf,
                                                    keyConverter, valueConverter, True)","['def', 'saveAsNewAPIHadoopDataset', '(', 'self', ',', 'conf', ',', 'keyConverter', '=', 'None', ',', 'valueConverter', '=', 'None', ')', ':', 'jconf', '=', 'self', '.', 'ctx', '.', '_dictToJavaMap', '(', 'conf', ')', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsHadoopDataset', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'jconf', ',', 'keyConverter', ',', 'valueConverter', ',', 'True', ')']","Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are
        converted for output using either user specified converters or, by default,
        L{org.apache.spark.api.python.JavaToWritableConverter}.

        :param conf: Hadoop job configuration, passed in as a dict
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)","['Output', 'a', 'Python', 'RDD', 'of', 'key', '-', 'value', 'pairs', '(', 'of', 'form', 'C', '{', 'RDD', '[', '(', 'K', 'V', ')', ']', '}', ')', 'to', 'any', 'Hadoop', 'file', 'system', 'using', 'the', 'new', 'Hadoop', 'OutputFormat', 'API', '(', 'mapreduce', 'package', ')', '.', 'Keys', '/', 'values', 'are', 'converted', 'for', 'output', 'using', 'either', 'user', 'specified', 'converters', 'or', 'by', 'default', 'L', '{', 'org', '.', 'apache', '.', 'spark', '.', 'api', '.', 'python', '.', 'JavaToWritableConverter', '}', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1398-L1412,train,"Save a Python RDD of key - value pairs to any Hadoop file
        system using the new Hadoop OutputFormat API."
apache/spark,python/pyspark/rdd.py,RDD.saveAsNewAPIHadoopFile,"def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,
                               keyConverter=None, valueConverter=None, conf=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. ""org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat"")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. ""org.apache.hadoop.io.IntWritable"", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. ""org.apache.hadoop.io.Text"", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
        """"""
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,
                                                       outputFormatClass,
                                                       keyClass, valueClass,
                                                       keyConverter, valueConverter, jconf)",python,"def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,
                               keyConverter=None, valueConverter=None, conf=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. ""org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat"")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. ""org.apache.hadoop.io.IntWritable"", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. ""org.apache.hadoop.io.Text"", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)
        """"""
        jconf = self.ctx._dictToJavaMap(conf)
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsNewAPIHadoopFile(pickledRDD._jrdd, True, path,
                                                       outputFormatClass,
                                                       keyClass, valueClass,
                                                       keyConverter, valueConverter, jconf)","['def', 'saveAsNewAPIHadoopFile', '(', 'self', ',', 'path', ',', 'outputFormatClass', ',', 'keyClass', '=', 'None', ',', 'valueClass', '=', 'None', ',', 'keyConverter', '=', 'None', ',', 'valueConverter', '=', 'None', ',', 'conf', '=', 'None', ')', ':', 'jconf', '=', 'self', '.', 'ctx', '.', '_dictToJavaMap', '(', 'conf', ')', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsNewAPIHadoopFile', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'path', ',', 'outputFormatClass', ',', 'keyClass', ',', 'valueClass', ',', 'keyConverter', ',', 'valueConverter', ',', 'jconf', ')']","Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types
        will be inferred if not specified. Keys and values are converted for output using either
        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The
        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext
        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.

        :param path: path to Hadoop file
        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat
               (e.g. ""org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat"")
        :param keyClass: fully qualified classname of key Writable class
               (e.g. ""org.apache.hadoop.io.IntWritable"", None by default)
        :param valueClass: fully qualified classname of value Writable class
               (e.g. ""org.apache.hadoop.io.Text"", None by default)
        :param keyConverter: (None by default)
        :param valueConverter: (None by default)
        :param conf: Hadoop job configuration, passed in as a dict (None by default)","['Output', 'a', 'Python', 'RDD', 'of', 'key', '-', 'value', 'pairs', '(', 'of', 'form', 'C', '{', 'RDD', '[', '(', 'K', 'V', ')', ']', '}', ')', 'to', 'any', 'Hadoop', 'file', 'system', 'using', 'the', 'new', 'Hadoop', 'OutputFormat', 'API', '(', 'mapreduce', 'package', ')', '.', 'Key', 'and', 'value', 'types', 'will', 'be', 'inferred', 'if', 'not', 'specified', '.', 'Keys', 'and', 'values', 'are', 'converted', 'for', 'output', 'using', 'either', 'user', 'specified', 'converters', 'or', 'L', '{', 'org', '.', 'apache', '.', 'spark', '.', 'api', '.', 'python', '.', 'JavaToWritableConverter', '}', '.', 'The', 'C', '{', 'conf', '}', 'is', 'applied', 'on', 'top', 'of', 'the', 'base', 'Hadoop', 'conf', 'associated', 'with', 'the', 'SparkContext', 'of', 'this', 'RDD', 'to', 'create', 'a', 'merged', 'Hadoop', 'MapReduce', 'job', 'configuration', 'for', 'saving', 'the', 'data', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1414-L1440,train,Save the current RDD to a new Hadoop file using the new API.
apache/spark,python/pyspark/rdd.py,RDD.saveAsSequenceFile,"def saveAsSequenceFile(self, path, compressionCodecClass=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
        """"""
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,
                                                   path, compressionCodecClass)",python,"def saveAsSequenceFile(self, path, compressionCodecClass=None):
        """"""
        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)
        """"""
        pickledRDD = self._pickled()
        self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,
                                                   path, compressionCodecClass)","['def', 'saveAsSequenceFile', '(', 'self', ',', 'path', ',', 'compressionCodecClass', '=', 'None', ')', ':', 'pickledRDD', '=', 'self', '.', '_pickled', '(', ')', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'saveAsSequenceFile', '(', 'pickledRDD', '.', '_jrdd', ',', 'True', ',', 'path', ',', 'compressionCodecClass', ')']","Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file
        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the
        RDD's key and value types. The mechanism is as follows:

            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.
            2. Keys and values of this Java RDD are converted to Writables and written out.

        :param path: path to sequence file
        :param compressionCodecClass: (None by default)","['Output', 'a', 'Python', 'RDD', 'of', 'key', '-', 'value', 'pairs', '(', 'of', 'form', 'C', '{', 'RDD', '[', '(', 'K', 'V', ')', ']', '}', ')', 'to', 'any', 'Hadoop', 'file', 'system', 'using', 'the', 'L', '{', 'org', '.', 'apache', '.', 'hadoop', '.', 'io', '.', 'Writable', '}', 'types', 'that', 'we', 'convert', 'from', 'the', 'RDD', 's', 'key', 'and', 'value', 'types', '.', 'The', 'mechanism', 'is', 'as', 'follows', ':']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1489-L1503,train,Save the current RDD to a sequence file.
apache/spark,python/pyspark/rdd.py,RDD.saveAsPickleFile,"def saveAsPickleFile(self, path, batchSize=10):
        """"""
        Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
        """"""
        if batchSize == 0:
            ser = AutoBatchedSerializer(PickleSerializer())
        else:
            ser = BatchedSerializer(PickleSerializer(), batchSize)
        self._reserialize(ser)._jrdd.saveAsObjectFile(path)",python,"def saveAsPickleFile(self, path, batchSize=10):
        """"""
        Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']
        """"""
        if batchSize == 0:
            ser = AutoBatchedSerializer(PickleSerializer())
        else:
            ser = BatchedSerializer(PickleSerializer(), batchSize)
        self._reserialize(ser)._jrdd.saveAsObjectFile(path)","['def', 'saveAsPickleFile', '(', 'self', ',', 'path', ',', 'batchSize', '=', '10', ')', ':', 'if', 'batchSize', '==', '0', ':', 'ser', '=', 'AutoBatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', 'else', ':', 'ser', '=', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', 'self', '.', '_reserialize', '(', 'ser', ')', '.', '_jrdd', '.', 'saveAsObjectFile', '(', 'path', ')']","Save this RDD as a SequenceFile of serialized objects. The serializer
        used is L{pyspark.serializers.PickleSerializer}, default batch size
        is 10.

        >>> tmpFile = NamedTemporaryFile(delete=True)
        >>> tmpFile.close()
        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)
        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())
        ['1', '2', 'rdd', 'spark']","['Save', 'this', 'RDD', 'as', 'a', 'SequenceFile', 'of', 'serialized', 'objects', '.', 'The', 'serializer', 'used', 'is', 'L', '{', 'pyspark', '.', 'serializers', '.', 'PickleSerializer', '}', 'default', 'batch', 'size', 'is', '10', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1505-L1521,train,Save this RDD as a PickleFile.
apache/spark,python/pyspark/rdd.py,RDD.saveAsTextFile,"def saveAsTextFile(self, path, compressionCodecClass=None):
        """"""
        Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            ""org.apache.hadoop.io.compress.GzipCodec""

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + ""/part-0000*""))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + ""/part-0000*""))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = ""org.apache.hadoop.io.compress.GzipCodec""
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + ""/part*.gz""), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
        """"""
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode(""utf-8"")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)",python,"def saveAsTextFile(self, path, compressionCodecClass=None):
        """"""
        Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            ""org.apache.hadoop.io.compress.GzipCodec""

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + ""/part-0000*""))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + ""/part-0000*""))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = ""org.apache.hadoop.io.compress.GzipCodec""
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + ""/part*.gz""), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'
        """"""
        def func(split, iterator):
            for x in iterator:
                if not isinstance(x, (unicode, bytes)):
                    x = unicode(x)
                if isinstance(x, unicode):
                    x = x.encode(""utf-8"")
                yield x
        keyed = self.mapPartitionsWithIndex(func)
        keyed._bypass_serializer = True
        if compressionCodecClass:
            compressionCodec = self.ctx._jvm.java.lang.Class.forName(compressionCodecClass)
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)
        else:
            keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)","['def', 'saveAsTextFile', '(', 'self', ',', 'path', ',', 'compressionCodecClass', '=', 'None', ')', ':', 'def', 'func', '(', 'split', ',', 'iterator', ')', ':', 'for', 'x', 'in', 'iterator', ':', 'if', 'not', 'isinstance', '(', 'x', ',', '(', 'unicode', ',', 'bytes', ')', ')', ':', 'x', '=', 'unicode', '(', 'x', ')', 'if', 'isinstance', '(', 'x', ',', 'unicode', ')', ':', 'x', '=', 'x', '.', 'encode', '(', '""utf-8""', ')', 'yield', 'x', 'keyed', '=', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')', 'keyed', '.', '_bypass_serializer', '=', 'True', 'if', 'compressionCodecClass', ':', 'compressionCodec', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'java', '.', 'lang', '.', 'Class', '.', 'forName', '(', 'compressionCodecClass', ')', 'keyed', '.', '_jrdd', '.', 'map', '(', 'self', '.', 'ctx', '.', '_jvm', '.', 'BytesToString', '(', ')', ')', '.', 'saveAsTextFile', '(', 'path', ',', 'compressionCodec', ')', 'else', ':', 'keyed', '.', '_jrdd', '.', 'map', '(', 'self', '.', 'ctx', '.', '_jvm', '.', 'BytesToString', '(', ')', ')', '.', 'saveAsTextFile', '(', 'path', ')']","Save this RDD as a text file, using string representations of elements.

        @param path: path to text file
        @param compressionCodecClass: (None by default) string i.e.
            ""org.apache.hadoop.io.compress.GzipCodec""

        >>> tempFile = NamedTemporaryFile(delete=True)
        >>> tempFile.close()
        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)
        >>> from fileinput import input
        >>> from glob import glob
        >>> ''.join(sorted(input(glob(tempFile.name + ""/part-0000*""))))
        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'

        Empty lines are tolerated when saving to text files.

        >>> tempFile2 = NamedTemporaryFile(delete=True)
        >>> tempFile2.close()
        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)
        >>> ''.join(sorted(input(glob(tempFile2.name + ""/part-0000*""))))
        '\\n\\n\\nbar\\nfoo\\n'

        Using compressionCodecClass

        >>> tempFile3 = NamedTemporaryFile(delete=True)
        >>> tempFile3.close()
        >>> codec = ""org.apache.hadoop.io.compress.GzipCodec""
        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)
        >>> from fileinput import input, hook_compressed
        >>> result = sorted(input(glob(tempFile3.name + ""/part*.gz""), openhook=hook_compressed))
        >>> b''.join(result).decode('utf-8')
        u'bar\\nfoo\\n'","['Save', 'this', 'RDD', 'as', 'a', 'text', 'file', 'using', 'string', 'representations', 'of', 'elements', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1524-L1572,train,Save this RDD as a text file using string representations of elements.
apache/spark,python/pyspark/rdd.py,RDD.reduceByKey,"def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
        """"""
        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)",python,"def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]
        """"""
        return self.combineByKey(lambda x: x, func, func, numPartitions, partitionFunc)","['def', 'reduceByKey', '(', 'self', ',', 'func', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'x', ':', 'x', ',', 'func', ',', 'func', ',', 'numPartitions', ',', 'partitionFunc', ')']","Merge the values for each key using an associative and commutative reduce function.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        Output will be partitioned with C{numPartitions} partitions, or
        the default parallelism level if C{numPartitions} is not specified.
        Default partitioner is hash-partition.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKey(add).collect())
        [('a', 2), ('b', 1)]","['Merge', 'the', 'values', 'for', 'each', 'key', 'using', 'an', 'associative', 'and', 'commutative', 'reduce', 'function', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1611-L1627,train,Return a new RDD with the values for each key using an associative and commutative reduce function.
apache/spark,python/pyspark/rdd.py,RDD.reduceByKeyLocally,"def reduceByKeyLocally(self, func):
        """"""
        Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
        """"""
        func = fail_on_stopiteration(func)

        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)",python,"def reduceByKeyLocally(self, func):
        """"""
        Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]
        """"""
        func = fail_on_stopiteration(func)

        def reducePartition(iterator):
            m = {}
            for k, v in iterator:
                m[k] = func(m[k], v) if k in m else v
            yield m

        def mergeMaps(m1, m2):
            for k, v in m2.items():
                m1[k] = func(m1[k], v) if k in m1 else v
            return m1
        return self.mapPartitions(reducePartition).reduce(mergeMaps)","['def', 'reduceByKeyLocally', '(', 'self', ',', 'func', ')', ':', 'func', '=', 'fail_on_stopiteration', '(', 'func', ')', 'def', 'reducePartition', '(', 'iterator', ')', ':', 'm', '=', '{', '}', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'm', '[', 'k', ']', '=', 'func', '(', 'm', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'm', 'else', 'v', 'yield', 'm', 'def', 'mergeMaps', '(', 'm1', ',', 'm2', ')', ':', 'for', 'k', ',', 'v', 'in', 'm2', '.', 'items', '(', ')', ':', 'm1', '[', 'k', ']', '=', 'func', '(', 'm1', '[', 'k', ']', ',', 'v', ')', 'if', 'k', 'in', 'm1', 'else', 'v', 'return', 'm1', 'return', 'self', '.', 'mapPartitions', '(', 'reducePartition', ')', '.', 'reduce', '(', 'mergeMaps', ')']","Merge the values for each key using an associative and commutative reduce function, but
        return the results immediately to the master as a dictionary.

        This will also perform the merging locally on each mapper before
        sending results to a reducer, similarly to a ""combiner"" in MapReduce.

        >>> from operator import add
        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.reduceByKeyLocally(add).items())
        [('a', 2), ('b', 1)]","['Merge', 'the', 'values', 'for', 'each', 'key', 'using', 'an', 'associative', 'and', 'commutative', 'reduce', 'function', 'but', 'return', 'the', 'results', 'immediately', 'to', 'the', 'master', 'as', 'a', 'dictionary', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1629-L1654,train,Return a new DStream with the values for each key using an associative and commutative reduce function.
apache/spark,python/pyspark/rdd.py,RDD.partitionBy,"def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """"""
        Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self

        # Transferring O(n) objects to Java is too expensive.
        # Instead, we'll form the hash buckets in Python,
        # transferring O(numPartitions) objects to Java.
        # Each object is a (splitNumber, [objects]) pair.
        # In order to avoid too huge objects, the objects are
        # grouped into chunks.
        outputSerializer = self.ctx._unbatched_serializer

        limit = (_parse_memory(self.ctx._conf.get(
            ""spark.python.worker.memory"", ""512m"")) / 2)

        def add_shuffle_key(split, iterator):

            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)

            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1

                # check used memory and avg size of chunk of objects
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)

                    avg = int(size / n) >> 20
                    # let 1M < avg < 10M
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0

            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)

        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd",python,"def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """"""
        Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()
        partitioner = Partitioner(numPartitions, partitionFunc)
        if self.partitioner == partitioner:
            return self

        # Transferring O(n) objects to Java is too expensive.
        # Instead, we'll form the hash buckets in Python,
        # transferring O(numPartitions) objects to Java.
        # Each object is a (splitNumber, [objects]) pair.
        # In order to avoid too huge objects, the objects are
        # grouped into chunks.
        outputSerializer = self.ctx._unbatched_serializer

        limit = (_parse_memory(self.ctx._conf.get(
            ""spark.python.worker.memory"", ""512m"")) / 2)

        def add_shuffle_key(split, iterator):

            buckets = defaultdict(list)
            c, batch = 0, min(10 * numPartitions, 1000)

            for k, v in iterator:
                buckets[partitionFunc(k) % numPartitions].append((k, v))
                c += 1

                # check used memory and avg size of chunk of objects
                if (c % 1000 == 0 and get_used_memory() > limit
                        or c > batch):
                    n, size = len(buckets), 0
                    for split in list(buckets.keys()):
                        yield pack_long(split)
                        d = outputSerializer.dumps(buckets[split])
                        del buckets[split]
                        yield d
                        size += len(d)

                    avg = int(size / n) >> 20
                    # let 1M < avg < 10M
                    if avg < 1:
                        batch *= 1.5
                    elif avg > 10:
                        batch = max(int(batch / 1.5), 1)
                    c = 0

            for split, items in buckets.items():
                yield pack_long(split)
                yield outputSerializer.dumps(items)

        keyed = self.mapPartitionsWithIndex(add_shuffle_key, preservesPartitioning=True)
        keyed._bypass_serializer = True
        with SCCallSiteSync(self.context) as css:
            pairRDD = self.ctx._jvm.PairwiseRDD(
                keyed._jrdd.rdd()).asJavaPairRDD()
            jpartitioner = self.ctx._jvm.PythonPartitioner(numPartitions,
                                                           id(partitionFunc))
        jrdd = self.ctx._jvm.PythonRDD.valueOfPair(pairRDD.partitionBy(jpartitioner))
        rdd = RDD(jrdd, self.ctx, BatchedSerializer(outputSerializer))
        rdd.partitioner = partitioner
        return rdd","['def', 'partitionBy', '(', 'self', ',', 'numPartitions', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'partitioner', '=', 'Partitioner', '(', 'numPartitions', ',', 'partitionFunc', ')', 'if', 'self', '.', 'partitioner', '==', 'partitioner', ':', 'return', 'self', '# Transferring O(n) objects to Java is too expensive.', ""# Instead, we'll form the hash buckets in Python,"", '# transferring O(numPartitions) objects to Java.', '# Each object is a (splitNumber, [objects]) pair.', '# In order to avoid too huge objects, the objects are', '# grouped into chunks.', 'outputSerializer', '=', 'self', '.', 'ctx', '.', '_unbatched_serializer', 'limit', '=', '(', '_parse_memory', '(', 'self', '.', 'ctx', '.', '_conf', '.', 'get', '(', '""spark.python.worker.memory""', ',', '""512m""', ')', ')', '/', '2', ')', 'def', 'add_shuffle_key', '(', 'split', ',', 'iterator', ')', ':', 'buckets', '=', 'defaultdict', '(', 'list', ')', 'c', ',', 'batch', '=', '0', ',', 'min', '(', '10', '*', 'numPartitions', ',', '1000', ')', 'for', 'k', ',', 'v', 'in', 'iterator', ':', 'buckets', '[', 'partitionFunc', '(', 'k', ')', '%', 'numPartitions', ']', '.', 'append', '(', '(', 'k', ',', 'v', ')', ')', 'c', '+=', '1', '# check used memory and avg size of chunk of objects', 'if', '(', 'c', '%', '1000', '==', '0', 'and', 'get_used_memory', '(', ')', '>', 'limit', 'or', 'c', '>', 'batch', ')', ':', 'n', ',', 'size', '=', 'len', '(', 'buckets', ')', ',', '0', 'for', 'split', 'in', 'list', '(', 'buckets', '.', 'keys', '(', ')', ')', ':', 'yield', 'pack_long', '(', 'split', ')', 'd', '=', 'outputSerializer', '.', 'dumps', '(', 'buckets', '[', 'split', ']', ')', 'del', 'buckets', '[', 'split', ']', 'yield', 'd', 'size', '+=', 'len', '(', 'd', ')', 'avg', '=', 'int', '(', 'size', '/', 'n', ')', '>>', '20', '# let 1M < avg < 10M', 'if', 'avg', '<', '1', ':', 'batch', '*=', '1.5', 'elif', 'avg', '>', '10', ':', 'batch', '=', 'max', '(', 'int', '(', 'batch', '/', '1.5', ')', ',', '1', ')', 'c', '=', '0', 'for', 'split', ',', 'items', 'in', 'buckets', '.', 'items', '(', ')', ':', 'yield', 'pack_long', '(', 'split', ')', 'yield', 'outputSerializer', '.', 'dumps', '(', 'items', ')', 'keyed', '=', 'self', '.', 'mapPartitionsWithIndex', '(', 'add_shuffle_key', ',', 'preservesPartitioning', '=', 'True', ')', 'keyed', '.', '_bypass_serializer', '=', 'True', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'pairRDD', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PairwiseRDD', '(', 'keyed', '.', '_jrdd', '.', 'rdd', '(', ')', ')', '.', 'asJavaPairRDD', '(', ')', 'jpartitioner', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonPartitioner', '(', 'numPartitions', ',', 'id', '(', 'partitionFunc', ')', ')', 'jrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'valueOfPair', '(', 'pairRDD', '.', 'partitionBy', '(', 'jpartitioner', ')', ')', 'rdd', '=', 'RDD', '(', 'jrdd', ',', 'self', '.', 'ctx', ',', 'BatchedSerializer', '(', 'outputSerializer', ')', ')', 'rdd', '.', 'partitioner', '=', 'partitioner', 'return', 'rdd']","Return a copy of the RDD partitioned using the specified partitioner.

        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))
        >>> sets = pairs.partitionBy(2).glom().collect()
        >>> len(set(sets[0]).intersection(set(sets[1])))
        0","['Return', 'a', 'copy', 'of', 'the', 'RDD', 'partitioned', 'using', 'the', 'specified', 'partitioner', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1742-L1810,train,Return a copy of the RDD partitioned by the specified partitioner.
apache/spark,python/pyspark/rdd.py,RDD.combineByKey,"def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        """"""
        Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a ""combined
        type"" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()

        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)",python,"def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None, partitionFunc=portable_hash):
        """"""
        Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a ""combined
        type"" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]
        """"""
        if numPartitions is None:
            numPartitions = self._defaultReducePartitions()

        serializer = self.ctx.serializer
        memory = self._memory_limit()
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combineLocally(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combineLocally, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def _mergeCombiners(iterator):
            merger = ExternalMerger(agg, memory, serializer)
            merger.mergeCombiners(iterator)
            return merger.items()

        return shuffled.mapPartitions(_mergeCombiners, preservesPartitioning=True)","['def', 'combineByKey', '(', 'self', ',', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_defaultReducePartitions', '(', ')', 'serializer', '=', 'self', '.', 'ctx', '.', 'serializer', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'agg', '=', 'Aggregator', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ')', 'def', 'combineLocally', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', '*', '0.9', ',', 'serializer', ')', 'merger', '.', 'mergeValues', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'locally_combined', '=', 'self', '.', 'mapPartitions', '(', 'combineLocally', ',', 'preservesPartitioning', '=', 'True', ')', 'shuffled', '=', 'locally_combined', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', 'def', '_mergeCombiners', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', ',', 'serializer', ')', 'merger', '.', 'mergeCombiners', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'return', 'shuffled', '.', 'mapPartitions', '(', '_mergeCombiners', ',', 'preservesPartitioning', '=', 'True', ')']","Generic function to combine the elements for each key using a custom
        set of aggregation functions.

        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a ""combined
        type"" C.

        Users provide three functions:

            - C{createCombiner}, which turns a V into a C (e.g., creates
              a one-element list)
            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of
              a list)
            - C{mergeCombiners}, to combine two C's into a single one (e.g., merges
              the lists)

        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to
        modify and return their first argument instead of creating a new C.

        In addition, users can control the partitioning of the output RDD.

        .. note:: V and C can be different -- for example, one might group an RDD of type
            (Int, Int) into an RDD of type (Int, List[Int]).

        >>> x = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 2)])
        >>> def to_list(a):
        ...     return [a]
        ...
        >>> def append(a, b):
        ...     a.append(b)
        ...     return a
        ...
        >>> def extend(a, b):
        ...     a.extend(b)
        ...     return a
        ...
        >>> sorted(x.combineByKey(to_list, append, extend).collect())
        [('a', [1, 2]), ('b', [1])]","['Generic', 'function', 'to', 'combine', 'the', 'elements', 'for', 'each', 'key', 'using', 'a', 'custom', 'set', 'of', 'aggregation', 'functions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1813-L1874,train,This function returns an RDD of elements from the first entry in the RDD that are combined with the second entry in the RDD.
apache/spark,python/pyspark/rdd.py,RDD.aggregateByKey,"def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,
                       partitionFunc=portable_hash):
        """"""
        Aggregate the values of each key, using given combine functions and a neutral
        ""zero value"". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
        """"""
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(
            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)",python,"def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,
                       partitionFunc=portable_hash):
        """"""
        Aggregate the values of each key, using given combine functions and a neutral
        ""zero value"". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.
        """"""
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(
            lambda v: seqFunc(createZero(), v), seqFunc, combFunc, numPartitions, partitionFunc)","['def', 'aggregateByKey', '(', 'self', ',', 'zeroValue', ',', 'seqFunc', ',', 'combFunc', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createZero', '(', ')', ':', 'return', 'copy', '.', 'deepcopy', '(', 'zeroValue', ')', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'v', ':', 'seqFunc', '(', 'createZero', '(', ')', ',', 'v', ')', ',', 'seqFunc', ',', 'combFunc', ',', 'numPartitions', ',', 'partitionFunc', ')']","Aggregate the values of each key, using given combine functions and a neutral
        ""zero value"". This function can return a different result type, U, than the type
        of the values in this RDD, V. Thus, we need one operation for merging a V into
        a U and one operation for merging two U's, The former operation is used for merging
        values within a partition, and the latter is used for merging values between
        partitions. To avoid memory allocation, both of these functions are
        allowed to modify and return their first argument instead of creating a new U.","['Aggregate', 'the', 'values', 'of', 'each', 'key', 'using', 'given', 'combine', 'functions', 'and', 'a', 'neutral', 'zero', 'value', '.', 'This', 'function', 'can', 'return', 'a', 'different', 'result', 'type', 'U', 'than', 'the', 'type', 'of', 'the', 'values', 'in', 'this', 'RDD', 'V', '.', 'Thus', 'we', 'need', 'one', 'operation', 'for', 'merging', 'a', 'V', 'into', 'a', 'U', 'and', 'one', 'operation', 'for', 'merging', 'two', 'U', 's', 'The', 'former', 'operation', 'is', 'used', 'for', 'merging', 'values', 'within', 'a', 'partition', 'and', 'the', 'latter', 'is', 'used', 'for', 'merging', 'values', 'between', 'partitions', '.', 'To', 'avoid', 'memory', 'allocation', 'both', 'of', 'these', 'functions', 'are', 'allowed', 'to', 'modify', 'and', 'return', 'their', 'first', 'argument', 'instead', 'of', 'creating', 'a', 'new', 'U', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1876-L1891,train,"Aggregate the values of each key using given combine functions and a neutral
        zero value."
apache/spark,python/pyspark/rdd.py,RDD.foldByKey,"def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Merge the values for each key using an associative function ""func""
        and a neutral ""zeroValue"" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
        """"""
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,
                                 partitionFunc)",python,"def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Merge the values for each key using an associative function ""func""
        and a neutral ""zeroValue"" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]
        """"""
        def createZero():
            return copy.deepcopy(zeroValue)

        return self.combineByKey(lambda v: func(createZero(), v), func, func, numPartitions,
                                 partitionFunc)","['def', 'foldByKey', '(', 'self', ',', 'zeroValue', ',', 'func', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createZero', '(', ')', ':', 'return', 'copy', '.', 'deepcopy', '(', 'zeroValue', ')', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'v', ':', 'func', '(', 'createZero', '(', ')', ',', 'v', ')', ',', 'func', ',', 'func', ',', 'numPartitions', ',', 'partitionFunc', ')']","Merge the values for each key using an associative function ""func""
        and a neutral ""zeroValue"" which may be added to the result an
        arbitrary number of times, and must not change the result
        (e.g., 0 for addition, or 1 for multiplication.).

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> from operator import add
        >>> sorted(rdd.foldByKey(0, add).collect())
        [('a', 2), ('b', 1)]","['Merge', 'the', 'values', 'for', 'each', 'key', 'using', 'an', 'associative', 'function', 'func', 'and', 'a', 'neutral', 'zeroValue', 'which', 'may', 'be', 'added', 'to', 'the', 'result', 'an', 'arbitrary', 'number', 'of', 'times', 'and', 'must', 'not', 'change', 'the', 'result', '(', 'e', '.', 'g', '.', '0', 'for', 'addition', 'or', '1', 'for', 'multiplication', '.', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1893-L1909,train,Return a new table with the values for each key in the table grouped by func.
apache/spark,python/pyspark/rdd.py,RDD.groupByKey,"def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Group the values for each key in the RDD into a single sequence.
        Hash-partitions the resulting RDD with numPartitions partitions.

        .. note:: If you are grouping in order to perform an aggregation (such as a
            sum or average) over each key, using reduceByKey or aggregateByKey will
            provide much better performance.

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.groupByKey().mapValues(len).collect())
        [('a', 2), ('b', 1)]
        >>> sorted(rdd.groupByKey().mapValues(list).collect())
        [('a', [1, 1]), ('b', [1])]
        """"""
        def createCombiner(x):
            return [x]

        def mergeValue(xs, x):
            xs.append(x)
            return xs

        def mergeCombiners(a, b):
            a.extend(b)
            return a

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combine(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def groupByKey(it):
            merger = ExternalGroupBy(agg, memory, serializer)
            merger.mergeCombiners(it)
            return merger.items()

        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)",python,"def groupByKey(self, numPartitions=None, partitionFunc=portable_hash):
        """"""
        Group the values for each key in the RDD into a single sequence.
        Hash-partitions the resulting RDD with numPartitions partitions.

        .. note:: If you are grouping in order to perform an aggregation (such as a
            sum or average) over each key, using reduceByKey or aggregateByKey will
            provide much better performance.

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.groupByKey().mapValues(len).collect())
        [('a', 2), ('b', 1)]
        >>> sorted(rdd.groupByKey().mapValues(list).collect())
        [('a', [1, 1]), ('b', [1])]
        """"""
        def createCombiner(x):
            return [x]

        def mergeValue(xs, x):
            xs.append(x)
            return xs

        def mergeCombiners(a, b):
            a.extend(b)
            return a

        memory = self._memory_limit()
        serializer = self._jrdd_deserializer
        agg = Aggregator(createCombiner, mergeValue, mergeCombiners)

        def combine(iterator):
            merger = ExternalMerger(agg, memory * 0.9, serializer)
            merger.mergeValues(iterator)
            return merger.items()

        locally_combined = self.mapPartitions(combine, preservesPartitioning=True)
        shuffled = locally_combined.partitionBy(numPartitions, partitionFunc)

        def groupByKey(it):
            merger = ExternalGroupBy(agg, memory, serializer)
            merger.mergeCombiners(it)
            return merger.items()

        return shuffled.mapPartitions(groupByKey, True).mapValues(ResultIterable)","['def', 'groupByKey', '(', 'self', ',', 'numPartitions', '=', 'None', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'def', 'createCombiner', '(', 'x', ')', ':', 'return', '[', 'x', ']', 'def', 'mergeValue', '(', 'xs', ',', 'x', ')', ':', 'xs', '.', 'append', '(', 'x', ')', 'return', 'xs', 'def', 'mergeCombiners', '(', 'a', ',', 'b', ')', ':', 'a', '.', 'extend', '(', 'b', ')', 'return', 'a', 'memory', '=', 'self', '.', '_memory_limit', '(', ')', 'serializer', '=', 'self', '.', '_jrdd_deserializer', 'agg', '=', 'Aggregator', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ')', 'def', 'combine', '(', 'iterator', ')', ':', 'merger', '=', 'ExternalMerger', '(', 'agg', ',', 'memory', '*', '0.9', ',', 'serializer', ')', 'merger', '.', 'mergeValues', '(', 'iterator', ')', 'return', 'merger', '.', 'items', '(', ')', 'locally_combined', '=', 'self', '.', 'mapPartitions', '(', 'combine', ',', 'preservesPartitioning', '=', 'True', ')', 'shuffled', '=', 'locally_combined', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', 'def', 'groupByKey', '(', 'it', ')', ':', 'merger', '=', 'ExternalGroupBy', '(', 'agg', ',', 'memory', ',', 'serializer', ')', 'merger', '.', 'mergeCombiners', '(', 'it', ')', 'return', 'merger', '.', 'items', '(', ')', 'return', 'shuffled', '.', 'mapPartitions', '(', 'groupByKey', ',', 'True', ')', '.', 'mapValues', '(', 'ResultIterable', ')']","Group the values for each key in the RDD into a single sequence.
        Hash-partitions the resulting RDD with numPartitions partitions.

        .. note:: If you are grouping in order to perform an aggregation (such as a
            sum or average) over each key, using reduceByKey or aggregateByKey will
            provide much better performance.

        >>> rdd = sc.parallelize([(""a"", 1), (""b"", 1), (""a"", 1)])
        >>> sorted(rdd.groupByKey().mapValues(len).collect())
        [('a', 2), ('b', 1)]
        >>> sorted(rdd.groupByKey().mapValues(list).collect())
        [('a', [1, 1]), ('b', [1])]","['Group', 'the', 'values', 'for', 'each', 'key', 'in', 'the', 'RDD', 'into', 'a', 'single', 'sequence', '.', 'Hash', '-', 'partitions', 'the', 'resulting', 'RDD', 'with', 'numPartitions', 'partitions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1915-L1958,train,Return an RDD of all the values for each key in the RDD.
apache/spark,python/pyspark/rdd.py,RDD.flatMapValues,"def flatMapValues(self, f):
        """"""
        Pass each value in the key-value pair RDD through a flatMap function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""x"", ""y"", ""z""]), (""b"", [""p"", ""r""])])
        >>> def f(x): return x
        >>> x.flatMapValues(f).collect()
        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
        """"""
        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))
        return self.flatMap(flat_map_fn, preservesPartitioning=True)",python,"def flatMapValues(self, f):
        """"""
        Pass each value in the key-value pair RDD through a flatMap function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""x"", ""y"", ""z""]), (""b"", [""p"", ""r""])])
        >>> def f(x): return x
        >>> x.flatMapValues(f).collect()
        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]
        """"""
        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))
        return self.flatMap(flat_map_fn, preservesPartitioning=True)","['def', 'flatMapValues', '(', 'self', ',', 'f', ')', ':', 'flat_map_fn', '=', 'lambda', 'kv', ':', '(', '(', 'kv', '[', '0', ']', ',', 'x', ')', 'for', 'x', 'in', 'f', '(', 'kv', '[', '1', ']', ')', ')', 'return', 'self', '.', 'flatMap', '(', 'flat_map_fn', ',', 'preservesPartitioning', '=', 'True', ')']","Pass each value in the key-value pair RDD through a flatMap function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""x"", ""y"", ""z""]), (""b"", [""p"", ""r""])])
        >>> def f(x): return x
        >>> x.flatMapValues(f).collect()
        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]","['Pass', 'each', 'value', 'in', 'the', 'key', '-', 'value', 'pair', 'RDD', 'through', 'a', 'flatMap', 'function', 'without', 'changing', 'the', 'keys', ';', 'this', 'also', 'retains', 'the', 'original', 'RDD', 's', 'partitioning', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1960-L1972,train,Return a new RDD with the elements of each key - value pair obtained by applying a function to each value in the key - value pair.
apache/spark,python/pyspark/rdd.py,RDD.mapValues,"def mapValues(self, f):
        """"""
        Pass each value in the key-value pair RDD through a map function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""apple"", ""banana"", ""lemon""]), (""b"", [""grapes""])])
        >>> def f(x): return len(x)
        >>> x.mapValues(f).collect()
        [('a', 3), ('b', 1)]
        """"""
        map_values_fn = lambda kv: (kv[0], f(kv[1]))
        return self.map(map_values_fn, preservesPartitioning=True)",python,"def mapValues(self, f):
        """"""
        Pass each value in the key-value pair RDD through a map function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""apple"", ""banana"", ""lemon""]), (""b"", [""grapes""])])
        >>> def f(x): return len(x)
        >>> x.mapValues(f).collect()
        [('a', 3), ('b', 1)]
        """"""
        map_values_fn = lambda kv: (kv[0], f(kv[1]))
        return self.map(map_values_fn, preservesPartitioning=True)","['def', 'mapValues', '(', 'self', ',', 'f', ')', ':', 'map_values_fn', '=', 'lambda', 'kv', ':', '(', 'kv', '[', '0', ']', ',', 'f', '(', 'kv', '[', '1', ']', ')', ')', 'return', 'self', '.', 'map', '(', 'map_values_fn', ',', 'preservesPartitioning', '=', 'True', ')']","Pass each value in the key-value pair RDD through a map function
        without changing the keys; this also retains the original RDD's
        partitioning.

        >>> x = sc.parallelize([(""a"", [""apple"", ""banana"", ""lemon""]), (""b"", [""grapes""])])
        >>> def f(x): return len(x)
        >>> x.mapValues(f).collect()
        [('a', 3), ('b', 1)]","['Pass', 'each', 'value', 'in', 'the', 'key', '-', 'value', 'pair', 'RDD', 'through', 'a', 'map', 'function', 'without', 'changing', 'the', 'keys', ';', 'this', 'also', 'retains', 'the', 'original', 'RDD', 's', 'partitioning', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L1974-L1986,train,Return an RDD with the values of each key - value pair mapped through a map function.
apache/spark,python/pyspark/rdd.py,RDD.sampleByKey,"def sampleByKey(self, withReplacement, fractions, seed=None):
        """"""
        Return a subset of this RDD sampled by key (via stratified sampling).
        Create a sample of this RDD using variable sampling rates for
        different keys as specified by fractions, a key to sampling rate map.

        >>> fractions = {""a"": 0.2, ""b"": 0.1}
        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))
        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())
        >>> 100 < len(sample[""a""]) < 300 and 50 < len(sample[""b""]) < 150
        True
        >>> max(sample[""a""]) <= 999 and min(sample[""a""]) >= 0
        True
        >>> max(sample[""b""]) <= 999 and min(sample[""b""]) >= 0
        True
        """"""
        for fraction in fractions.values():
            assert fraction >= 0.0, ""Negative fraction value: %s"" % fraction
        return self.mapPartitionsWithIndex(
            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)",python,"def sampleByKey(self, withReplacement, fractions, seed=None):
        """"""
        Return a subset of this RDD sampled by key (via stratified sampling).
        Create a sample of this RDD using variable sampling rates for
        different keys as specified by fractions, a key to sampling rate map.

        >>> fractions = {""a"": 0.2, ""b"": 0.1}
        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))
        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())
        >>> 100 < len(sample[""a""]) < 300 and 50 < len(sample[""b""]) < 150
        True
        >>> max(sample[""a""]) <= 999 and min(sample[""a""]) >= 0
        True
        >>> max(sample[""b""]) <= 999 and min(sample[""b""]) >= 0
        True
        """"""
        for fraction in fractions.values():
            assert fraction >= 0.0, ""Negative fraction value: %s"" % fraction
        return self.mapPartitionsWithIndex(
            RDDStratifiedSampler(withReplacement, fractions, seed).func, True)","['def', 'sampleByKey', '(', 'self', ',', 'withReplacement', ',', 'fractions', ',', 'seed', '=', 'None', ')', ':', 'for', 'fraction', 'in', 'fractions', '.', 'values', '(', ')', ':', 'assert', 'fraction', '>=', '0.0', ',', '""Negative fraction value: %s""', '%', 'fraction', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'RDDStratifiedSampler', '(', 'withReplacement', ',', 'fractions', ',', 'seed', ')', '.', 'func', ',', 'True', ')']","Return a subset of this RDD sampled by key (via stratified sampling).
        Create a sample of this RDD using variable sampling rates for
        different keys as specified by fractions, a key to sampling rate map.

        >>> fractions = {""a"": 0.2, ""b"": 0.1}
        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))
        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())
        >>> 100 < len(sample[""a""]) < 300 and 50 < len(sample[""b""]) < 150
        True
        >>> max(sample[""a""]) <= 999 and min(sample[""a""]) >= 0
        True
        >>> max(sample[""b""]) <= 999 and min(sample[""b""]) >= 0
        True","['Return', 'a', 'subset', 'of', 'this', 'RDD', 'sampled', 'by', 'key', '(', 'via', 'stratified', 'sampling', ')', '.', 'Create', 'a', 'sample', 'of', 'this', 'RDD', 'using', 'variable', 'sampling', 'rates', 'for', 'different', 'keys', 'as', 'specified', 'by', 'fractions', 'a', 'key', 'to', 'sampling', 'rate', 'map', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2016-L2035,train,Return a subset of this RDD with the values of the keys specified by fractions.
apache/spark,python/pyspark/rdd.py,RDD.subtractByKey,"def subtractByKey(self, other, numPartitions=None):
        """"""
        Return each (key, value) pair in C{self} that has no pair with matching
        key in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 2)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtractByKey(y).collect())
        [('b', 4), ('b', 5)]
        """"""
        def filter_func(pair):
            key, (val1, val2) = pair
            return val1 and not val2
        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])",python,"def subtractByKey(self, other, numPartitions=None):
        """"""
        Return each (key, value) pair in C{self} that has no pair with matching
        key in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 2)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtractByKey(y).collect())
        [('b', 4), ('b', 5)]
        """"""
        def filter_func(pair):
            key, (val1, val2) = pair
            return val1 and not val2
        return self.cogroup(other, numPartitions).filter(filter_func).flatMapValues(lambda x: x[0])","['def', 'subtractByKey', '(', 'self', ',', 'other', ',', 'numPartitions', '=', 'None', ')', ':', 'def', 'filter_func', '(', 'pair', ')', ':', 'key', ',', '(', 'val1', ',', 'val2', ')', '=', 'pair', 'return', 'val1', 'and', 'not', 'val2', 'return', 'self', '.', 'cogroup', '(', 'other', ',', 'numPartitions', ')', '.', 'filter', '(', 'filter_func', ')', '.', 'flatMapValues', '(', 'lambda', 'x', ':', 'x', '[', '0', ']', ')']","Return each (key, value) pair in C{self} that has no pair with matching
        key in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 2)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtractByKey(y).collect())
        [('b', 4), ('b', 5)]","['Return', 'each', '(', 'key', 'value', ')', 'pair', 'in', 'C', '{', 'self', '}', 'that', 'has', 'no', 'pair', 'with', 'matching', 'key', 'in', 'C', '{', 'other', '}', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2037-L2050,train,"Return each key value pair in self that has no pair with matching
        key in other."
apache/spark,python/pyspark/rdd.py,RDD.subtract,"def subtract(self, other, numPartitions=None):
        """"""
        Return each value in C{self} that is not contained in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 3)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtract(y).collect())
        [('a', 1), ('b', 4), ('b', 5)]
        """"""
        # note: here 'True' is just a placeholder
        rdd = other.map(lambda x: (x, True))
        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()",python,"def subtract(self, other, numPartitions=None):
        """"""
        Return each value in C{self} that is not contained in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 3)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtract(y).collect())
        [('a', 1), ('b', 4), ('b', 5)]
        """"""
        # note: here 'True' is just a placeholder
        rdd = other.map(lambda x: (x, True))
        return self.map(lambda x: (x, True)).subtractByKey(rdd, numPartitions).keys()","['def', 'subtract', '(', 'self', ',', 'other', ',', 'numPartitions', '=', 'None', ')', ':', ""# note: here 'True' is just a placeholder"", 'rdd', '=', 'other', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'True', ')', ')', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'x', ',', 'True', ')', ')', '.', 'subtractByKey', '(', 'rdd', ',', 'numPartitions', ')', '.', 'keys', '(', ')']","Return each value in C{self} that is not contained in C{other}.

        >>> x = sc.parallelize([(""a"", 1), (""b"", 4), (""b"", 5), (""a"", 3)])
        >>> y = sc.parallelize([(""a"", 3), (""c"", None)])
        >>> sorted(x.subtract(y).collect())
        [('a', 1), ('b', 4), ('b', 5)]","['Return', 'each', 'value', 'in', 'C', '{', 'self', '}', 'that', 'is', 'not', 'contained', 'in', 'C', '{', 'other', '}', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2052-L2063,train,Return each value in self that is not contained in other.
apache/spark,python/pyspark/rdd.py,RDD.coalesce,"def coalesce(self, numPartitions, shuffle=False):
        """"""
        Return a new RDD that is reduced into `numPartitions` partitions.

        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()
        [[1], [2, 3], [4, 5]]
        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
        [[1, 2, 3, 4, 5]]
        """"""
        if shuffle:
            # Decrease the batch size in order to distribute evenly the elements across output
            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.
            batchSize = min(10, self.ctx._batchSize or 1024)
            ser = BatchedSerializer(PickleSerializer(), batchSize)
            selfCopy = self._reserialize(ser)
            jrdd_deserializer = selfCopy._jrdd_deserializer
            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
        else:
            jrdd_deserializer = self._jrdd_deserializer
            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
        return RDD(jrdd, self.ctx, jrdd_deserializer)",python,"def coalesce(self, numPartitions, shuffle=False):
        """"""
        Return a new RDD that is reduced into `numPartitions` partitions.

        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()
        [[1], [2, 3], [4, 5]]
        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
        [[1, 2, 3, 4, 5]]
        """"""
        if shuffle:
            # Decrease the batch size in order to distribute evenly the elements across output
            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.
            batchSize = min(10, self.ctx._batchSize or 1024)
            ser = BatchedSerializer(PickleSerializer(), batchSize)
            selfCopy = self._reserialize(ser)
            jrdd_deserializer = selfCopy._jrdd_deserializer
            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
        else:
            jrdd_deserializer = self._jrdd_deserializer
            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
        return RDD(jrdd, self.ctx, jrdd_deserializer)","['def', 'coalesce', '(', 'self', ',', 'numPartitions', ',', 'shuffle', '=', 'False', ')', ':', 'if', 'shuffle', ':', '# Decrease the batch size in order to distribute evenly the elements across output', '# partitions. Otherwise, repartition will possibly produce highly skewed partitions.', 'batchSize', '=', 'min', '(', '10', ',', 'self', '.', 'ctx', '.', '_batchSize', 'or', '1024', ')', 'ser', '=', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', 'selfCopy', '=', 'self', '.', '_reserialize', '(', 'ser', ')', 'jrdd_deserializer', '=', 'selfCopy', '.', '_jrdd_deserializer', 'jrdd', '=', 'selfCopy', '.', '_jrdd', '.', 'coalesce', '(', 'numPartitions', ',', 'shuffle', ')', 'else', ':', 'jrdd_deserializer', '=', 'self', '.', '_jrdd_deserializer', 'jrdd', '=', 'self', '.', '_jrdd', '.', 'coalesce', '(', 'numPartitions', ',', 'shuffle', ')', 'return', 'RDD', '(', 'jrdd', ',', 'self', '.', 'ctx', ',', 'jrdd_deserializer', ')']","Return a new RDD that is reduced into `numPartitions` partitions.

        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()
        [[1], [2, 3], [4, 5]]
        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
        [[1, 2, 3, 4, 5]]","['Return', 'a', 'new', 'RDD', 'that', 'is', 'reduced', 'into', 'numPartitions', 'partitions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2095-L2115,train,Return a new RDD that is reduced into numPartitions partitions.
apache/spark,python/pyspark/rdd.py,RDD.zip,"def zip(self, other):
        """"""
        Zips this RDD with another one, returning key-value pairs with the
        first element in each RDD second element in each RDD, etc. Assumes
        that the two RDDs have the same number of partitions and the same
        number of elements in each partition (e.g. one was made through
        a map on the other).

        >>> x = sc.parallelize(range(0,5))
        >>> y = sc.parallelize(range(1000, 1005))
        >>> x.zip(y).collect()
        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]
        """"""
        def get_batch_size(ser):
            if isinstance(ser, BatchedSerializer):
                return ser.batchSize
            return 1  # not batched

        def batch_as(rdd, batchSize):
            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))

        my_batch = get_batch_size(self._jrdd_deserializer)
        other_batch = get_batch_size(other._jrdd_deserializer)
        if my_batch != other_batch or not my_batch:
            # use the smallest batchSize for both of them
            batchSize = min(my_batch, other_batch)
            if batchSize <= 0:
                # auto batched or unlimited
                batchSize = 100
            other = batch_as(other, batchSize)
            self = batch_as(self, batchSize)

        if self.getNumPartitions() != other.getNumPartitions():
            raise ValueError(""Can only zip with RDD which has the same number of partitions"")

        # There will be an Exception in JVM if there are different number
        # of items in each partitions.
        pairRDD = self._jrdd.zip(other._jrdd)
        deserializer = PairDeserializer(self._jrdd_deserializer,
                                        other._jrdd_deserializer)
        return RDD(pairRDD, self.ctx, deserializer)",python,"def zip(self, other):
        """"""
        Zips this RDD with another one, returning key-value pairs with the
        first element in each RDD second element in each RDD, etc. Assumes
        that the two RDDs have the same number of partitions and the same
        number of elements in each partition (e.g. one was made through
        a map on the other).

        >>> x = sc.parallelize(range(0,5))
        >>> y = sc.parallelize(range(1000, 1005))
        >>> x.zip(y).collect()
        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]
        """"""
        def get_batch_size(ser):
            if isinstance(ser, BatchedSerializer):
                return ser.batchSize
            return 1  # not batched

        def batch_as(rdd, batchSize):
            return rdd._reserialize(BatchedSerializer(PickleSerializer(), batchSize))

        my_batch = get_batch_size(self._jrdd_deserializer)
        other_batch = get_batch_size(other._jrdd_deserializer)
        if my_batch != other_batch or not my_batch:
            # use the smallest batchSize for both of them
            batchSize = min(my_batch, other_batch)
            if batchSize <= 0:
                # auto batched or unlimited
                batchSize = 100
            other = batch_as(other, batchSize)
            self = batch_as(self, batchSize)

        if self.getNumPartitions() != other.getNumPartitions():
            raise ValueError(""Can only zip with RDD which has the same number of partitions"")

        # There will be an Exception in JVM if there are different number
        # of items in each partitions.
        pairRDD = self._jrdd.zip(other._jrdd)
        deserializer = PairDeserializer(self._jrdd_deserializer,
                                        other._jrdd_deserializer)
        return RDD(pairRDD, self.ctx, deserializer)","['def', 'zip', '(', 'self', ',', 'other', ')', ':', 'def', 'get_batch_size', '(', 'ser', ')', ':', 'if', 'isinstance', '(', 'ser', ',', 'BatchedSerializer', ')', ':', 'return', 'ser', '.', 'batchSize', 'return', '1', '# not batched', 'def', 'batch_as', '(', 'rdd', ',', 'batchSize', ')', ':', 'return', 'rdd', '.', '_reserialize', '(', 'BatchedSerializer', '(', 'PickleSerializer', '(', ')', ',', 'batchSize', ')', ')', 'my_batch', '=', 'get_batch_size', '(', 'self', '.', '_jrdd_deserializer', ')', 'other_batch', '=', 'get_batch_size', '(', 'other', '.', '_jrdd_deserializer', ')', 'if', 'my_batch', '!=', 'other_batch', 'or', 'not', 'my_batch', ':', '# use the smallest batchSize for both of them', 'batchSize', '=', 'min', '(', 'my_batch', ',', 'other_batch', ')', 'if', 'batchSize', '<=', '0', ':', '# auto batched or unlimited', 'batchSize', '=', '100', 'other', '=', 'batch_as', '(', 'other', ',', 'batchSize', ')', 'self', '=', 'batch_as', '(', 'self', ',', 'batchSize', ')', 'if', 'self', '.', 'getNumPartitions', '(', ')', '!=', 'other', '.', 'getNumPartitions', '(', ')', ':', 'raise', 'ValueError', '(', '""Can only zip with RDD which has the same number of partitions""', ')', '# There will be an Exception in JVM if there are different number', '# of items in each partitions.', 'pairRDD', '=', 'self', '.', '_jrdd', '.', 'zip', '(', 'other', '.', '_jrdd', ')', 'deserializer', '=', 'PairDeserializer', '(', 'self', '.', '_jrdd_deserializer', ',', 'other', '.', '_jrdd_deserializer', ')', 'return', 'RDD', '(', 'pairRDD', ',', 'self', '.', 'ctx', ',', 'deserializer', ')']","Zips this RDD with another one, returning key-value pairs with the
        first element in each RDD second element in each RDD, etc. Assumes
        that the two RDDs have the same number of partitions and the same
        number of elements in each partition (e.g. one was made through
        a map on the other).

        >>> x = sc.parallelize(range(0,5))
        >>> y = sc.parallelize(range(1000, 1005))
        >>> x.zip(y).collect()
        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]","['Zips', 'this', 'RDD', 'with', 'another', 'one', 'returning', 'key', '-', 'value', 'pairs', 'with', 'the', 'first', 'element', 'in', 'each', 'RDD', 'second', 'element', 'in', 'each', 'RDD', 'etc', '.', 'Assumes', 'that', 'the', 'two', 'RDDs', 'have', 'the', 'same', 'number', 'of', 'partitions', 'and', 'the', 'same', 'number', 'of', 'elements', 'in', 'each', 'partition', '(', 'e', '.', 'g', '.', 'one', 'was', 'made', 'through', 'a', 'map', 'on', 'the', 'other', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2117-L2157,train,Returns an RDD of key - value pairs with the same entry - level as this one.
apache/spark,python/pyspark/rdd.py,RDD.zipWithIndex,"def zipWithIndex(self):
        """"""
        Zips this RDD with its element indices.

        The ordering is first based on the partition index and then the
        ordering of items within each partition. So the first item in
        the first partition gets index 0, and the last item in the last
        partition receives the largest index.

        This method needs to trigger a spark job when this RDD contains
        more than one partitions.

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d""], 3).zipWithIndex().collect()
        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]
        """"""
        starts = [0]
        if self.getNumPartitions() > 1:
            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
            for i in range(len(nums) - 1):
                starts.append(starts[-1] + nums[i])

        def func(k, it):
            for i, v in enumerate(it, starts[k]):
                yield v, i

        return self.mapPartitionsWithIndex(func)",python,"def zipWithIndex(self):
        """"""
        Zips this RDD with its element indices.

        The ordering is first based on the partition index and then the
        ordering of items within each partition. So the first item in
        the first partition gets index 0, and the last item in the last
        partition receives the largest index.

        This method needs to trigger a spark job when this RDD contains
        more than one partitions.

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d""], 3).zipWithIndex().collect()
        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]
        """"""
        starts = [0]
        if self.getNumPartitions() > 1:
            nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()
            for i in range(len(nums) - 1):
                starts.append(starts[-1] + nums[i])

        def func(k, it):
            for i, v in enumerate(it, starts[k]):
                yield v, i

        return self.mapPartitionsWithIndex(func)","['def', 'zipWithIndex', '(', 'self', ')', ':', 'starts', '=', '[', '0', ']', 'if', 'self', '.', 'getNumPartitions', '(', ')', '>', '1', ':', 'nums', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'sum', '(', '1', 'for', 'i', 'in', 'it', ')', ']', ')', '.', 'collect', '(', ')', 'for', 'i', 'in', 'range', '(', 'len', '(', 'nums', ')', '-', '1', ')', ':', 'starts', '.', 'append', '(', 'starts', '[', '-', '1', ']', '+', 'nums', '[', 'i', ']', ')', 'def', 'func', '(', 'k', ',', 'it', ')', ':', 'for', 'i', ',', 'v', 'in', 'enumerate', '(', 'it', ',', 'starts', '[', 'k', ']', ')', ':', 'yield', 'v', ',', 'i', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')']","Zips this RDD with its element indices.

        The ordering is first based on the partition index and then the
        ordering of items within each partition. So the first item in
        the first partition gets index 0, and the last item in the last
        partition receives the largest index.

        This method needs to trigger a spark job when this RDD contains
        more than one partitions.

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d""], 3).zipWithIndex().collect()
        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]","['Zips', 'this', 'RDD', 'with', 'its', 'element', 'indices', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2159-L2184,train,Returns an RDD with its element indices.
apache/spark,python/pyspark/rdd.py,RDD.zipWithUniqueId,"def zipWithUniqueId(self):
        """"""
        Zips this RDD with generated unique Long ids.

        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where
        n is the number of partitions. So there may exist gaps, but this
        method won't trigger a spark job, which is different from
        L{zipWithIndex}

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d"", ""e""], 3).zipWithUniqueId().collect()
        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]
        """"""
        n = self.getNumPartitions()

        def func(k, it):
            for i, v in enumerate(it):
                yield v, i * n + k

        return self.mapPartitionsWithIndex(func)",python,"def zipWithUniqueId(self):
        """"""
        Zips this RDD with generated unique Long ids.

        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where
        n is the number of partitions. So there may exist gaps, but this
        method won't trigger a spark job, which is different from
        L{zipWithIndex}

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d"", ""e""], 3).zipWithUniqueId().collect()
        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]
        """"""
        n = self.getNumPartitions()

        def func(k, it):
            for i, v in enumerate(it):
                yield v, i * n + k

        return self.mapPartitionsWithIndex(func)","['def', 'zipWithUniqueId', '(', 'self', ')', ':', 'n', '=', 'self', '.', 'getNumPartitions', '(', ')', 'def', 'func', '(', 'k', ',', 'it', ')', ':', 'for', 'i', ',', 'v', 'in', 'enumerate', '(', 'it', ')', ':', 'yield', 'v', ',', 'i', '*', 'n', '+', 'k', 'return', 'self', '.', 'mapPartitionsWithIndex', '(', 'func', ')']","Zips this RDD with generated unique Long ids.

        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where
        n is the number of partitions. So there may exist gaps, but this
        method won't trigger a spark job, which is different from
        L{zipWithIndex}

        >>> sc.parallelize([""a"", ""b"", ""c"", ""d"", ""e""], 3).zipWithUniqueId().collect()
        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]","['Zips', 'this', 'RDD', 'with', 'generated', 'unique', 'Long', 'ids', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2186-L2204,train,Returns an RDD with generated unique Long ids.
apache/spark,python/pyspark/rdd.py,RDD.getStorageLevel,"def getStorageLevel(self):
        """"""
        Get the RDD's current storage level.

        >>> rdd1 = sc.parallelize([1,2])
        >>> rdd1.getStorageLevel()
        StorageLevel(False, False, False, False, 1)
        >>> print(rdd1.getStorageLevel())
        Serialized 1x Replicated
        """"""
        java_storage_level = self._jrdd.getStorageLevel()
        storage_level = StorageLevel(java_storage_level.useDisk(),
                                     java_storage_level.useMemory(),
                                     java_storage_level.useOffHeap(),
                                     java_storage_level.deserialized(),
                                     java_storage_level.replication())
        return storage_level",python,"def getStorageLevel(self):
        """"""
        Get the RDD's current storage level.

        >>> rdd1 = sc.parallelize([1,2])
        >>> rdd1.getStorageLevel()
        StorageLevel(False, False, False, False, 1)
        >>> print(rdd1.getStorageLevel())
        Serialized 1x Replicated
        """"""
        java_storage_level = self._jrdd.getStorageLevel()
        storage_level = StorageLevel(java_storage_level.useDisk(),
                                     java_storage_level.useMemory(),
                                     java_storage_level.useOffHeap(),
                                     java_storage_level.deserialized(),
                                     java_storage_level.replication())
        return storage_level","['def', 'getStorageLevel', '(', 'self', ')', ':', 'java_storage_level', '=', 'self', '.', '_jrdd', '.', 'getStorageLevel', '(', ')', 'storage_level', '=', 'StorageLevel', '(', 'java_storage_level', '.', 'useDisk', '(', ')', ',', 'java_storage_level', '.', 'useMemory', '(', ')', ',', 'java_storage_level', '.', 'useOffHeap', '(', ')', ',', 'java_storage_level', '.', 'deserialized', '(', ')', ',', 'java_storage_level', '.', 'replication', '(', ')', ')', 'return', 'storage_level']","Get the RDD's current storage level.

        >>> rdd1 = sc.parallelize([1,2])
        >>> rdd1.getStorageLevel()
        StorageLevel(False, False, False, False, 1)
        >>> print(rdd1.getStorageLevel())
        Serialized 1x Replicated","['Get', 'the', 'RDD', 's', 'current', 'storage', 'level', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2234-L2250,train,Get the current storage level of the RDD.
apache/spark,python/pyspark/rdd.py,RDD._defaultReducePartitions,"def _defaultReducePartitions(self):
        """"""
        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).
        If spark.default.parallelism is set, then we'll use the value from SparkContext
        defaultParallelism, otherwise we'll use the number of partitions in this RDD.

        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce
        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will
        be inherent.
        """"""
        if self.ctx._conf.contains(""spark.default.parallelism""):
            return self.ctx.defaultParallelism
        else:
            return self.getNumPartitions()",python,"def _defaultReducePartitions(self):
        """"""
        Returns the default number of partitions to use during reduce tasks (e.g., groupBy).
        If spark.default.parallelism is set, then we'll use the value from SparkContext
        defaultParallelism, otherwise we'll use the number of partitions in this RDD.

        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce
        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will
        be inherent.
        """"""
        if self.ctx._conf.contains(""spark.default.parallelism""):
            return self.ctx.defaultParallelism
        else:
            return self.getNumPartitions()","['def', '_defaultReducePartitions', '(', 'self', ')', ':', 'if', 'self', '.', 'ctx', '.', '_conf', '.', 'contains', '(', '""spark.default.parallelism""', ')', ':', 'return', 'self', '.', 'ctx', '.', 'defaultParallelism', 'else', ':', 'return', 'self', '.', 'getNumPartitions', '(', ')']","Returns the default number of partitions to use during reduce tasks (e.g., groupBy).
        If spark.default.parallelism is set, then we'll use the value from SparkContext
        defaultParallelism, otherwise we'll use the number of partitions in this RDD.

        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce
        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will
        be inherent.","['Returns', 'the', 'default', 'number', 'of', 'partitions', 'to', 'use', 'during', 'reduce', 'tasks', '(', 'e', '.', 'g', '.', 'groupBy', ')', '.', 'If', 'spark', '.', 'default', '.', 'parallelism', 'is', 'set', 'then', 'we', 'll', 'use', 'the', 'value', 'from', 'SparkContext', 'defaultParallelism', 'otherwise', 'we', 'll', 'use', 'the', 'number', 'of', 'partitions', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2252-L2265,train,Returns the default number of partitions to use during reduce tasks.
apache/spark,python/pyspark/rdd.py,RDD.lookup,"def lookup(self, key):
        """"""
        Return the list of values in the RDD for key `key`. This operation
        is done efficiently if the RDD has a known partitioner by only
        searching the partition that the key maps to.

        >>> l = range(1000)
        >>> rdd = sc.parallelize(zip(l, l), 10)
        >>> rdd.lookup(42)  # slow
        [42]
        >>> sorted = rdd.sortByKey()
        >>> sorted.lookup(42)  # fast
        [42]
        >>> sorted.lookup(1024)
        []
        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()
        >>> list(rdd2.lookup(('a', 'b'))[0])
        ['c']
        """"""
        values = self.filter(lambda kv: kv[0] == key).values()

        if self.partitioner is not None:
            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])

        return values.collect()",python,"def lookup(self, key):
        """"""
        Return the list of values in the RDD for key `key`. This operation
        is done efficiently if the RDD has a known partitioner by only
        searching the partition that the key maps to.

        >>> l = range(1000)
        >>> rdd = sc.parallelize(zip(l, l), 10)
        >>> rdd.lookup(42)  # slow
        [42]
        >>> sorted = rdd.sortByKey()
        >>> sorted.lookup(42)  # fast
        [42]
        >>> sorted.lookup(1024)
        []
        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()
        >>> list(rdd2.lookup(('a', 'b'))[0])
        ['c']
        """"""
        values = self.filter(lambda kv: kv[0] == key).values()

        if self.partitioner is not None:
            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])

        return values.collect()","['def', 'lookup', '(', 'self', ',', 'key', ')', ':', 'values', '=', 'self', '.', 'filter', '(', 'lambda', 'kv', ':', 'kv', '[', '0', ']', '==', 'key', ')', '.', 'values', '(', ')', 'if', 'self', '.', 'partitioner', 'is', 'not', 'None', ':', 'return', 'self', '.', 'ctx', '.', 'runJob', '(', 'values', ',', 'lambda', 'x', ':', 'x', ',', '[', 'self', '.', 'partitioner', '(', 'key', ')', ']', ')', 'return', 'values', '.', 'collect', '(', ')']","Return the list of values in the RDD for key `key`. This operation
        is done efficiently if the RDD has a known partitioner by only
        searching the partition that the key maps to.

        >>> l = range(1000)
        >>> rdd = sc.parallelize(zip(l, l), 10)
        >>> rdd.lookup(42)  # slow
        [42]
        >>> sorted = rdd.sortByKey()
        >>> sorted.lookup(42)  # fast
        [42]
        >>> sorted.lookup(1024)
        []
        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()
        >>> list(rdd2.lookup(('a', 'b'))[0])
        ['c']","['Return', 'the', 'list', 'of', 'values', 'in', 'the', 'RDD', 'for', 'key', 'key', '.', 'This', 'operation', 'is', 'done', 'efficiently', 'if', 'the', 'RDD', 'has', 'a', 'known', 'partitioner', 'by', 'only', 'searching', 'the', 'partition', 'that', 'the', 'key', 'maps', 'to', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2267-L2291,train,"Return the list of values in the RDD for key key. This operation is efficiently by searching the partitioner by only
       ."
apache/spark,python/pyspark/rdd.py,RDD._to_java_object_rdd,"def _to_java_object_rdd(self):
        """""" Return a JavaRDD of Object by unpickling

        It will convert each Python object into Java object by Pyrolite, whenever the
        RDD is serialized in batch or not.
        """"""
        rdd = self._pickled()
        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)",python,"def _to_java_object_rdd(self):
        """""" Return a JavaRDD of Object by unpickling

        It will convert each Python object into Java object by Pyrolite, whenever the
        RDD is serialized in batch or not.
        """"""
        rdd = self._pickled()
        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)","['def', '_to_java_object_rdd', '(', 'self', ')', ':', 'rdd', '=', 'self', '.', '_pickled', '(', ')', 'return', 'self', '.', 'ctx', '.', '_jvm', '.', 'SerDeUtil', '.', 'pythonToJava', '(', 'rdd', '.', '_jrdd', ',', 'True', ')']","Return a JavaRDD of Object by unpickling

        It will convert each Python object into Java object by Pyrolite, whenever the
        RDD is serialized in batch or not.","['Return', 'a', 'JavaRDD', 'of', 'Object', 'by', 'unpickling']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2293-L2300,train,"Return a JavaRDD of Object by unpickling
       "
apache/spark,python/pyspark/rdd.py,RDD.countApprox,"def countApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate version of count() that returns a potentially incomplete
        result within a timeout, even if not all tasks have finished.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> rdd.countApprox(1000, 1.0)
        1000
        """"""
        drdd = self.mapPartitions(lambda it: [float(sum(1 for i in it))])
        return int(drdd.sumApprox(timeout, confidence))",python,"def countApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate version of count() that returns a potentially incomplete
        result within a timeout, even if not all tasks have finished.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> rdd.countApprox(1000, 1.0)
        1000
        """"""
        drdd = self.mapPartitions(lambda it: [float(sum(1 for i in it))])
        return int(drdd.sumApprox(timeout, confidence))","['def', 'countApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'drdd', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'float', '(', 'sum', '(', '1', 'for', 'i', 'in', 'it', ')', ')', ']', ')', 'return', 'int', '(', 'drdd', '.', 'sumApprox', '(', 'timeout', ',', 'confidence', ')', ')']",".. note:: Experimental

        Approximate version of count() that returns a potentially incomplete
        result within a timeout, even if not all tasks have finished.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> rdd.countApprox(1000, 1.0)
        1000","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2302-L2314,train,Count the number of incomplete entries within a given timeout.
apache/spark,python/pyspark/rdd.py,RDD.sumApprox,"def sumApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate operation to return the sum within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000))
        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05
        True
        """"""
        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())",python,"def sumApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate operation to return the sum within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000))
        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05
        True
        """"""
        jrdd = self.mapPartitions(lambda it: [float(sum(it))])._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.sumApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())","['def', 'sumApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'jrdd', '=', 'self', '.', 'mapPartitions', '(', 'lambda', 'it', ':', '[', 'float', '(', 'sum', '(', 'it', ')', ')', ']', ')', '.', '_to_java_object_rdd', '(', ')', 'jdrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'JavaDoubleRDD', '.', 'fromRDD', '(', 'jrdd', '.', 'rdd', '(', ')', ')', 'r', '=', 'jdrdd', '.', 'sumApprox', '(', 'timeout', ',', 'confidence', ')', '.', 'getFinalValue', '(', ')', 'return', 'BoundedFloat', '(', 'r', '.', 'mean', '(', ')', ',', 'r', '.', 'confidence', '(', ')', ',', 'r', '.', 'low', '(', ')', ',', 'r', '.', 'high', '(', ')', ')']",".. note:: Experimental

        Approximate operation to return the sum within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000))
        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05
        True","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2316-L2331,train,Return the sum of the elements within a given timeout.
apache/spark,python/pyspark/rdd.py,RDD.meanApprox,"def meanApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate operation to return the mean within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000)) / 1000.0
        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05
        True
        """"""
        jrdd = self.map(float)._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())",python,"def meanApprox(self, timeout, confidence=0.95):
        """"""
        .. note:: Experimental

        Approximate operation to return the mean within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000)) / 1000.0
        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05
        True
        """"""
        jrdd = self.map(float)._to_java_object_rdd()
        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())
        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()
        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())","['def', 'meanApprox', '(', 'self', ',', 'timeout', ',', 'confidence', '=', '0.95', ')', ':', 'jrdd', '=', 'self', '.', 'map', '(', 'float', ')', '.', '_to_java_object_rdd', '(', ')', 'jdrdd', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'JavaDoubleRDD', '.', 'fromRDD', '(', 'jrdd', '.', 'rdd', '(', ')', ')', 'r', '=', 'jdrdd', '.', 'meanApprox', '(', 'timeout', ',', 'confidence', ')', '.', 'getFinalValue', '(', ')', 'return', 'BoundedFloat', '(', 'r', '.', 'mean', '(', ')', ',', 'r', '.', 'confidence', '(', ')', ',', 'r', '.', 'low', '(', ')', ',', 'r', '.', 'high', '(', ')', ')']",".. note:: Experimental

        Approximate operation to return the mean within a timeout
        or meet the confidence.

        >>> rdd = sc.parallelize(range(1000), 10)
        >>> r = sum(range(1000)) / 1000.0
        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05
        True","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2333-L2348,train,Return the mean of the set of entries within a given timeout.
apache/spark,python/pyspark/rdd.py,RDD.countApproxDistinct,"def countApproxDistinct(self, relativeSD=0.05):
        """"""
        .. note:: Experimental

        Return approximate number of distinct elements in the RDD.

        The algorithm used is based on streamlib's implementation of
        `""HyperLogLog in Practice: Algorithmic Engineering of a State
        of The Art Cardinality Estimation Algorithm"", available here
        <https://doi.org/10.1145/2452376.2452456>`_.

        :param relativeSD: Relative accuracy. Smaller values create
                           counters that require more space.
                           It must be greater than 0.000017.

        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()
        >>> 900 < n < 1100
        True
        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()
        >>> 16 < n < 24
        True
        """"""
        if relativeSD < 0.000017:
            raise ValueError(""relativeSD should be greater than 0.000017"")
        # the hash space in Java is 2^32
        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)
        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)",python,"def countApproxDistinct(self, relativeSD=0.05):
        """"""
        .. note:: Experimental

        Return approximate number of distinct elements in the RDD.

        The algorithm used is based on streamlib's implementation of
        `""HyperLogLog in Practice: Algorithmic Engineering of a State
        of The Art Cardinality Estimation Algorithm"", available here
        <https://doi.org/10.1145/2452376.2452456>`_.

        :param relativeSD: Relative accuracy. Smaller values create
                           counters that require more space.
                           It must be greater than 0.000017.

        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()
        >>> 900 < n < 1100
        True
        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()
        >>> 16 < n < 24
        True
        """"""
        if relativeSD < 0.000017:
            raise ValueError(""relativeSD should be greater than 0.000017"")
        # the hash space in Java is 2^32
        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)
        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)","['def', 'countApproxDistinct', '(', 'self', ',', 'relativeSD', '=', '0.05', ')', ':', 'if', 'relativeSD', '<', '0.000017', ':', 'raise', 'ValueError', '(', '""relativeSD should be greater than 0.000017""', ')', '# the hash space in Java is 2^32', 'hashRDD', '=', 'self', '.', 'map', '(', 'lambda', 'x', ':', 'portable_hash', '(', 'x', ')', '&', '0xFFFFFFFF', ')', 'return', 'hashRDD', '.', '_to_java_object_rdd', '(', ')', '.', 'countApproxDistinct', '(', 'relativeSD', ')']",".. note:: Experimental

        Return approximate number of distinct elements in the RDD.

        The algorithm used is based on streamlib's implementation of
        `""HyperLogLog in Practice: Algorithmic Engineering of a State
        of The Art Cardinality Estimation Algorithm"", available here
        <https://doi.org/10.1145/2452376.2452456>`_.

        :param relativeSD: Relative accuracy. Smaller values create
                           counters that require more space.
                           It must be greater than 0.000017.

        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()
        >>> 900 < n < 1100
        True
        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()
        >>> 16 < n < 24
        True","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2350-L2376,train,Return approximate number of distinct elements in the RDD.
apache/spark,python/pyspark/rdd.py,RDD.toLocalIterator,"def toLocalIterator(self):
        """"""
        Return an iterator that contains all of the elements in this RDD.
        The iterator will consume as much memory as the largest partition in this RDD.

        >>> rdd = sc.parallelize(range(10))
        >>> [x for x in rdd.toLocalIterator()]
        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        """"""
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())
        return _load_from_socket(sock_info, self._jrdd_deserializer)",python,"def toLocalIterator(self):
        """"""
        Return an iterator that contains all of the elements in this RDD.
        The iterator will consume as much memory as the largest partition in this RDD.

        >>> rdd = sc.parallelize(range(10))
        >>> [x for x in rdd.toLocalIterator()]
        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
        """"""
        with SCCallSiteSync(self.context) as css:
            sock_info = self.ctx._jvm.PythonRDD.toLocalIteratorAndServe(self._jrdd.rdd())
        return _load_from_socket(sock_info, self._jrdd_deserializer)","['def', 'toLocalIterator', '(', 'self', ')', ':', 'with', 'SCCallSiteSync', '(', 'self', '.', 'context', ')', 'as', 'css', ':', 'sock_info', '=', 'self', '.', 'ctx', '.', '_jvm', '.', 'PythonRDD', '.', 'toLocalIteratorAndServe', '(', 'self', '.', '_jrdd', '.', 'rdd', '(', ')', ')', 'return', '_load_from_socket', '(', 'sock_info', ',', 'self', '.', '_jrdd_deserializer', ')']","Return an iterator that contains all of the elements in this RDD.
        The iterator will consume as much memory as the largest partition in this RDD.

        >>> rdd = sc.parallelize(range(10))
        >>> [x for x in rdd.toLocalIterator()]
        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]","['Return', 'an', 'iterator', 'that', 'contains', 'all', 'of', 'the', 'elements', 'in', 'this', 'RDD', '.', 'The', 'iterator', 'will', 'consume', 'as', 'much', 'memory', 'as', 'the', 'largest', 'partition', 'in', 'this', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2378-L2389,train,Returns an iterator that contains all of the elements in this RDD.
apache/spark,python/pyspark/rdd.py,RDDBarrier.mapPartitions,"def mapPartitions(self, f, preservesPartitioning=False):
        """"""
        .. note:: Experimental

        Returns a new RDD by applying a function to each partition of the wrapped RDD,
        where tasks are launched together in a barrier stage.
        The interface is the same as :func:`RDD.mapPartitions`.
        Please see the API doc there.

        .. versionadded:: 2.4.0
        """"""
        def func(s, iterator):
            return f(iterator)
        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)",python,"def mapPartitions(self, f, preservesPartitioning=False):
        """"""
        .. note:: Experimental

        Returns a new RDD by applying a function to each partition of the wrapped RDD,
        where tasks are launched together in a barrier stage.
        The interface is the same as :func:`RDD.mapPartitions`.
        Please see the API doc there.

        .. versionadded:: 2.4.0
        """"""
        def func(s, iterator):
            return f(iterator)
        return PipelinedRDD(self.rdd, func, preservesPartitioning, isFromBarrier=True)","['def', 'mapPartitions', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 's', ',', 'iterator', ')', ':', 'return', 'f', '(', 'iterator', ')', 'return', 'PipelinedRDD', '(', 'self', '.', 'rdd', ',', 'func', ',', 'preservesPartitioning', ',', 'isFromBarrier', '=', 'True', ')']",".. note:: Experimental

        Returns a new RDD by applying a function to each partition of the wrapped RDD,
        where tasks are launched together in a barrier stage.
        The interface is the same as :func:`RDD.mapPartitions`.
        Please see the API doc there.

        .. versionadded:: 2.4.0","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/rdd.py#L2455-L2468,train,Returns a new RDD by applying a function to each partition of the wrapped RDD.
apache/spark,python/pyspark/sql/column.py,_to_seq,"def _to_seq(sc, cols, converter=None):
    """"""
    Convert a list of Column (or names) into a JVM Seq of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.
    """"""
    if converter:
        cols = [converter(c) for c in cols]
    return sc._jvm.PythonUtils.toSeq(cols)",python,"def _to_seq(sc, cols, converter=None):
    """"""
    Convert a list of Column (or names) into a JVM Seq of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.
    """"""
    if converter:
        cols = [converter(c) for c in cols]
    return sc._jvm.PythonUtils.toSeq(cols)","['def', '_to_seq', '(', 'sc', ',', 'cols', ',', 'converter', '=', 'None', ')', ':', 'if', 'converter', ':', 'cols', '=', '[', 'converter', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'return', 'sc', '.', '_jvm', '.', 'PythonUtils', '.', 'toSeq', '(', 'cols', ')']","Convert a list of Column (or names) into a JVM Seq of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.","['Convert', 'a', 'list', 'of', 'Column', '(', 'or', 'names', ')', 'into', 'a', 'JVM', 'Seq', 'of', 'Column', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L57-L66,train,Convert a list of Column names or names into a JVM Seq of Column objects.
apache/spark,python/pyspark/sql/column.py,_to_list,"def _to_list(sc, cols, converter=None):
    """"""
    Convert a list of Column (or names) into a JVM (Scala) List of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.
    """"""
    if converter:
        cols = [converter(c) for c in cols]
    return sc._jvm.PythonUtils.toList(cols)",python,"def _to_list(sc, cols, converter=None):
    """"""
    Convert a list of Column (or names) into a JVM (Scala) List of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.
    """"""
    if converter:
        cols = [converter(c) for c in cols]
    return sc._jvm.PythonUtils.toList(cols)","['def', '_to_list', '(', 'sc', ',', 'cols', ',', 'converter', '=', 'None', ')', ':', 'if', 'converter', ':', 'cols', '=', '[', 'converter', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'return', 'sc', '.', '_jvm', '.', 'PythonUtils', '.', 'toList', '(', 'cols', ')']","Convert a list of Column (or names) into a JVM (Scala) List of Column.

    An optional `converter` could be used to convert items in `cols`
    into JVM Column objects.","['Convert', 'a', 'list', 'of', 'Column', '(', 'or', 'names', ')', 'into', 'a', 'JVM', '(', 'Scala', ')', 'List', 'of', 'Column', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L69-L78,train,Convert a list of Column or names into a JVM ( Scala ) List of Column objects.
apache/spark,python/pyspark/sql/column.py,_unary_op,"def _unary_op(name, doc=""unary operator""):
    """""" Create a method for given unary operator """"""
    def _(self):
        jc = getattr(self._jc, name)()
        return Column(jc)
    _.__doc__ = doc
    return _",python,"def _unary_op(name, doc=""unary operator""):
    """""" Create a method for given unary operator """"""
    def _(self):
        jc = getattr(self._jc, name)()
        return Column(jc)
    _.__doc__ = doc
    return _","['def', '_unary_op', '(', 'name', ',', 'doc', '=', '""unary operator""', ')', ':', 'def', '_', '(', 'self', ')', ':', 'jc', '=', 'getattr', '(', 'self', '.', '_jc', ',', 'name', ')', '(', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']",Create a method for given unary operator,"['Create', 'a', 'method', 'for', 'given', 'unary', 'operator']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L81-L87,train,Create a method for given unary operator
apache/spark,python/pyspark/sql/column.py,_bin_op,"def _bin_op(name, doc=""binary operator""):
    """""" Create a method for given binary operator
    """"""
    def _(self, other):
        jc = other._jc if isinstance(other, Column) else other
        njc = getattr(self._jc, name)(jc)
        return Column(njc)
    _.__doc__ = doc
    return _",python,"def _bin_op(name, doc=""binary operator""):
    """""" Create a method for given binary operator
    """"""
    def _(self, other):
        jc = other._jc if isinstance(other, Column) else other
        njc = getattr(self._jc, name)(jc)
        return Column(njc)
    _.__doc__ = doc
    return _","['def', '_bin_op', '(', 'name', ',', 'doc', '=', '""binary operator""', ')', ':', 'def', '_', '(', 'self', ',', 'other', ')', ':', 'jc', '=', 'other', '.', '_jc', 'if', 'isinstance', '(', 'other', ',', 'Column', ')', 'else', 'other', 'njc', '=', 'getattr', '(', 'self', '.', '_jc', ',', 'name', ')', '(', 'jc', ')', 'return', 'Column', '(', 'njc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']",Create a method for given binary operator,"['Create', 'a', 'method', 'for', 'given', 'binary', 'operator']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L110-L118,train,Create a method that returns a new object for the given binary operator.
apache/spark,python/pyspark/sql/column.py,_reverse_op,"def _reverse_op(name, doc=""binary operator""):
    """""" Create a method for binary operator (this object is on right side)
    """"""
    def _(self, other):
        jother = _create_column_from_literal(other)
        jc = getattr(jother, name)(self._jc)
        return Column(jc)
    _.__doc__ = doc
    return _",python,"def _reverse_op(name, doc=""binary operator""):
    """""" Create a method for binary operator (this object is on right side)
    """"""
    def _(self, other):
        jother = _create_column_from_literal(other)
        jc = getattr(jother, name)(self._jc)
        return Column(jc)
    _.__doc__ = doc
    return _","['def', '_reverse_op', '(', 'name', ',', 'doc', '=', '""binary operator""', ')', ':', 'def', '_', '(', 'self', ',', 'other', ')', ':', 'jother', '=', '_create_column_from_literal', '(', 'other', ')', 'jc', '=', 'getattr', '(', 'jother', ',', 'name', ')', '(', 'self', '.', '_jc', ')', 'return', 'Column', '(', 'jc', ')', '_', '.', '__doc__', '=', 'doc', 'return', '_']",Create a method for binary operator (this object is on right side),"['Create', 'a', 'method', 'for', 'binary', 'operator', '(', 'this', 'object', 'is', 'on', 'right', 'side', ')']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L121-L129,train,Create a method for binary operator
apache/spark,python/pyspark/sql/column.py,Column.substr,"def substr(self, startPos, length):
        """"""
        Return a :class:`Column` which is a substring of the column.

        :param startPos: start position (int or Column)
        :param length:  length of the substring (int or Column)

        >>> df.select(df.name.substr(1, 3).alias(""col"")).collect()
        [Row(col=u'Ali'), Row(col=u'Bob')]
        """"""
        if type(startPos) != type(length):
            raise TypeError(
                ""startPos and length must be the same type. ""
                ""Got {startPos_t} and {length_t}, respectively.""
                .format(
                    startPos_t=type(startPos),
                    length_t=type(length),
                ))
        if isinstance(startPos, int):
            jc = self._jc.substr(startPos, length)
        elif isinstance(startPos, Column):
            jc = self._jc.substr(startPos._jc, length._jc)
        else:
            raise TypeError(""Unexpected type: %s"" % type(startPos))
        return Column(jc)",python,"def substr(self, startPos, length):
        """"""
        Return a :class:`Column` which is a substring of the column.

        :param startPos: start position (int or Column)
        :param length:  length of the substring (int or Column)

        >>> df.select(df.name.substr(1, 3).alias(""col"")).collect()
        [Row(col=u'Ali'), Row(col=u'Bob')]
        """"""
        if type(startPos) != type(length):
            raise TypeError(
                ""startPos and length must be the same type. ""
                ""Got {startPos_t} and {length_t}, respectively.""
                .format(
                    startPos_t=type(startPos),
                    length_t=type(length),
                ))
        if isinstance(startPos, int):
            jc = self._jc.substr(startPos, length)
        elif isinstance(startPos, Column):
            jc = self._jc.substr(startPos._jc, length._jc)
        else:
            raise TypeError(""Unexpected type: %s"" % type(startPos))
        return Column(jc)","['def', 'substr', '(', 'self', ',', 'startPos', ',', 'length', ')', ':', 'if', 'type', '(', 'startPos', ')', '!=', 'type', '(', 'length', ')', ':', 'raise', 'TypeError', '(', '""startPos and length must be the same type. ""', '""Got {startPos_t} and {length_t}, respectively.""', '.', 'format', '(', 'startPos_t', '=', 'type', '(', 'startPos', ')', ',', 'length_t', '=', 'type', '(', 'length', ')', ',', ')', ')', 'if', 'isinstance', '(', 'startPos', ',', 'int', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'substr', '(', 'startPos', ',', 'length', ')', 'elif', 'isinstance', '(', 'startPos', ',', 'Column', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'substr', '(', 'startPos', '.', '_jc', ',', 'length', '.', '_jc', ')', 'else', ':', 'raise', 'TypeError', '(', '""Unexpected type: %s""', '%', 'type', '(', 'startPos', ')', ')', 'return', 'Column', '(', 'jc', ')']","Return a :class:`Column` which is a substring of the column.

        :param startPos: start position (int or Column)
        :param length:  length of the substring (int or Column)

        >>> df.select(df.name.substr(1, 3).alias(""col"")).collect()
        [Row(col=u'Ali'), Row(col=u'Bob')]","['Return', 'a', ':', 'class', ':', 'Column', 'which', 'is', 'a', 'substring', 'of', 'the', 'column', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L403-L427,train,Return a Column which is a substring of the column.
apache/spark,python/pyspark/sql/column.py,Column.isin,"def isin(self, *cols):
        """"""
        A boolean expression that is evaluated to true if the value of this
        expression is contained by the evaluated values of the arguments.

        >>> df[df.name.isin(""Bob"", ""Mike"")].collect()
        [Row(age=5, name=u'Bob')]
        >>> df[df.age.isin([1, 2, 3])].collect()
        [Row(age=2, name=u'Alice')]
        """"""
        if len(cols) == 1 and isinstance(cols[0], (list, set)):
            cols = cols[0]
        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]
        sc = SparkContext._active_spark_context
        jc = getattr(self._jc, ""isin"")(_to_seq(sc, cols))
        return Column(jc)",python,"def isin(self, *cols):
        """"""
        A boolean expression that is evaluated to true if the value of this
        expression is contained by the evaluated values of the arguments.

        >>> df[df.name.isin(""Bob"", ""Mike"")].collect()
        [Row(age=5, name=u'Bob')]
        >>> df[df.age.isin([1, 2, 3])].collect()
        [Row(age=2, name=u'Alice')]
        """"""
        if len(cols) == 1 and isinstance(cols[0], (list, set)):
            cols = cols[0]
        cols = [c._jc if isinstance(c, Column) else _create_column_from_literal(c) for c in cols]
        sc = SparkContext._active_spark_context
        jc = getattr(self._jc, ""isin"")(_to_seq(sc, cols))
        return Column(jc)","['def', 'isin', '(', 'self', ',', '*', 'cols', ')', ':', 'if', 'len', '(', 'cols', ')', '==', '1', 'and', 'isinstance', '(', 'cols', '[', '0', ']', ',', '(', 'list', ',', 'set', ')', ')', ':', 'cols', '=', 'cols', '[', '0', ']', 'cols', '=', '[', 'c', '.', '_jc', 'if', 'isinstance', '(', 'c', ',', 'Column', ')', 'else', '_create_column_from_literal', '(', 'c', ')', 'for', 'c', 'in', 'cols', ']', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'jc', '=', 'getattr', '(', 'self', '.', '_jc', ',', '""isin""', ')', '(', '_to_seq', '(', 'sc', ',', 'cols', ')', ')', 'return', 'Column', '(', 'jc', ')']","A boolean expression that is evaluated to true if the value of this
        expression is contained by the evaluated values of the arguments.

        >>> df[df.name.isin(""Bob"", ""Mike"")].collect()
        [Row(age=5, name=u'Bob')]
        >>> df[df.age.isin([1, 2, 3])].collect()
        [Row(age=2, name=u'Alice')]","['A', 'boolean', 'expression', 'that', 'is', 'evaluated', 'to', 'true', 'if', 'the', 'value', 'of', 'this', 'expression', 'is', 'contained', 'by', 'the', 'evaluated', 'values', 'of', 'the', 'arguments', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L431-L446,train,"A boolean expression that is evaluated to true if the value of this
        expression is contained by the evaluated values of the arguments."
apache/spark,python/pyspark/sql/column.py,Column.alias,"def alias(self, *alias, **kwargs):
        """"""
        Returns this column aliased with a new name or names (in the case of expressions that
        return more than one column, such as explode).

        :param alias: strings of desired column names (collects all positional arguments passed)
        :param metadata: a dict of information to be stored in ``metadata`` attribute of the
            corresponding :class: `StructField` (optional, keyword only argument)

        .. versionchanged:: 2.2
           Added optional ``metadata`` argument.

        >>> df.select(df.age.alias(""age2"")).collect()
        [Row(age2=2), Row(age2=5)]
        >>> df.select(df.age.alias(""age3"", metadata={'max': 99})).schema['age3'].metadata['max']
        99
        """"""

        metadata = kwargs.pop('metadata', None)
        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs

        sc = SparkContext._active_spark_context
        if len(alias) == 1:
            if metadata:
                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(
                    json.dumps(metadata))
                return Column(getattr(self._jc, ""as"")(alias[0], jmeta))
            else:
                return Column(getattr(self._jc, ""as"")(alias[0]))
        else:
            if metadata:
                raise ValueError('metadata can only be provided for a single column')
            return Column(getattr(self._jc, ""as"")(_to_seq(sc, list(alias))))",python,"def alias(self, *alias, **kwargs):
        """"""
        Returns this column aliased with a new name or names (in the case of expressions that
        return more than one column, such as explode).

        :param alias: strings of desired column names (collects all positional arguments passed)
        :param metadata: a dict of information to be stored in ``metadata`` attribute of the
            corresponding :class: `StructField` (optional, keyword only argument)

        .. versionchanged:: 2.2
           Added optional ``metadata`` argument.

        >>> df.select(df.age.alias(""age2"")).collect()
        [Row(age2=2), Row(age2=5)]
        >>> df.select(df.age.alias(""age3"", metadata={'max': 99})).schema['age3'].metadata['max']
        99
        """"""

        metadata = kwargs.pop('metadata', None)
        assert not kwargs, 'Unexpected kwargs where passed: %s' % kwargs

        sc = SparkContext._active_spark_context
        if len(alias) == 1:
            if metadata:
                jmeta = sc._jvm.org.apache.spark.sql.types.Metadata.fromJson(
                    json.dumps(metadata))
                return Column(getattr(self._jc, ""as"")(alias[0], jmeta))
            else:
                return Column(getattr(self._jc, ""as"")(alias[0]))
        else:
            if metadata:
                raise ValueError('metadata can only be provided for a single column')
            return Column(getattr(self._jc, ""as"")(_to_seq(sc, list(alias))))","['def', 'alias', '(', 'self', ',', '*', 'alias', ',', '*', '*', 'kwargs', ')', ':', 'metadata', '=', 'kwargs', '.', 'pop', '(', ""'metadata'"", ',', 'None', ')', 'assert', 'not', 'kwargs', ',', ""'Unexpected kwargs where passed: %s'"", '%', 'kwargs', 'sc', '=', 'SparkContext', '.', '_active_spark_context', 'if', 'len', '(', 'alias', ')', '==', '1', ':', 'if', 'metadata', ':', 'jmeta', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'sql', '.', 'types', '.', 'Metadata', '.', 'fromJson', '(', 'json', '.', 'dumps', '(', 'metadata', ')', ')', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '""as""', ')', '(', 'alias', '[', '0', ']', ',', 'jmeta', ')', ')', 'else', ':', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '""as""', ')', '(', 'alias', '[', '0', ']', ')', ')', 'else', ':', 'if', 'metadata', ':', 'raise', 'ValueError', '(', ""'metadata can only be provided for a single column'"", ')', 'return', 'Column', '(', 'getattr', '(', 'self', '.', '_jc', ',', '""as""', ')', '(', '_to_seq', '(', 'sc', ',', 'list', '(', 'alias', ')', ')', ')', ')']","Returns this column aliased with a new name or names (in the case of expressions that
        return more than one column, such as explode).

        :param alias: strings of desired column names (collects all positional arguments passed)
        :param metadata: a dict of information to be stored in ``metadata`` attribute of the
            corresponding :class: `StructField` (optional, keyword only argument)

        .. versionchanged:: 2.2
           Added optional ``metadata`` argument.

        >>> df.select(df.age.alias(""age2"")).collect()
        [Row(age2=2), Row(age2=5)]
        >>> df.select(df.age.alias(""age3"", metadata={'max': 99})).schema['age3'].metadata['max']
        99","['Returns', 'this', 'column', 'aliased', 'with', 'a', 'new', 'name', 'or', 'names', '(', 'in', 'the', 'case', 'of', 'expressions', 'that', 'return', 'more', 'than', 'one', 'column', 'such', 'as', 'explode', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L538-L570,train,Returns this column aliased with a new name or names.
apache/spark,python/pyspark/sql/column.py,Column.cast,"def cast(self, dataType):
        """""" Convert the column into type ``dataType``.

        >>> df.select(df.age.cast(""string"").alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]
        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]
        """"""
        if isinstance(dataType, basestring):
            jc = self._jc.cast(dataType)
        elif isinstance(dataType, DataType):
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            jdt = spark._jsparkSession.parseDataType(dataType.json())
            jc = self._jc.cast(jdt)
        else:
            raise TypeError(""unexpected type: %s"" % type(dataType))
        return Column(jc)",python,"def cast(self, dataType):
        """""" Convert the column into type ``dataType``.

        >>> df.select(df.age.cast(""string"").alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]
        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]
        """"""
        if isinstance(dataType, basestring):
            jc = self._jc.cast(dataType)
        elif isinstance(dataType, DataType):
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.getOrCreate()
            jdt = spark._jsparkSession.parseDataType(dataType.json())
            jc = self._jc.cast(jdt)
        else:
            raise TypeError(""unexpected type: %s"" % type(dataType))
        return Column(jc)","['def', 'cast', '(', 'self', ',', 'dataType', ')', ':', 'if', 'isinstance', '(', 'dataType', ',', 'basestring', ')', ':', 'jc', '=', 'self', '.', '_jc', '.', 'cast', '(', 'dataType', ')', 'elif', 'isinstance', '(', 'dataType', ',', 'DataType', ')', ':', 'from', 'pyspark', '.', 'sql', 'import', 'SparkSession', 'spark', '=', 'SparkSession', '.', 'builder', '.', 'getOrCreate', '(', ')', 'jdt', '=', 'spark', '.', '_jsparkSession', '.', 'parseDataType', '(', 'dataType', '.', 'json', '(', ')', ')', 'jc', '=', 'self', '.', '_jc', '.', 'cast', '(', 'jdt', ')', 'else', ':', 'raise', 'TypeError', '(', '""unexpected type: %s""', '%', 'type', '(', 'dataType', ')', ')', 'return', 'Column', '(', 'jc', ')']","Convert the column into type ``dataType``.

        >>> df.select(df.age.cast(""string"").alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]
        >>> df.select(df.age.cast(StringType()).alias('ages')).collect()
        [Row(ages=u'2'), Row(ages=u'5')]","['Convert', 'the', 'column', 'into', 'type', 'dataType', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L576-L593,train,Convert the column into type dataType.
apache/spark,python/pyspark/sql/column.py,Column.when,"def when(self, condition, value):
        """"""
        Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param condition: a boolean :class:`Column` expression.
        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()
        +-----+------------------------------------------------------------+
        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|
        +-----+------------------------------------------------------------+
        |Alice|                                                          -1|
        |  Bob|                                                           1|
        +-----+------------------------------------------------------------+
        """"""
        if not isinstance(condition, Column):
            raise TypeError(""condition should be a Column"")
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.when(condition._jc, v)
        return Column(jc)",python,"def when(self, condition, value):
        """"""
        Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param condition: a boolean :class:`Column` expression.
        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()
        +-----+------------------------------------------------------------+
        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|
        +-----+------------------------------------------------------------+
        |Alice|                                                          -1|
        |  Bob|                                                           1|
        +-----+------------------------------------------------------------+
        """"""
        if not isinstance(condition, Column):
            raise TypeError(""condition should be a Column"")
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.when(condition._jc, v)
        return Column(jc)","['def', 'when', '(', 'self', ',', 'condition', ',', 'value', ')', ':', 'if', 'not', 'isinstance', '(', 'condition', ',', 'Column', ')', ':', 'raise', 'TypeError', '(', '""condition should be a Column""', ')', 'v', '=', 'value', '.', '_jc', 'if', 'isinstance', '(', 'value', ',', 'Column', ')', 'else', 'value', 'jc', '=', 'self', '.', '_jc', '.', 'when', '(', 'condition', '.', '_jc', ',', 'v', ')', 'return', 'Column', '(', 'jc', ')']","Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param condition: a boolean :class:`Column` expression.
        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()
        +-----+------------------------------------------------------------+
        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|
        +-----+------------------------------------------------------------+
        |Alice|                                                          -1|
        |  Bob|                                                           1|
        +-----+------------------------------------------------------------+","['Evaluates', 'a', 'list', 'of', 'conditions', 'and', 'returns', 'one', 'of', 'multiple', 'possible', 'result', 'expressions', '.', 'If', ':', 'func', ':', 'Column', '.', 'otherwise', 'is', 'not', 'invoked', 'None', 'is', 'returned', 'for', 'unmatched', 'conditions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L614-L637,train,Evaluates a list of conditions and returns one of multiple possible result expressions.
apache/spark,python/pyspark/sql/column.py,Column.otherwise,"def otherwise(self, value):
        """"""
        Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()
        +-----+-------------------------------------+
        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|
        +-----+-------------------------------------+
        |Alice|                                    0|
        |  Bob|                                    1|
        +-----+-------------------------------------+
        """"""
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.otherwise(v)
        return Column(jc)",python,"def otherwise(self, value):
        """"""
        Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()
        +-----+-------------------------------------+
        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|
        +-----+-------------------------------------+
        |Alice|                                    0|
        |  Bob|                                    1|
        +-----+-------------------------------------+
        """"""
        v = value._jc if isinstance(value, Column) else value
        jc = self._jc.otherwise(v)
        return Column(jc)","['def', 'otherwise', '(', 'self', ',', 'value', ')', ':', 'v', '=', 'value', '.', '_jc', 'if', 'isinstance', '(', 'value', ',', 'Column', ')', 'else', 'value', 'jc', '=', 'self', '.', '_jc', '.', 'otherwise', '(', 'v', ')', 'return', 'Column', '(', 'jc', ')']","Evaluates a list of conditions and returns one of multiple possible result expressions.
        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.

        See :func:`pyspark.sql.functions.when` for example usage.

        :param value: a literal value, or a :class:`Column` expression.

        >>> from pyspark.sql import functions as F
        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()
        +-----+-------------------------------------+
        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|
        +-----+-------------------------------------+
        |Alice|                                    0|
        |  Bob|                                    1|
        +-----+-------------------------------------+","['Evaluates', 'a', 'list', 'of', 'conditions', 'and', 'returns', 'one', 'of', 'multiple', 'possible', 'result', 'expressions', '.', 'If', ':', 'func', ':', 'Column', '.', 'otherwise', 'is', 'not', 'invoked', 'None', 'is', 'returned', 'for', 'unmatched', 'conditions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L640-L660,train,Evaluates a list of conditions and returns one of multiple possible result expressions.
apache/spark,python/pyspark/sql/column.py,Column.over,"def over(self, window):
        """"""
        Define a windowing column.

        :param window: a :class:`WindowSpec`
        :return: a Column

        >>> from pyspark.sql import Window
        >>> window = Window.partitionBy(""name"").orderBy(""age"").rowsBetween(-1, 1)
        >>> from pyspark.sql.functions import rank, min
        >>> # df.select(rank().over(window), min('age').over(window))
        """"""
        from pyspark.sql.window import WindowSpec
        if not isinstance(window, WindowSpec):
            raise TypeError(""window should be WindowSpec"")
        jc = self._jc.over(window._jspec)
        return Column(jc)",python,"def over(self, window):
        """"""
        Define a windowing column.

        :param window: a :class:`WindowSpec`
        :return: a Column

        >>> from pyspark.sql import Window
        >>> window = Window.partitionBy(""name"").orderBy(""age"").rowsBetween(-1, 1)
        >>> from pyspark.sql.functions import rank, min
        >>> # df.select(rank().over(window), min('age').over(window))
        """"""
        from pyspark.sql.window import WindowSpec
        if not isinstance(window, WindowSpec):
            raise TypeError(""window should be WindowSpec"")
        jc = self._jc.over(window._jspec)
        return Column(jc)","['def', 'over', '(', 'self', ',', 'window', ')', ':', 'from', 'pyspark', '.', 'sql', '.', 'window', 'import', 'WindowSpec', 'if', 'not', 'isinstance', '(', 'window', ',', 'WindowSpec', ')', ':', 'raise', 'TypeError', '(', '""window should be WindowSpec""', ')', 'jc', '=', 'self', '.', '_jc', '.', 'over', '(', 'window', '.', '_jspec', ')', 'return', 'Column', '(', 'jc', ')']","Define a windowing column.

        :param window: a :class:`WindowSpec`
        :return: a Column

        >>> from pyspark.sql import Window
        >>> window = Window.partitionBy(""name"").orderBy(""age"").rowsBetween(-1, 1)
        >>> from pyspark.sql.functions import rank, min
        >>> # df.select(rank().over(window), min('age').over(window))","['Define', 'a', 'windowing', 'column', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/column.py#L663-L679,train,Define a windowing column.
apache/spark,python/pyspark/mllib/feature.py,JavaVectorTransformer.transform,"def transform(self, vector):
        """"""
        Applies transformation on a vector or an RDD[Vector].

        .. note:: In Python, transform cannot currently be used within
            an RDD transformation or action.
            Call transform directly on the RDD instead.

        :param vector: Vector or RDD of Vector to be transformed.
        """"""
        if isinstance(vector, RDD):
            vector = vector.map(_convert_to_vector)
        else:
            vector = _convert_to_vector(vector)
        return self.call(""transform"", vector)",python,"def transform(self, vector):
        """"""
        Applies transformation on a vector or an RDD[Vector].

        .. note:: In Python, transform cannot currently be used within
            an RDD transformation or action.
            Call transform directly on the RDD instead.

        :param vector: Vector or RDD of Vector to be transformed.
        """"""
        if isinstance(vector, RDD):
            vector = vector.map(_convert_to_vector)
        else:
            vector = _convert_to_vector(vector)
        return self.call(""transform"", vector)","['def', 'transform', '(', 'self', ',', 'vector', ')', ':', 'if', 'isinstance', '(', 'vector', ',', 'RDD', ')', ':', 'vector', '=', 'vector', '.', 'map', '(', '_convert_to_vector', ')', 'else', ':', 'vector', '=', '_convert_to_vector', '(', 'vector', ')', 'return', 'self', '.', 'call', '(', '""transform""', ',', 'vector', ')']","Applies transformation on a vector or an RDD[Vector].

        .. note:: In Python, transform cannot currently be used within
            an RDD transformation or action.
            Call transform directly on the RDD instead.

        :param vector: Vector or RDD of Vector to be transformed.","['Applies', 'transformation', 'on', 'a', 'vector', 'or', 'an', 'RDD', '[', 'Vector', ']', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L111-L125,train,Applies transformation on a vector or an RDD of Vectors.
apache/spark,python/pyspark/mllib/feature.py,StandardScaler.fit,"def fit(self, dataset):
        """"""
        Computes the mean and variance and stores as a model to be used
        for later scaling.

        :param dataset: The data used to compute the mean and variance
                     to build the transformation model.
        :return: a StandardScalarModel
        """"""
        dataset = dataset.map(_convert_to_vector)
        jmodel = callMLlibFunc(""fitStandardScaler"", self.withMean, self.withStd, dataset)
        return StandardScalerModel(jmodel)",python,"def fit(self, dataset):
        """"""
        Computes the mean and variance and stores as a model to be used
        for later scaling.

        :param dataset: The data used to compute the mean and variance
                     to build the transformation model.
        :return: a StandardScalarModel
        """"""
        dataset = dataset.map(_convert_to_vector)
        jmodel = callMLlibFunc(""fitStandardScaler"", self.withMean, self.withStd, dataset)
        return StandardScalerModel(jmodel)","['def', 'fit', '(', 'self', ',', 'dataset', ')', ':', 'dataset', '=', 'dataset', '.', 'map', '(', '_convert_to_vector', ')', 'jmodel', '=', 'callMLlibFunc', '(', '""fitStandardScaler""', ',', 'self', '.', 'withMean', ',', 'self', '.', 'withStd', ',', 'dataset', ')', 'return', 'StandardScalerModel', '(', 'jmodel', ')']","Computes the mean and variance and stores as a model to be used
        for later scaling.

        :param dataset: The data used to compute the mean and variance
                     to build the transformation model.
        :return: a StandardScalarModel","['Computes', 'the', 'mean', 'and', 'variance', 'and', 'stores', 'as', 'a', 'model', 'to', 'be', 'used', 'for', 'later', 'scaling', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L240-L251,train,Fits the model to the given dataset.
apache/spark,python/pyspark/mllib/feature.py,ChiSqSelector.fit,"def fit(self, data):
        """"""
        Returns a ChiSquared feature selector.

        :param data: an `RDD[LabeledPoint]` containing the labeled dataset
                     with categorical features. Real-valued features will be
                     treated as categorical for each distinct value.
                     Apply feature discretizer before using this function.
        """"""
        jmodel = callMLlibFunc(""fitChiSqSelector"", self.selectorType, self.numTopFeatures,
                               self.percentile, self.fpr, self.fdr, self.fwe, data)
        return ChiSqSelectorModel(jmodel)",python,"def fit(self, data):
        """"""
        Returns a ChiSquared feature selector.

        :param data: an `RDD[LabeledPoint]` containing the labeled dataset
                     with categorical features. Real-valued features will be
                     treated as categorical for each distinct value.
                     Apply feature discretizer before using this function.
        """"""
        jmodel = callMLlibFunc(""fitChiSqSelector"", self.selectorType, self.numTopFeatures,
                               self.percentile, self.fpr, self.fdr, self.fwe, data)
        return ChiSqSelectorModel(jmodel)","['def', 'fit', '(', 'self', ',', 'data', ')', ':', 'jmodel', '=', 'callMLlibFunc', '(', '""fitChiSqSelector""', ',', 'self', '.', 'selectorType', ',', 'self', '.', 'numTopFeatures', ',', 'self', '.', 'percentile', ',', 'self', '.', 'fpr', ',', 'self', '.', 'fdr', ',', 'self', '.', 'fwe', ',', 'data', ')', 'return', 'ChiSqSelectorModel', '(', 'jmodel', ')']","Returns a ChiSquared feature selector.

        :param data: an `RDD[LabeledPoint]` containing the labeled dataset
                     with categorical features. Real-valued features will be
                     treated as categorical for each distinct value.
                     Apply feature discretizer before using this function.","['Returns', 'a', 'ChiSquared', 'feature', 'selector', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L383-L394,train,Fits a ChiSquared feature selector.
apache/spark,python/pyspark/mllib/feature.py,PCA.fit,"def fit(self, data):
        """"""
        Computes a [[PCAModel]] that contains the principal components of the input vectors.
        :param data: source vectors
        """"""
        jmodel = callMLlibFunc(""fitPCA"", self.k, data)
        return PCAModel(jmodel)",python,"def fit(self, data):
        """"""
        Computes a [[PCAModel]] that contains the principal components of the input vectors.
        :param data: source vectors
        """"""
        jmodel = callMLlibFunc(""fitPCA"", self.k, data)
        return PCAModel(jmodel)","['def', 'fit', '(', 'self', ',', 'data', ')', ':', 'jmodel', '=', 'callMLlibFunc', '(', '""fitPCA""', ',', 'self', '.', 'k', ',', 'data', ')', 'return', 'PCAModel', '(', 'jmodel', ')']","Computes a [[PCAModel]] that contains the principal components of the input vectors.
        :param data: source vectors","['Computes', 'a', '[[', 'PCAModel', ']]', 'that', 'contains', 'the', 'principal', 'components', 'of', 'the', 'input', 'vectors', '.', ':', 'param', 'data', ':', 'source', 'vectors']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L428-L434,train,Computes a PCAModel that contains the principal components of the input vectors.
apache/spark,python/pyspark/mllib/feature.py,HashingTF.transform,"def transform(self, document):
        """"""
        Transforms the input document (list of terms) to term frequency
        vectors, or transform the RDD of document to RDD of term
        frequency vectors.
        """"""
        if isinstance(document, RDD):
            return document.map(self.transform)

        freq = {}
        for term in document:
            i = self.indexOf(term)
            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0
        return Vectors.sparse(self.numFeatures, freq.items())",python,"def transform(self, document):
        """"""
        Transforms the input document (list of terms) to term frequency
        vectors, or transform the RDD of document to RDD of term
        frequency vectors.
        """"""
        if isinstance(document, RDD):
            return document.map(self.transform)

        freq = {}
        for term in document:
            i = self.indexOf(term)
            freq[i] = 1.0 if self.binary else freq.get(i, 0) + 1.0
        return Vectors.sparse(self.numFeatures, freq.items())","['def', 'transform', '(', 'self', ',', 'document', ')', ':', 'if', 'isinstance', '(', 'document', ',', 'RDD', ')', ':', 'return', 'document', '.', 'map', '(', 'self', '.', 'transform', ')', 'freq', '=', '{', '}', 'for', 'term', 'in', 'document', ':', 'i', '=', 'self', '.', 'indexOf', '(', 'term', ')', 'freq', '[', 'i', ']', '=', '1.0', 'if', 'self', '.', 'binary', 'else', 'freq', '.', 'get', '(', 'i', ',', '0', ')', '+', '1.0', 'return', 'Vectors', '.', 'sparse', '(', 'self', '.', 'numFeatures', ',', 'freq', '.', 'items', '(', ')', ')']","Transforms the input document (list of terms) to term frequency
        vectors, or transform the RDD of document to RDD of term
        frequency vectors.","['Transforms', 'the', 'input', 'document', '(', 'list', 'of', 'terms', ')', 'to', 'term', 'frequency', 'vectors', 'or', 'transform', 'the', 'RDD', 'of', 'document', 'to', 'RDD', 'of', 'term', 'frequency', 'vectors', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L473-L486,train,"Transforms the input document to term frequency
        vectors or transform the RDD of document to RDD of term frequency
        vectors or transform the input document to RDD of term frequency
        vectors or transform the RDD of document to RDD of term frequency
        vectors."
apache/spark,python/pyspark/mllib/feature.py,IDF.fit,"def fit(self, dataset):
        """"""
        Computes the inverse document frequency.

        :param dataset: an RDD of term frequency vectors
        """"""
        if not isinstance(dataset, RDD):
            raise TypeError(""dataset should be an RDD of term frequency vectors"")
        jmodel = callMLlibFunc(""fitIDF"", self.minDocFreq, dataset.map(_convert_to_vector))
        return IDFModel(jmodel)",python,"def fit(self, dataset):
        """"""
        Computes the inverse document frequency.

        :param dataset: an RDD of term frequency vectors
        """"""
        if not isinstance(dataset, RDD):
            raise TypeError(""dataset should be an RDD of term frequency vectors"")
        jmodel = callMLlibFunc(""fitIDF"", self.minDocFreq, dataset.map(_convert_to_vector))
        return IDFModel(jmodel)","['def', 'fit', '(', 'self', ',', 'dataset', ')', ':', 'if', 'not', 'isinstance', '(', 'dataset', ',', 'RDD', ')', ':', 'raise', 'TypeError', '(', '""dataset should be an RDD of term frequency vectors""', ')', 'jmodel', '=', 'callMLlibFunc', '(', '""fitIDF""', ',', 'self', '.', 'minDocFreq', ',', 'dataset', '.', 'map', '(', '_convert_to_vector', ')', ')', 'return', 'IDFModel', '(', 'jmodel', ')']","Computes the inverse document frequency.

        :param dataset: an RDD of term frequency vectors","['Computes', 'the', 'inverse', 'document', 'frequency', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L577-L586,train,Computes the inverse document frequency of the given term frequency vectors.
apache/spark,python/pyspark/mllib/feature.py,Word2VecModel.findSynonyms,"def findSynonyms(self, word, num):
        """"""
        Find synonyms of a word

        :param word: a word or a vector representation of word
        :param num: number of synonyms to find
        :return: array of (word, cosineSimilarity)

        .. note:: Local use only
        """"""
        if not isinstance(word, basestring):
            word = _convert_to_vector(word)
        words, similarity = self.call(""findSynonyms"", word, num)
        return zip(words, similarity)",python,"def findSynonyms(self, word, num):
        """"""
        Find synonyms of a word

        :param word: a word or a vector representation of word
        :param num: number of synonyms to find
        :return: array of (word, cosineSimilarity)

        .. note:: Local use only
        """"""
        if not isinstance(word, basestring):
            word = _convert_to_vector(word)
        words, similarity = self.call(""findSynonyms"", word, num)
        return zip(words, similarity)","['def', 'findSynonyms', '(', 'self', ',', 'word', ',', 'num', ')', ':', 'if', 'not', 'isinstance', '(', 'word', ',', 'basestring', ')', ':', 'word', '=', '_convert_to_vector', '(', 'word', ')', 'words', ',', 'similarity', '=', 'self', '.', 'call', '(', '""findSynonyms""', ',', 'word', ',', 'num', ')', 'return', 'zip', '(', 'words', ',', 'similarity', ')']","Find synonyms of a word

        :param word: a word or a vector representation of word
        :param num: number of synonyms to find
        :return: array of (word, cosineSimilarity)

        .. note:: Local use only","['Find', 'synonyms', 'of', 'a', 'word']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L611-L624,train,Find synonyms of a word.
apache/spark,python/pyspark/mllib/feature.py,Word2VecModel.load,"def load(cls, sc, path):
        """"""
        Load a model from the given path.
        """"""
        jmodel = sc._jvm.org.apache.spark.mllib.feature \
            .Word2VecModel.load(sc._jsc.sc(), path)
        model = sc._jvm.org.apache.spark.mllib.api.python.Word2VecModelWrapper(jmodel)
        return Word2VecModel(model)",python,"def load(cls, sc, path):
        """"""
        Load a model from the given path.
        """"""
        jmodel = sc._jvm.org.apache.spark.mllib.feature \
            .Word2VecModel.load(sc._jsc.sc(), path)
        model = sc._jvm.org.apache.spark.mllib.api.python.Word2VecModelWrapper(jmodel)
        return Word2VecModel(model)","['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'jmodel', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'feature', '.', 'Word2VecModel', '.', 'load', '(', 'sc', '.', '_jsc', '.', 'sc', '(', ')', ',', 'path', ')', 'model', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'Word2VecModelWrapper', '(', 'jmodel', ')', 'return', 'Word2VecModel', '(', 'model', ')']",Load a model from the given path.,"['Load', 'a', 'model', 'from', 'the', 'given', 'path', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L635-L642,train,Loads a word2vec model from the given path.
apache/spark,python/pyspark/mllib/feature.py,ElementwiseProduct.transform,"def transform(self, vector):
        """"""
        Computes the Hadamard product of the vector.
        """"""
        if isinstance(vector, RDD):
            vector = vector.map(_convert_to_vector)

        else:
            vector = _convert_to_vector(vector)
        return callMLlibFunc(""elementwiseProductVector"", self.scalingVector, vector)",python,"def transform(self, vector):
        """"""
        Computes the Hadamard product of the vector.
        """"""
        if isinstance(vector, RDD):
            vector = vector.map(_convert_to_vector)

        else:
            vector = _convert_to_vector(vector)
        return callMLlibFunc(""elementwiseProductVector"", self.scalingVector, vector)","['def', 'transform', '(', 'self', ',', 'vector', ')', ':', 'if', 'isinstance', '(', 'vector', ',', 'RDD', ')', ':', 'vector', '=', 'vector', '.', 'map', '(', '_convert_to_vector', ')', 'else', ':', 'vector', '=', '_convert_to_vector', '(', 'vector', ')', 'return', 'callMLlibFunc', '(', '""elementwiseProductVector""', ',', 'self', '.', 'scalingVector', ',', 'vector', ')']",Computes the Hadamard product of the vector.,"['Computes', 'the', 'Hadamard', 'product', 'of', 'the', 'vector', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/feature.py#L810-L819,train,Computes the Hadamard product of the vector.
apache/spark,python/pyspark/mllib/tree.py,TreeEnsembleModel.predict,"def predict(self, x):
        """"""
        Predict values for a single data point or an RDD of points using
        the model trained.

        .. note:: In Python, predict cannot currently be used within an RDD
            transformation or action.
            Call predict directly on the RDD instead.
        """"""
        if isinstance(x, RDD):
            return self.call(""predict"", x.map(_convert_to_vector))

        else:
            return self.call(""predict"", _convert_to_vector(x))",python,"def predict(self, x):
        """"""
        Predict values for a single data point or an RDD of points using
        the model trained.

        .. note:: In Python, predict cannot currently be used within an RDD
            transformation or action.
            Call predict directly on the RDD instead.
        """"""
        if isinstance(x, RDD):
            return self.call(""predict"", x.map(_convert_to_vector))

        else:
            return self.call(""predict"", _convert_to_vector(x))","['def', 'predict', '(', 'self', ',', 'x', ')', ':', 'if', 'isinstance', '(', 'x', ',', 'RDD', ')', ':', 'return', 'self', '.', 'call', '(', '""predict""', ',', 'x', '.', 'map', '(', '_convert_to_vector', ')', ')', 'else', ':', 'return', 'self', '.', 'call', '(', '""predict""', ',', '_convert_to_vector', '(', 'x', ')', ')']","Predict values for a single data point or an RDD of points using
        the model trained.

        .. note:: In Python, predict cannot currently be used within an RDD
            transformation or action.
            Call predict directly on the RDD instead.","['Predict', 'values', 'for', 'a', 'single', 'data', 'point', 'or', 'an', 'RDD', 'of', 'points', 'using', 'the', 'model', 'trained', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L39-L52,train,Predict values for a single data point or an RDD of points.
apache/spark,python/pyspark/mllib/tree.py,DecisionTree.trainClassifier,"def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,
                        impurity=""gini"", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                        minInfoGain=0.0):
        """"""
        Train a decision tree model for classification.

        :param data:
          Training data: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from numpy import array
        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(1.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})
        >>> print(model)
        DecisionTreeModel classifier of depth 1 with 3 nodes

        >>> print(model.toDebugString())
        DecisionTreeModel classifier of depth 1 with 3 nodes
          If (feature 0 <= 0.5)
           Predict: 0.0
          Else (feature 0 > 0.5)
           Predict: 1.0
        <BLANKLINE>
        >>> model.predict(array([1.0]))
        1.0
        >>> model.predict(array([0.0]))
        0.0
        >>> rdd = sc.parallelize([[1.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", numClasses, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)",python,"def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,
                        impurity=""gini"", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                        minInfoGain=0.0):
        """"""
        Train a decision tree model for classification.

        :param data:
          Training data: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from numpy import array
        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(1.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})
        >>> print(model)
        DecisionTreeModel classifier of depth 1 with 3 nodes

        >>> print(model.toDebugString())
        DecisionTreeModel classifier of depth 1 with 3 nodes
          If (feature 0 <= 0.5)
           Predict: 0.0
          Else (feature 0 > 0.5)
           Predict: 1.0
        <BLANKLINE>
        >>> model.predict(array([1.0]))
        1.0
        >>> model.predict(array([0.0]))
        0.0
        >>> rdd = sc.parallelize([[1.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", numClasses, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)","['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'impurity', '=', '""gini""', ',', 'maxDepth', '=', '5', ',', 'maxBins', '=', '32', ',', 'minInstancesPerNode', '=', '1', ',', 'minInfoGain', '=', '0.0', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '""classification""', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'minInstancesPerNode', ',', 'minInfoGain', ')']","Train a decision tree model for classification.

        :param data:
          Training data: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from numpy import array
        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(1.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})
        >>> print(model)
        DecisionTreeModel classifier of depth 1 with 3 nodes

        >>> print(model.toDebugString())
        DecisionTreeModel classifier of depth 1 with 3 nodes
          If (feature 0 <= 0.5)
           Predict: 0.0
          Else (feature 0 > 0.5)
           Predict: 1.0
        <BLANKLINE>
        >>> model.predict(array([1.0]))
        1.0
        >>> model.predict(array([0.0]))
        0.0
        >>> rdd = sc.parallelize([[1.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]","['Train', 'a', 'decision', 'tree', 'model', 'for', 'classification', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L149-L217,train,Train a decision tree model for classification.
apache/spark,python/pyspark/mllib/tree.py,DecisionTree.trainRegressor,"def trainRegressor(cls, data, categoricalFeaturesInfo,
                       impurity=""variance"", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                       minInfoGain=0.0):
        """"""
        Train a decision tree model for regression.

        :param data:
          Training data: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {1: 0.0}))
        0.0
        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""regression"", 0, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)",python,"def trainRegressor(cls, data, categoricalFeaturesInfo,
                       impurity=""variance"", maxDepth=5, maxBins=32, minInstancesPerNode=1,
                       minInfoGain=0.0):
        """"""
        Train a decision tree model for regression.

        :param data:
          Training data: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {1: 0.0}))
        0.0
        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""regression"", 0, categoricalFeaturesInfo,
                          impurity, maxDepth, maxBins, minInstancesPerNode, minInfoGain)","['def', 'trainRegressor', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'impurity', '=', '""variance""', ',', 'maxDepth', '=', '5', ',', 'maxBins', '=', '32', ',', 'minInstancesPerNode', '=', '1', ',', 'minInfoGain', '=', '0.0', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '""regression""', ',', '0', ',', 'categoricalFeaturesInfo', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'minInstancesPerNode', ',', 'minInfoGain', ')']","Train a decision tree model for regression.

        :param data:
          Training data: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 5)
        :param maxBins:
          Number of bins used for finding splits at each node.
          (default: 32)
        :param minInstancesPerNode:
          Minimum number of instances required at child nodes to create
          the parent split.
          (default: 1)
        :param minInfoGain:
          Minimum info gain required to create a split.
          (default: 0.0)
        :return:
          DecisionTreeModel.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import DecisionTree
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {1: 0.0}))
        0.0
        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]","['Train', 'a', 'decision', 'tree', 'model', 'for', 'regression', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L221-L277,train,Train a Decision Tree model for regression.
apache/spark,python/pyspark/mllib/tree.py,RandomForest.trainClassifier,"def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,
                        featureSubsetStrategy=""auto"", impurity=""gini"", maxDepth=4, maxBins=32,
                        seed=None):
        """"""
        Train a random forest model for binary or multiclass
        classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""sqrt"".
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)
        >>> model.numTrees()
        3
        >>> model.totalNumNodes()
        7
        >>> print(model)
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
        >>> print(model.toDebugString())
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
          Tree 0:
            Predict: 1.0
          Tree 1:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
          Tree 2:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[3.0], [1.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", numClasses,
                          categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity,
                          maxDepth, maxBins, seed)",python,"def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,
                        featureSubsetStrategy=""auto"", impurity=""gini"", maxDepth=4, maxBins=32,
                        seed=None):
        """"""
        Train a random forest model for binary or multiclass
        classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""sqrt"".
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)
        >>> model.numTrees()
        3
        >>> model.totalNumNodes()
        7
        >>> print(model)
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
        >>> print(model.toDebugString())
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
          Tree 0:
            Predict: 1.0
          Tree 1:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
          Tree 2:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[3.0], [1.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", numClasses,
                          categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity,
                          maxDepth, maxBins, seed)","['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', '=', '""auto""', ',', 'impurity', '=', '""gini""', ',', 'maxDepth', '=', '4', ',', 'maxBins', '=', '32', ',', 'seed', '=', 'None', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '""classification""', ',', 'numClasses', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'seed', ')']","Train a random forest model for binary or multiclass
        classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1, ..., numClasses-1}.
        :param numClasses:
          Number of classes for classification.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""sqrt"".
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          Supported values: ""gini"" or ""entropy"".
          (default: ""gini"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)
        >>> model.numTrees()
        3
        >>> model.totalNumNodes()
        7
        >>> print(model)
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
        >>> print(model.toDebugString())
        TreeEnsembleModel classifier with 3 trees
        <BLANKLINE>
          Tree 0:
            Predict: 1.0
          Tree 1:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
          Tree 2:
            If (feature 0 <= 1.5)
             Predict: 0.0
            Else (feature 0 > 1.5)
             Predict: 1.0
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[3.0], [1.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]","['Train', 'a', 'random', 'forest', 'model', 'for', 'binary', 'or', 'multiclass', 'classification', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L319-L407,train,Train a RandomForestModel for binary or multiclass classification.
apache/spark,python/pyspark/mllib/tree.py,RandomForest.trainRegressor,"def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy=""auto"",
                       impurity=""variance"", maxDepth=4, maxBins=32, seed=None):
        """"""
        Train a random forest model for regression.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""onethird"" for regression.
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)
        >>> model.numTrees()
        2
        >>> model.totalNumNodes()
        4
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {0: 1.0}))
        0.5
        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.5]
        """"""
        return cls._train(data, ""regression"", 0, categoricalFeaturesInfo, numTrees,
                          featureSubsetStrategy, impurity, maxDepth, maxBins, seed)",python,"def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy=""auto"",
                       impurity=""variance"", maxDepth=4, maxBins=32, seed=None):
        """"""
        Train a random forest model for regression.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""onethird"" for regression.
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)
        >>> model.numTrees()
        2
        >>> model.totalNumNodes()
        4
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {0: 1.0}))
        0.5
        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.5]
        """"""
        return cls._train(data, ""regression"", 0, categoricalFeaturesInfo, numTrees,
                          featureSubsetStrategy, impurity, maxDepth, maxBins, seed)","['def', 'trainRegressor', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', '=', '""auto""', ',', 'impurity', '=', '""variance""', ',', 'maxDepth', '=', '4', ',', 'maxBins', '=', '32', ',', 'seed', '=', 'None', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '""regression""', ',', '0', ',', 'categoricalFeaturesInfo', ',', 'numTrees', ',', 'featureSubsetStrategy', ',', 'impurity', ',', 'maxDepth', ',', 'maxBins', ',', 'seed', ')']","Train a random forest model for regression.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels are real numbers.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param numTrees:
          Number of trees in the random forest.
        :param featureSubsetStrategy:
          Number of features to consider for splits at each node.
          Supported values: ""auto"", ""all"", ""sqrt"", ""log2"", ""onethird"".
          If ""auto"" is set, this parameter is set based on numTrees:
          if numTrees == 1, set to ""all"";
          if numTrees > 1 (forest) set to ""onethird"" for regression.
          (default: ""auto"")
        :param impurity:
          Criterion used for information gain calculation.
          The only supported value for regression is ""variance"".
          (default: ""variance"")
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 4)
        :param maxBins:
          Maximum number of bins used for splitting features.
          (default: 32)
        :param seed:
          Random seed for bootstrapping and choosing feature subsets.
          Set as None to generate seed based on system time.
          (default: None)
        :return:
          RandomForestModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import RandomForest
        >>> from pyspark.mllib.linalg import SparseVector
        >>>
        >>> sparse_data = [
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),
        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),
        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))
        ... ]
        >>>
        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)
        >>> model.numTrees()
        2
        >>> model.totalNumNodes()
        4
        >>> model.predict(SparseVector(2, {1: 1.0}))
        1.0
        >>> model.predict(SparseVector(2, {0: 1.0}))
        0.5
        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.5]","['Train', 'a', 'random', 'forest', 'model', 'for', 'regression', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L411-L476,train,Train a random forest model for regression.
apache/spark,python/pyspark/mllib/tree.py,GradientBoostedTrees.trainClassifier,"def trainClassifier(cls, data, categoricalFeaturesInfo,
                        loss=""logLoss"", numIterations=100, learningRate=0.1, maxDepth=3,
                        maxBins=32):
        """"""
        Train a gradient-boosted trees model for classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1}.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param loss:
          Loss function used for minimization during gradient boosting.
          Supported values: ""logLoss"", ""leastSquaresError"",
          ""leastAbsoluteError"".
          (default: ""logLoss"")
        :param numIterations:
          Number of iterations of boosting.
          (default: 100)
        :param learningRate:
          Learning rate for shrinking the contribution of each estimator.
          The learning rate should be between in the interval (0, 1].
          (default: 0.1)
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 3)
        :param maxBins:
          Maximum number of bins used for splitting features. DecisionTree
          requires maxBins >= max categories.
          (default: 32)
        :return:
          GradientBoostedTreesModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import GradientBoostedTrees
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>>
        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)
        >>> model.numTrees()
        10
        >>> model.totalNumNodes()
        30
        >>> print(model)  # it already has newline
        TreeEnsembleModel classifier with 10 trees
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[2.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", categoricalFeaturesInfo,
                          loss, numIterations, learningRate, maxDepth, maxBins)",python,"def trainClassifier(cls, data, categoricalFeaturesInfo,
                        loss=""logLoss"", numIterations=100, learningRate=0.1, maxDepth=3,
                        maxBins=32):
        """"""
        Train a gradient-boosted trees model for classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1}.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param loss:
          Loss function used for minimization during gradient boosting.
          Supported values: ""logLoss"", ""leastSquaresError"",
          ""leastAbsoluteError"".
          (default: ""logLoss"")
        :param numIterations:
          Number of iterations of boosting.
          (default: 100)
        :param learningRate:
          Learning rate for shrinking the contribution of each estimator.
          The learning rate should be between in the interval (0, 1].
          (default: 0.1)
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 3)
        :param maxBins:
          Maximum number of bins used for splitting features. DecisionTree
          requires maxBins >= max categories.
          (default: 32)
        :return:
          GradientBoostedTreesModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import GradientBoostedTrees
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>>
        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)
        >>> model.numTrees()
        10
        >>> model.totalNumNodes()
        30
        >>> print(model)  # it already has newline
        TreeEnsembleModel classifier with 10 trees
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[2.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]
        """"""
        return cls._train(data, ""classification"", categoricalFeaturesInfo,
                          loss, numIterations, learningRate, maxDepth, maxBins)","['def', 'trainClassifier', '(', 'cls', ',', 'data', ',', 'categoricalFeaturesInfo', ',', 'loss', '=', '""logLoss""', ',', 'numIterations', '=', '100', ',', 'learningRate', '=', '0.1', ',', 'maxDepth', '=', '3', ',', 'maxBins', '=', '32', ')', ':', 'return', 'cls', '.', '_train', '(', 'data', ',', '""classification""', ',', 'categoricalFeaturesInfo', ',', 'loss', ',', 'numIterations', ',', 'learningRate', ',', 'maxDepth', ',', 'maxBins', ')']","Train a gradient-boosted trees model for classification.

        :param data:
          Training dataset: RDD of LabeledPoint. Labels should take values
          {0, 1}.
        :param categoricalFeaturesInfo:
          Map storing arity of categorical features. An entry (n -> k)
          indicates that feature n is categorical with k categories
          indexed from 0: {0, 1, ..., k-1}.
        :param loss:
          Loss function used for minimization during gradient boosting.
          Supported values: ""logLoss"", ""leastSquaresError"",
          ""leastAbsoluteError"".
          (default: ""logLoss"")
        :param numIterations:
          Number of iterations of boosting.
          (default: 100)
        :param learningRate:
          Learning rate for shrinking the contribution of each estimator.
          The learning rate should be between in the interval (0, 1].
          (default: 0.1)
        :param maxDepth:
          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1
          means 1 internal node + 2 leaf nodes).
          (default: 3)
        :param maxBins:
          Maximum number of bins used for splitting features. DecisionTree
          requires maxBins >= max categories.
          (default: 32)
        :return:
          GradientBoostedTreesModel that can be used for prediction.

        Example usage:

        >>> from pyspark.mllib.regression import LabeledPoint
        >>> from pyspark.mllib.tree import GradientBoostedTrees
        >>>
        >>> data = [
        ...     LabeledPoint(0.0, [0.0]),
        ...     LabeledPoint(0.0, [1.0]),
        ...     LabeledPoint(1.0, [2.0]),
        ...     LabeledPoint(1.0, [3.0])
        ... ]
        >>>
        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)
        >>> model.numTrees()
        10
        >>> model.totalNumNodes()
        30
        >>> print(model)  # it already has newline
        TreeEnsembleModel classifier with 10 trees
        <BLANKLINE>
        >>> model.predict([2.0])
        1.0
        >>> model.predict([0.0])
        0.0
        >>> rdd = sc.parallelize([[2.0], [0.0]])
        >>> model.predict(rdd).collect()
        [1.0, 0.0]","['Train', 'a', 'gradient', '-', 'boosted', 'trees', 'model', 'for', 'classification', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/tree.py#L511-L576,train,Train a gradient - boosted trees model for classification.
apache/spark,python/pyspark/conf.py,SparkConf.set,"def set(self, key, value):
        """"""Set a configuration property.""""""
        # Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.
        if self._jconf is not None:
            self._jconf.set(key, unicode(value))
        else:
            self._conf[key] = unicode(value)
        return self",python,"def set(self, key, value):
        """"""Set a configuration property.""""""
        # Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.
        if self._jconf is not None:
            self._jconf.set(key, unicode(value))
        else:
            self._conf[key] = unicode(value)
        return self","['def', 'set', '(', 'self', ',', 'key', ',', 'value', ')', ':', '# Try to set self._jconf first if JVM is created, set self._conf if JVM is not created yet.', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'self', '.', '_jconf', '.', 'set', '(', 'key', ',', 'unicode', '(', 'value', ')', ')', 'else', ':', 'self', '.', '_conf', '[', 'key', ']', '=', 'unicode', '(', 'value', ')', 'return', 'self']",Set a configuration property.,"['Set', 'a', 'configuration', 'property', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L123-L130,train,Set a configuration property.
apache/spark,python/pyspark/conf.py,SparkConf.setIfMissing,"def setIfMissing(self, key, value):
        """"""Set a configuration property, if not already set.""""""
        if self.get(key) is None:
            self.set(key, value)
        return self",python,"def setIfMissing(self, key, value):
        """"""Set a configuration property, if not already set.""""""
        if self.get(key) is None:
            self.set(key, value)
        return self","['def', 'setIfMissing', '(', 'self', ',', 'key', ',', 'value', ')', ':', 'if', 'self', '.', 'get', '(', 'key', ')', 'is', 'None', ':', 'self', '.', 'set', '(', 'key', ',', 'value', ')', 'return', 'self']","Set a configuration property, if not already set.","['Set', 'a', 'configuration', 'property', 'if', 'not', 'already', 'set', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L132-L136,train,Set a configuration property if not already set.
apache/spark,python/pyspark/conf.py,SparkConf.setExecutorEnv,"def setExecutorEnv(self, key=None, value=None, pairs=None):
        """"""Set an environment variable to be passed to executors.""""""
        if (key is not None and pairs is not None) or (key is None and pairs is None):
            raise Exception(""Either pass one key-value pair or a list of pairs"")
        elif key is not None:
            self.set(""spark.executorEnv."" + key, value)
        elif pairs is not None:
            for (k, v) in pairs:
                self.set(""spark.executorEnv."" + k, v)
        return self",python,"def setExecutorEnv(self, key=None, value=None, pairs=None):
        """"""Set an environment variable to be passed to executors.""""""
        if (key is not None and pairs is not None) or (key is None and pairs is None):
            raise Exception(""Either pass one key-value pair or a list of pairs"")
        elif key is not None:
            self.set(""spark.executorEnv."" + key, value)
        elif pairs is not None:
            for (k, v) in pairs:
                self.set(""spark.executorEnv."" + k, v)
        return self","['def', 'setExecutorEnv', '(', 'self', ',', 'key', '=', 'None', ',', 'value', '=', 'None', ',', 'pairs', '=', 'None', ')', ':', 'if', '(', 'key', 'is', 'not', 'None', 'and', 'pairs', 'is', 'not', 'None', ')', 'or', '(', 'key', 'is', 'None', 'and', 'pairs', 'is', 'None', ')', ':', 'raise', 'Exception', '(', '""Either pass one key-value pair or a list of pairs""', ')', 'elif', 'key', 'is', 'not', 'None', ':', 'self', '.', 'set', '(', '""spark.executorEnv.""', '+', 'key', ',', 'value', ')', 'elif', 'pairs', 'is', 'not', 'None', ':', 'for', '(', 'k', ',', 'v', ')', 'in', 'pairs', ':', 'self', '.', 'set', '(', '""spark.executorEnv.""', '+', 'k', ',', 'v', ')', 'return', 'self']",Set an environment variable to be passed to executors.,"['Set', 'an', 'environment', 'variable', 'to', 'be', 'passed', 'to', 'executors', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L153-L162,train,Set an environment variable to be passed to executors.
apache/spark,python/pyspark/conf.py,SparkConf.setAll,"def setAll(self, pairs):
        """"""
        Set multiple parameters, passed as a list of key-value pairs.

        :param pairs: list of key-value pairs to set
        """"""
        for (k, v) in pairs:
            self.set(k, v)
        return self",python,"def setAll(self, pairs):
        """"""
        Set multiple parameters, passed as a list of key-value pairs.

        :param pairs: list of key-value pairs to set
        """"""
        for (k, v) in pairs:
            self.set(k, v)
        return self","['def', 'setAll', '(', 'self', ',', 'pairs', ')', ':', 'for', '(', 'k', ',', 'v', ')', 'in', 'pairs', ':', 'self', '.', 'set', '(', 'k', ',', 'v', ')', 'return', 'self']","Set multiple parameters, passed as a list of key-value pairs.

        :param pairs: list of key-value pairs to set","['Set', 'multiple', 'parameters', 'passed', 'as', 'a', 'list', 'of', 'key', '-', 'value', 'pairs', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L164-L172,train,Set multiple parameters passed as a list of key - value pairs.
apache/spark,python/pyspark/conf.py,SparkConf.get,"def get(self, key, defaultValue=None):
        """"""Get the configured value for some key, or return a default otherwise.""""""
        if defaultValue is None:   # Py4J doesn't call the right get() if we pass None
            if self._jconf is not None:
                if not self._jconf.contains(key):
                    return None
                return self._jconf.get(key)
            else:
                if key not in self._conf:
                    return None
                return self._conf[key]
        else:
            if self._jconf is not None:
                return self._jconf.get(key, defaultValue)
            else:
                return self._conf.get(key, defaultValue)",python,"def get(self, key, defaultValue=None):
        """"""Get the configured value for some key, or return a default otherwise.""""""
        if defaultValue is None:   # Py4J doesn't call the right get() if we pass None
            if self._jconf is not None:
                if not self._jconf.contains(key):
                    return None
                return self._jconf.get(key)
            else:
                if key not in self._conf:
                    return None
                return self._conf[key]
        else:
            if self._jconf is not None:
                return self._jconf.get(key, defaultValue)
            else:
                return self._conf.get(key, defaultValue)","['def', 'get', '(', 'self', ',', 'key', ',', 'defaultValue', '=', 'None', ')', ':', 'if', 'defaultValue', 'is', 'None', ':', ""# Py4J doesn't call the right get() if we pass None"", 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'if', 'not', 'self', '.', '_jconf', '.', 'contains', '(', 'key', ')', ':', 'return', 'None', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ')', 'else', ':', 'if', 'key', 'not', 'in', 'self', '.', '_conf', ':', 'return', 'None', 'return', 'self', '.', '_conf', '[', 'key', ']', 'else', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'get', '(', 'key', ',', 'defaultValue', ')', 'else', ':', 'return', 'self', '.', '_conf', '.', 'get', '(', 'key', ',', 'defaultValue', ')']","Get the configured value for some key, or return a default otherwise.","['Get', 'the', 'configured', 'value', 'for', 'some', 'key', 'or', 'return', 'a', 'default', 'otherwise', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L174-L189,train,Get the configured value for some key or return a default otherwise.
apache/spark,python/pyspark/conf.py,SparkConf.getAll,"def getAll(self):
        """"""Get all values as a list of key-value pairs.""""""
        if self._jconf is not None:
            return [(elem._1(), elem._2()) for elem in self._jconf.getAll()]
        else:
            return self._conf.items()",python,"def getAll(self):
        """"""Get all values as a list of key-value pairs.""""""
        if self._jconf is not None:
            return [(elem._1(), elem._2()) for elem in self._jconf.getAll()]
        else:
            return self._conf.items()","['def', 'getAll', '(', 'self', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', '[', '(', 'elem', '.', '_1', '(', ')', ',', 'elem', '.', '_2', '(', ')', ')', 'for', 'elem', 'in', 'self', '.', '_jconf', '.', 'getAll', '(', ')', ']', 'else', ':', 'return', 'self', '.', '_conf', '.', 'items', '(', ')']",Get all values as a list of key-value pairs.,"['Get', 'all', 'values', 'as', 'a', 'list', 'of', 'key', '-', 'value', 'pairs', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L191-L196,train,Get all values as a list of key - value pairs.
apache/spark,python/pyspark/conf.py,SparkConf.contains,"def contains(self, key):
        """"""Does this configuration contain a given key?""""""
        if self._jconf is not None:
            return self._jconf.contains(key)
        else:
            return key in self._conf",python,"def contains(self, key):
        """"""Does this configuration contain a given key?""""""
        if self._jconf is not None:
            return self._jconf.contains(key)
        else:
            return key in self._conf","['def', 'contains', '(', 'self', ',', 'key', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'contains', '(', 'key', ')', 'else', ':', 'return', 'key', 'in', 'self', '.', '_conf']",Does this configuration contain a given key?,"['Does', 'this', 'configuration', 'contain', 'a', 'given', 'key?']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L198-L203,train,Does this configuration contain a given key?
apache/spark,python/pyspark/conf.py,SparkConf.toDebugString,"def toDebugString(self):
        """"""
        Returns a printable version of the configuration, as a list of
        key=value pairs, one per line.
        """"""
        if self._jconf is not None:
            return self._jconf.toDebugString()
        else:
            return '\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())",python,"def toDebugString(self):
        """"""
        Returns a printable version of the configuration, as a list of
        key=value pairs, one per line.
        """"""
        if self._jconf is not None:
            return self._jconf.toDebugString()
        else:
            return '\n'.join('%s=%s' % (k, v) for k, v in self._conf.items())","['def', 'toDebugString', '(', 'self', ')', ':', 'if', 'self', '.', '_jconf', 'is', 'not', 'None', ':', 'return', 'self', '.', '_jconf', '.', 'toDebugString', '(', ')', 'else', ':', 'return', ""'\\n'"", '.', 'join', '(', ""'%s=%s'"", '%', '(', 'k', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'self', '.', '_conf', '.', 'items', '(', ')', ')']","Returns a printable version of the configuration, as a list of
        key=value pairs, one per line.","['Returns', 'a', 'printable', 'version', 'of', 'the', 'configuration', 'as', 'a', 'list', 'of', 'key', '=', 'value', 'pairs', 'one', 'per', 'line', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/conf.py#L205-L213,train,Returns a printable version of the configuration as a list of key = value pairs one per line.
apache/spark,python/pyspark/sql/catalog.py,Catalog.listDatabases,"def listDatabases(self):
        """"""Returns a list of databases available across all sessions.""""""
        iter = self._jcatalog.listDatabases().toLocalIterator()
        databases = []
        while iter.hasNext():
            jdb = iter.next()
            databases.append(Database(
                name=jdb.name(),
                description=jdb.description(),
                locationUri=jdb.locationUri()))
        return databases",python,"def listDatabases(self):
        """"""Returns a list of databases available across all sessions.""""""
        iter = self._jcatalog.listDatabases().toLocalIterator()
        databases = []
        while iter.hasNext():
            jdb = iter.next()
            databases.append(Database(
                name=jdb.name(),
                description=jdb.description(),
                locationUri=jdb.locationUri()))
        return databases","['def', 'listDatabases', '(', 'self', ')', ':', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listDatabases', '(', ')', '.', 'toLocalIterator', '(', ')', 'databases', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jdb', '=', 'iter', '.', 'next', '(', ')', 'databases', '.', 'append', '(', 'Database', '(', 'name', '=', 'jdb', '.', 'name', '(', ')', ',', 'description', '=', 'jdb', '.', 'description', '(', ')', ',', 'locationUri', '=', 'jdb', '.', 'locationUri', '(', ')', ')', ')', 'return', 'databases']",Returns a list of databases available across all sessions.,"['Returns', 'a', 'list', 'of', 'databases', 'available', 'across', 'all', 'sessions', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L61-L71,train,Returns a list of databases available across all sessions.
apache/spark,python/pyspark/sql/catalog.py,Catalog.listTables,"def listTables(self, dbName=None):
        """"""Returns a list of tables/views in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary views.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listTables(dbName).toLocalIterator()
        tables = []
        while iter.hasNext():
            jtable = iter.next()
            tables.append(Table(
                name=jtable.name(),
                database=jtable.database(),
                description=jtable.description(),
                tableType=jtable.tableType(),
                isTemporary=jtable.isTemporary()))
        return tables",python,"def listTables(self, dbName=None):
        """"""Returns a list of tables/views in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary views.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listTables(dbName).toLocalIterator()
        tables = []
        while iter.hasNext():
            jtable = iter.next()
            tables.append(Table(
                name=jtable.name(),
                database=jtable.database(),
                description=jtable.description(),
                tableType=jtable.tableType(),
                isTemporary=jtable.isTemporary()))
        return tables","['def', 'listTables', '(', 'self', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listTables', '(', 'dbName', ')', '.', 'toLocalIterator', '(', ')', 'tables', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jtable', '=', 'iter', '.', 'next', '(', ')', 'tables', '.', 'append', '(', 'Table', '(', 'name', '=', 'jtable', '.', 'name', '(', ')', ',', 'database', '=', 'jtable', '.', 'database', '(', ')', ',', 'description', '=', 'jtable', '.', 'description', '(', ')', ',', 'tableType', '=', 'jtable', '.', 'tableType', '(', ')', ',', 'isTemporary', '=', 'jtable', '.', 'isTemporary', '(', ')', ')', ')', 'return', 'tables']","Returns a list of tables/views in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary views.","['Returns', 'a', 'list', 'of', 'tables', '/', 'views', 'in', 'the', 'specified', 'database', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L75-L93,train,Returns a list of tables in the specified database.
apache/spark,python/pyspark/sql/catalog.py,Catalog.listFunctions,"def listFunctions(self, dbName=None):
        """"""Returns a list of functions registered in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary functions.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()
        functions = []
        while iter.hasNext():
            jfunction = iter.next()
            functions.append(Function(
                name=jfunction.name(),
                description=jfunction.description(),
                className=jfunction.className(),
                isTemporary=jfunction.isTemporary()))
        return functions",python,"def listFunctions(self, dbName=None):
        """"""Returns a list of functions registered in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary functions.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listFunctions(dbName).toLocalIterator()
        functions = []
        while iter.hasNext():
            jfunction = iter.next()
            functions.append(Function(
                name=jfunction.name(),
                description=jfunction.description(),
                className=jfunction.className(),
                isTemporary=jfunction.isTemporary()))
        return functions","['def', 'listFunctions', '(', 'self', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listFunctions', '(', 'dbName', ')', '.', 'toLocalIterator', '(', ')', 'functions', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jfunction', '=', 'iter', '.', 'next', '(', ')', 'functions', '.', 'append', '(', 'Function', '(', 'name', '=', 'jfunction', '.', 'name', '(', ')', ',', 'description', '=', 'jfunction', '.', 'description', '(', ')', ',', 'className', '=', 'jfunction', '.', 'className', '(', ')', ',', 'isTemporary', '=', 'jfunction', '.', 'isTemporary', '(', ')', ')', ')', 'return', 'functions']","Returns a list of functions registered in the specified database.

        If no database is specified, the current database is used.
        This includes all temporary functions.","['Returns', 'a', 'list', 'of', 'functions', 'registered', 'in', 'the', 'specified', 'database', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L97-L114,train,Returns a list of functions registered in the specified database.
apache/spark,python/pyspark/sql/catalog.py,Catalog.listColumns,"def listColumns(self, tableName, dbName=None):
        """"""Returns a list of columns for the given table/view in the specified database.

        If no database is specified, the current database is used.

        Note: the order of arguments here is different from that of its JVM counterpart
        because Python does not support method overloading.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()
        columns = []
        while iter.hasNext():
            jcolumn = iter.next()
            columns.append(Column(
                name=jcolumn.name(),
                description=jcolumn.description(),
                dataType=jcolumn.dataType(),
                nullable=jcolumn.nullable(),
                isPartition=jcolumn.isPartition(),
                isBucket=jcolumn.isBucket()))
        return columns",python,"def listColumns(self, tableName, dbName=None):
        """"""Returns a list of columns for the given table/view in the specified database.

        If no database is specified, the current database is used.

        Note: the order of arguments here is different from that of its JVM counterpart
        because Python does not support method overloading.
        """"""
        if dbName is None:
            dbName = self.currentDatabase()
        iter = self._jcatalog.listColumns(dbName, tableName).toLocalIterator()
        columns = []
        while iter.hasNext():
            jcolumn = iter.next()
            columns.append(Column(
                name=jcolumn.name(),
                description=jcolumn.description(),
                dataType=jcolumn.dataType(),
                nullable=jcolumn.nullable(),
                isPartition=jcolumn.isPartition(),
                isBucket=jcolumn.isBucket()))
        return columns","['def', 'listColumns', '(', 'self', ',', 'tableName', ',', 'dbName', '=', 'None', ')', ':', 'if', 'dbName', 'is', 'None', ':', 'dbName', '=', 'self', '.', 'currentDatabase', '(', ')', 'iter', '=', 'self', '.', '_jcatalog', '.', 'listColumns', '(', 'dbName', ',', 'tableName', ')', '.', 'toLocalIterator', '(', ')', 'columns', '=', '[', ']', 'while', 'iter', '.', 'hasNext', '(', ')', ':', 'jcolumn', '=', 'iter', '.', 'next', '(', ')', 'columns', '.', 'append', '(', 'Column', '(', 'name', '=', 'jcolumn', '.', 'name', '(', ')', ',', 'description', '=', 'jcolumn', '.', 'description', '(', ')', ',', 'dataType', '=', 'jcolumn', '.', 'dataType', '(', ')', ',', 'nullable', '=', 'jcolumn', '.', 'nullable', '(', ')', ',', 'isPartition', '=', 'jcolumn', '.', 'isPartition', '(', ')', ',', 'isBucket', '=', 'jcolumn', '.', 'isBucket', '(', ')', ')', ')', 'return', 'columns']","Returns a list of columns for the given table/view in the specified database.

        If no database is specified, the current database is used.

        Note: the order of arguments here is different from that of its JVM counterpart
        because Python does not support method overloading.","['Returns', 'a', 'list', 'of', 'columns', 'for', 'the', 'given', 'table', '/', 'view', 'in', 'the', 'specified', 'database', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L118-L139,train,Returns a list of columns for the given table or view in the specified database.
apache/spark,python/pyspark/sql/catalog.py,Catalog.createExternalTable,"def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):
        """"""Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the external table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created external table.

        :return: :class:`DataFrame`
        """"""
        warnings.warn(
            ""createExternalTable is deprecated since Spark 2.2, please use createTable instead."",
            DeprecationWarning)
        return self.createTable(tableName, path, source, schema, **options)",python,"def createExternalTable(self, tableName, path=None, source=None, schema=None, **options):
        """"""Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the external table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created external table.

        :return: :class:`DataFrame`
        """"""
        warnings.warn(
            ""createExternalTable is deprecated since Spark 2.2, please use createTable instead."",
            DeprecationWarning)
        return self.createTable(tableName, path, source, schema, **options)","['def', 'createExternalTable', '(', 'self', ',', 'tableName', ',', 'path', '=', 'None', ',', 'source', '=', 'None', ',', 'schema', '=', 'None', ',', '*', '*', 'options', ')', ':', 'warnings', '.', 'warn', '(', '""createExternalTable is deprecated since Spark 2.2, please use createTable instead.""', ',', 'DeprecationWarning', ')', 'return', 'self', '.', 'createTable', '(', 'tableName', ',', 'path', ',', 'source', ',', 'schema', ',', '*', '*', 'options', ')']","Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the external table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created external table.

        :return: :class:`DataFrame`","['Creates', 'a', 'table', 'based', 'on', 'the', 'dataset', 'in', 'a', 'data', 'source', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L142-L159,train,Creates an external table based on the dataset in a data source.
apache/spark,python/pyspark/sql/catalog.py,Catalog.createTable,"def createTable(self, tableName, path=None, source=None, schema=None, **options):
        """"""Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is
        created from the data at the given path. Otherwise a managed table is created.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created table.

        :return: :class:`DataFrame`
        """"""
        if path is not None:
            options[""path""] = path
        if source is None:
            source = self._sparkSession._wrapped._conf.defaultDataSourceName()
        if schema is None:
            df = self._jcatalog.createTable(tableName, source, options)
        else:
            if not isinstance(schema, StructType):
                raise TypeError(""schema should be StructType"")
            scala_datatype = self._jsparkSession.parseDataType(schema.json())
            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)
        return DataFrame(df, self._sparkSession._wrapped)",python,"def createTable(self, tableName, path=None, source=None, schema=None, **options):
        """"""Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is
        created from the data at the given path. Otherwise a managed table is created.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created table.

        :return: :class:`DataFrame`
        """"""
        if path is not None:
            options[""path""] = path
        if source is None:
            source = self._sparkSession._wrapped._conf.defaultDataSourceName()
        if schema is None:
            df = self._jcatalog.createTable(tableName, source, options)
        else:
            if not isinstance(schema, StructType):
                raise TypeError(""schema should be StructType"")
            scala_datatype = self._jsparkSession.parseDataType(schema.json())
            df = self._jcatalog.createTable(tableName, source, scala_datatype, options)
        return DataFrame(df, self._sparkSession._wrapped)","['def', 'createTable', '(', 'self', ',', 'tableName', ',', 'path', '=', 'None', ',', 'source', '=', 'None', ',', 'schema', '=', 'None', ',', '*', '*', 'options', ')', ':', 'if', 'path', 'is', 'not', 'None', ':', 'options', '[', '""path""', ']', '=', 'path', 'if', 'source', 'is', 'None', ':', 'source', '=', 'self', '.', '_sparkSession', '.', '_wrapped', '.', '_conf', '.', 'defaultDataSourceName', '(', ')', 'if', 'schema', 'is', 'None', ':', 'df', '=', 'self', '.', '_jcatalog', '.', 'createTable', '(', 'tableName', ',', 'source', ',', 'options', ')', 'else', ':', 'if', 'not', 'isinstance', '(', 'schema', ',', 'StructType', ')', ':', 'raise', 'TypeError', '(', '""schema should be StructType""', ')', 'scala_datatype', '=', 'self', '.', '_jsparkSession', '.', 'parseDataType', '(', 'schema', '.', 'json', '(', ')', ')', 'df', '=', 'self', '.', '_jcatalog', '.', 'createTable', '(', 'tableName', ',', 'source', ',', 'scala_datatype', ',', 'options', ')', 'return', 'DataFrame', '(', 'df', ',', 'self', '.', '_sparkSession', '.', '_wrapped', ')']","Creates a table based on the dataset in a data source.

        It returns the DataFrame associated with the table.

        The data source is specified by the ``source`` and a set of ``options``.
        If ``source`` is not specified, the default data source configured by
        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is
        created from the data at the given path. Otherwise a managed table is created.

        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and
        created table.

        :return: :class:`DataFrame`","['Creates', 'a', 'table', 'based', 'on', 'the', 'dataset', 'in', 'a', 'data', 'source', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/sql/catalog.py#L162-L188,train,Creates a table based on the dataset in a data source.
apache/spark,python/pyspark/taskcontext.py,_load_from_socket,"def _load_from_socket(port, auth_secret):
    """"""
    Load data from a given socket, this is a blocking method thus only return when the socket
    connection has been closed.
    """"""
    (sockfile, sock) = local_connect_and_auth(port, auth_secret)
    # The barrier() call may block forever, so no timeout
    sock.settimeout(None)
    # Make a barrier() function call.
    write_int(BARRIER_FUNCTION, sockfile)
    sockfile.flush()

    # Collect result.
    res = UTF8Deserializer().loads(sockfile)

    # Release resources.
    sockfile.close()
    sock.close()

    return res",python,"def _load_from_socket(port, auth_secret):
    """"""
    Load data from a given socket, this is a blocking method thus only return when the socket
    connection has been closed.
    """"""
    (sockfile, sock) = local_connect_and_auth(port, auth_secret)
    # The barrier() call may block forever, so no timeout
    sock.settimeout(None)
    # Make a barrier() function call.
    write_int(BARRIER_FUNCTION, sockfile)
    sockfile.flush()

    # Collect result.
    res = UTF8Deserializer().loads(sockfile)

    # Release resources.
    sockfile.close()
    sock.close()

    return res","['def', '_load_from_socket', '(', 'port', ',', 'auth_secret', ')', ':', '(', 'sockfile', ',', 'sock', ')', '=', 'local_connect_and_auth', '(', 'port', ',', 'auth_secret', ')', '# The barrier() call may block forever, so no timeout', 'sock', '.', 'settimeout', '(', 'None', ')', '# Make a barrier() function call.', 'write_int', '(', 'BARRIER_FUNCTION', ',', 'sockfile', ')', 'sockfile', '.', 'flush', '(', ')', '# Collect result.', 'res', '=', 'UTF8Deserializer', '(', ')', '.', 'loads', '(', 'sockfile', ')', '# Release resources.', 'sockfile', '.', 'close', '(', ')', 'sock', '.', 'close', '(', ')', 'return', 'res']","Load data from a given socket, this is a blocking method thus only return when the socket
    connection has been closed.","['Load', 'data', 'from', 'a', 'given', 'socket', 'this', 'is', 'a', 'blocking', 'method', 'thus', 'only', 'return', 'when', 'the', 'socket', 'connection', 'has', 'been', 'closed', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/taskcontext.py#L102-L121,train,Load data from a given socket.
apache/spark,python/pyspark/taskcontext.py,BarrierTaskContext._getOrCreate,"def _getOrCreate(cls):
        """"""
        Internal function to get or create global BarrierTaskContext. We need to make sure
        BarrierTaskContext is returned from here because it is needed in python worker reuse
        scenario, see SPARK-25921 for more details.
        """"""
        if not isinstance(cls._taskContext, BarrierTaskContext):
            cls._taskContext = object.__new__(cls)
        return cls._taskContext",python,"def _getOrCreate(cls):
        """"""
        Internal function to get or create global BarrierTaskContext. We need to make sure
        BarrierTaskContext is returned from here because it is needed in python worker reuse
        scenario, see SPARK-25921 for more details.
        """"""
        if not isinstance(cls._taskContext, BarrierTaskContext):
            cls._taskContext = object.__new__(cls)
        return cls._taskContext","['def', '_getOrCreate', '(', 'cls', ')', ':', 'if', 'not', 'isinstance', '(', 'cls', '.', '_taskContext', ',', 'BarrierTaskContext', ')', ':', 'cls', '.', '_taskContext', '=', 'object', '.', '__new__', '(', 'cls', ')', 'return', 'cls', '.', '_taskContext']","Internal function to get or create global BarrierTaskContext. We need to make sure
        BarrierTaskContext is returned from here because it is needed in python worker reuse
        scenario, see SPARK-25921 for more details.","['Internal', 'function', 'to', 'get', 'or', 'create', 'global', 'BarrierTaskContext', '.', 'We', 'need', 'to', 'make', 'sure', 'BarrierTaskContext', 'is', 'returned', 'from', 'here', 'because', 'it', 'is', 'needed', 'in', 'python', 'worker', 'reuse', 'scenario', 'see', 'SPARK', '-', '25921', 'for', 'more', 'details', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/taskcontext.py#L139-L147,train,Internal function to get or create BarrierTaskContext object.
apache/spark,python/pyspark/taskcontext.py,BarrierTaskContext._initialize,"def _initialize(cls, port, secret):
        """"""
        Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called
        after BarrierTaskContext is initialized.
        """"""
        cls._port = port
        cls._secret = secret",python,"def _initialize(cls, port, secret):
        """"""
        Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called
        after BarrierTaskContext is initialized.
        """"""
        cls._port = port
        cls._secret = secret","['def', '_initialize', '(', 'cls', ',', 'port', ',', 'secret', ')', ':', 'cls', '.', '_port', '=', 'port', 'cls', '.', '_secret', '=', 'secret']","Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called
        after BarrierTaskContext is initialized.","['Initialize', 'BarrierTaskContext', 'other', 'methods', 'within', 'BarrierTaskContext', 'can', 'only', 'be', 'called', 'after', 'BarrierTaskContext', 'is', 'initialized', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/taskcontext.py#L163-L169,train,Initialize BarrierTaskContext.
apache/spark,python/pyspark/taskcontext.py,BarrierTaskContext.barrier,"def barrier(self):
        """"""
        .. note:: Experimental

        Sets a global barrier and waits until all tasks in this stage hit this barrier.
        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks
        in the same stage have reached this routine.

        .. warning:: In a barrier stage, each task much have the same number of `barrier()`
            calls, in all possible code branches.
            Otherwise, you may get the job hanging or a SparkException after timeout.

        .. versionadded:: 2.4.0
        """"""
        if self._port is None or self._secret is None:
            raise Exception(""Not supported to call barrier() before initialize "" +
                            ""BarrierTaskContext."")
        else:
            _load_from_socket(self._port, self._secret)",python,"def barrier(self):
        """"""
        .. note:: Experimental

        Sets a global barrier and waits until all tasks in this stage hit this barrier.
        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks
        in the same stage have reached this routine.

        .. warning:: In a barrier stage, each task much have the same number of `barrier()`
            calls, in all possible code branches.
            Otherwise, you may get the job hanging or a SparkException after timeout.

        .. versionadded:: 2.4.0
        """"""
        if self._port is None or self._secret is None:
            raise Exception(""Not supported to call barrier() before initialize "" +
                            ""BarrierTaskContext."")
        else:
            _load_from_socket(self._port, self._secret)","['def', 'barrier', '(', 'self', ')', ':', 'if', 'self', '.', '_port', 'is', 'None', 'or', 'self', '.', '_secret', 'is', 'None', ':', 'raise', 'Exception', '(', '""Not supported to call barrier() before initialize ""', '+', '""BarrierTaskContext.""', ')', 'else', ':', '_load_from_socket', '(', 'self', '.', '_port', ',', 'self', '.', '_secret', ')']",".. note:: Experimental

        Sets a global barrier and waits until all tasks in this stage hit this barrier.
        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks
        in the same stage have reached this routine.

        .. warning:: In a barrier stage, each task much have the same number of `barrier()`
            calls, in all possible code branches.
            Otherwise, you may get the job hanging or a SparkException after timeout.

        .. versionadded:: 2.4.0","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/taskcontext.py#L171-L189,train,Sets a global barrier and waits until all tasks in this stage hit this barrier.
apache/spark,python/pyspark/taskcontext.py,BarrierTaskContext.getTaskInfos,"def getTaskInfos(self):
        """"""
        .. note:: Experimental

        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,
        ordered by partition ID.

        .. versionadded:: 2.4.0
        """"""
        if self._port is None or self._secret is None:
            raise Exception(""Not supported to call getTaskInfos() before initialize "" +
                            ""BarrierTaskContext."")
        else:
            addresses = self._localProperties.get(""addresses"", """")
            return [BarrierTaskInfo(h.strip()) for h in addresses.split("","")]",python,"def getTaskInfos(self):
        """"""
        .. note:: Experimental

        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,
        ordered by partition ID.

        .. versionadded:: 2.4.0
        """"""
        if self._port is None or self._secret is None:
            raise Exception(""Not supported to call getTaskInfos() before initialize "" +
                            ""BarrierTaskContext."")
        else:
            addresses = self._localProperties.get(""addresses"", """")
            return [BarrierTaskInfo(h.strip()) for h in addresses.split("","")]","['def', 'getTaskInfos', '(', 'self', ')', ':', 'if', 'self', '.', '_port', 'is', 'None', 'or', 'self', '.', '_secret', 'is', 'None', ':', 'raise', 'Exception', '(', '""Not supported to call getTaskInfos() before initialize ""', '+', '""BarrierTaskContext.""', ')', 'else', ':', 'addresses', '=', 'self', '.', '_localProperties', '.', 'get', '(', '""addresses""', ',', '""""', ')', 'return', '[', 'BarrierTaskInfo', '(', 'h', '.', 'strip', '(', ')', ')', 'for', 'h', 'in', 'addresses', '.', 'split', '(', '"",""', ')', ']']",".. note:: Experimental

        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,
        ordered by partition ID.

        .. versionadded:: 2.4.0","['..', 'note', '::', 'Experimental']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/taskcontext.py#L191-L205,train,Returns a list of BarrierTaskInfo objects for all tasks in this barrier stage ordered by partition ID.
apache/spark,python/pyspark/__init__.py,since,"def since(version):
    """"""
    A decorator that annotates a function to append the version of Spark the function was added.
    """"""
    import re
    indent_p = re.compile(r'\n( +)')

    def deco(f):
        indents = indent_p.findall(f.__doc__)
        indent = ' ' * (min(len(m) for m in indents) if indents else 0)
        f.__doc__ = f.__doc__.rstrip() + ""\n\n%s.. versionadded:: %s"" % (indent, version)
        return f
    return deco",python,"def since(version):
    """"""
    A decorator that annotates a function to append the version of Spark the function was added.
    """"""
    import re
    indent_p = re.compile(r'\n( +)')

    def deco(f):
        indents = indent_p.findall(f.__doc__)
        indent = ' ' * (min(len(m) for m in indents) if indents else 0)
        f.__doc__ = f.__doc__.rstrip() + ""\n\n%s.. versionadded:: %s"" % (indent, version)
        return f
    return deco","['def', 'since', '(', 'version', ')', ':', 'import', 're', 'indent_p', '=', 're', '.', 'compile', '(', ""r'\\n( +)'"", ')', 'def', 'deco', '(', 'f', ')', ':', 'indents', '=', 'indent_p', '.', 'findall', '(', 'f', '.', '__doc__', ')', 'indent', '=', ""' '"", '*', '(', 'min', '(', 'len', '(', 'm', ')', 'for', 'm', 'in', 'indents', ')', 'if', 'indents', 'else', '0', ')', 'f', '.', '__doc__', '=', 'f', '.', '__doc__', '.', 'rstrip', '(', ')', '+', '""\\n\\n%s.. versionadded:: %s""', '%', '(', 'indent', ',', 'version', ')', 'return', 'f', 'return', 'deco']",A decorator that annotates a function to append the version of Spark the function was added.,"['A', 'decorator', 'that', 'annotates', 'a', 'function', 'to', 'append', 'the', 'version', 'of', 'Spark', 'the', 'function', 'was', 'added', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/__init__.py#L65-L77,train,A decorator that adds the version of Spark the function was added.
apache/spark,python/pyspark/__init__.py,copy_func,"def copy_func(f, name=None, sinceversion=None, doc=None):
    """"""
    Returns a function with same code, globals, defaults, closure, and
    name (or provide a new name).
    """"""
    # See
    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python
    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,
                            f.__closure__)
    # in case f was given attrs (note this dict is a shallow copy):
    fn.__dict__.update(f.__dict__)
    if doc is not None:
        fn.__doc__ = doc
    if sinceversion is not None:
        fn = since(sinceversion)(fn)
    return fn",python,"def copy_func(f, name=None, sinceversion=None, doc=None):
    """"""
    Returns a function with same code, globals, defaults, closure, and
    name (or provide a new name).
    """"""
    # See
    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python
    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,
                            f.__closure__)
    # in case f was given attrs (note this dict is a shallow copy):
    fn.__dict__.update(f.__dict__)
    if doc is not None:
        fn.__doc__ = doc
    if sinceversion is not None:
        fn = since(sinceversion)(fn)
    return fn","['def', 'copy_func', '(', 'f', ',', 'name', '=', 'None', ',', 'sinceversion', '=', 'None', ',', 'doc', '=', 'None', ')', ':', '# See', '# http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python', 'fn', '=', 'types', '.', 'FunctionType', '(', 'f', '.', '__code__', ',', 'f', '.', '__globals__', ',', 'name', 'or', 'f', '.', '__name__', ',', 'f', '.', '__defaults__', ',', 'f', '.', '__closure__', ')', '# in case f was given attrs (note this dict is a shallow copy):', 'fn', '.', '__dict__', '.', 'update', '(', 'f', '.', '__dict__', ')', 'if', 'doc', 'is', 'not', 'None', ':', 'fn', '.', '__doc__', '=', 'doc', 'if', 'sinceversion', 'is', 'not', 'None', ':', 'fn', '=', 'since', '(', 'sinceversion', ')', '(', 'fn', ')', 'return', 'fn']","Returns a function with same code, globals, defaults, closure, and
    name (or provide a new name).","['Returns', 'a', 'function', 'with', 'same', 'code', 'globals', 'defaults', 'closure', 'and', 'name', '(', 'or', 'provide', 'a', 'new', 'name', ')', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/__init__.py#L80-L95,train,Returns a copy of a function f.
apache/spark,python/pyspark/__init__.py,keyword_only,"def keyword_only(func):
    """"""
    A decorator that forces keyword arguments in the wrapped method
    and saves actual input keyword arguments in `_input_kwargs`.

    .. note:: Should only be used to wrap a method where first arg is `self`
    """"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if len(args) > 0:
            raise TypeError(""Method %s forces keyword arguments."" % func.__name__)
        self._input_kwargs = kwargs
        return func(self, **kwargs)
    return wrapper",python,"def keyword_only(func):
    """"""
    A decorator that forces keyword arguments in the wrapped method
    and saves actual input keyword arguments in `_input_kwargs`.

    .. note:: Should only be used to wrap a method where first arg is `self`
    """"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        if len(args) > 0:
            raise TypeError(""Method %s forces keyword arguments."" % func.__name__)
        self._input_kwargs = kwargs
        return func(self, **kwargs)
    return wrapper","['def', 'keyword_only', '(', 'func', ')', ':', '@', 'wraps', '(', 'func', ')', 'def', 'wrapper', '(', 'self', ',', '*', 'args', ',', '*', '*', 'kwargs', ')', ':', 'if', 'len', '(', 'args', ')', '>', '0', ':', 'raise', 'TypeError', '(', '""Method %s forces keyword arguments.""', '%', 'func', '.', '__name__', ')', 'self', '.', '_input_kwargs', '=', 'kwargs', 'return', 'func', '(', 'self', ',', '*', '*', 'kwargs', ')', 'return', 'wrapper']","A decorator that forces keyword arguments in the wrapped method
    and saves actual input keyword arguments in `_input_kwargs`.

    .. note:: Should only be used to wrap a method where first arg is `self`","['A', 'decorator', 'that', 'forces', 'keyword', 'arguments', 'in', 'the', 'wrapped', 'method', 'and', 'saves', 'actual', 'input', 'keyword', 'arguments', 'in', '_input_kwargs', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/__init__.py#L98-L111,train,A decorator that ensures that a method is called with keyword arguments.
apache/spark,python/pyspark/ml/param/_shared_params_code_gen.py,_gen_param_header,"def _gen_param_header(name, doc, defaultValueStr, typeConverter):
    """"""
    Generates the header part for shared variables

    :param name: param name
    :param doc: param doc
    """"""
    template = '''class Has$Name(Params):
    """"""
    Mixin for param $name: $doc
    """"""

    $name = Param(Params._dummy(), ""$name"", ""$doc"", typeConverter=$typeConverter)

    def __init__(self):
        super(Has$Name, self).__init__()'''

    if defaultValueStr is not None:
        template += '''
        self._setDefault($name=$defaultValueStr)'''

    Name = name[0].upper() + name[1:]
    if typeConverter is None:
        typeConverter = str(None)
    return template \
        .replace(""$name"", name) \
        .replace(""$Name"", Name) \
        .replace(""$doc"", doc) \
        .replace(""$defaultValueStr"", str(defaultValueStr)) \
        .replace(""$typeConverter"", typeConverter)",python,"def _gen_param_header(name, doc, defaultValueStr, typeConverter):
    """"""
    Generates the header part for shared variables

    :param name: param name
    :param doc: param doc
    """"""
    template = '''class Has$Name(Params):
    """"""
    Mixin for param $name: $doc
    """"""

    $name = Param(Params._dummy(), ""$name"", ""$doc"", typeConverter=$typeConverter)

    def __init__(self):
        super(Has$Name, self).__init__()'''

    if defaultValueStr is not None:
        template += '''
        self._setDefault($name=$defaultValueStr)'''

    Name = name[0].upper() + name[1:]
    if typeConverter is None:
        typeConverter = str(None)
    return template \
        .replace(""$name"", name) \
        .replace(""$Name"", Name) \
        .replace(""$doc"", doc) \
        .replace(""$defaultValueStr"", str(defaultValueStr)) \
        .replace(""$typeConverter"", typeConverter)","['def', '_gen_param_header', '(', 'name', ',', 'doc', ',', 'defaultValueStr', ',', 'typeConverter', ')', ':', 'template', '=', '\'\'\'class Has$Name(Params):\n    """"""\n    Mixin for param $name: $doc\n    """"""\n\n    $name = Param(Params._dummy(), ""$name"", ""$doc"", typeConverter=$typeConverter)\n\n    def __init__(self):\n        super(Has$Name, self).__init__()\'\'\'', 'if', 'defaultValueStr', 'is', 'not', 'None', ':', 'template', '+=', ""'''\n        self._setDefault($name=$defaultValueStr)'''"", 'Name', '=', 'name', '[', '0', ']', '.', 'upper', '(', ')', '+', 'name', '[', '1', ':', ']', 'if', 'typeConverter', 'is', 'None', ':', 'typeConverter', '=', 'str', '(', 'None', ')', 'return', 'template', '.', 'replace', '(', '""$name""', ',', 'name', ')', '.', 'replace', '(', '""$Name""', ',', 'Name', ')', '.', 'replace', '(', '""$doc""', ',', 'doc', ')', '.', 'replace', '(', '""$defaultValueStr""', ',', 'str', '(', 'defaultValueStr', ')', ')', '.', 'replace', '(', '""$typeConverter""', ',', 'typeConverter', ')']","Generates the header part for shared variables

    :param name: param name
    :param doc: param doc","['Generates', 'the', 'header', 'part', 'for', 'shared', 'variables']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/param/_shared_params_code_gen.py#L41-L70,train,Generates the header part for a param.
apache/spark,python/pyspark/ml/param/_shared_params_code_gen.py,_gen_param_code,"def _gen_param_code(name, doc, defaultValueStr):
    """"""
    Generates Python code for a shared param class.

    :param name: param name
    :param doc: param doc
    :param defaultValueStr: string representation of the default value
    :return: code string
    """"""
    # TODO: How to correctly inherit instance attributes?
    template = '''
    def set$Name(self, value):
        """"""
        Sets the value of :py:attr:`$name`.
        """"""
        return self._set($name=value)

    def get$Name(self):
        """"""
        Gets the value of $name or its default value.
        """"""
        return self.getOrDefault(self.$name)'''

    Name = name[0].upper() + name[1:]
    return template \
        .replace(""$name"", name) \
        .replace(""$Name"", Name) \
        .replace(""$doc"", doc) \
        .replace(""$defaultValueStr"", str(defaultValueStr))",python,"def _gen_param_code(name, doc, defaultValueStr):
    """"""
    Generates Python code for a shared param class.

    :param name: param name
    :param doc: param doc
    :param defaultValueStr: string representation of the default value
    :return: code string
    """"""
    # TODO: How to correctly inherit instance attributes?
    template = '''
    def set$Name(self, value):
        """"""
        Sets the value of :py:attr:`$name`.
        """"""
        return self._set($name=value)

    def get$Name(self):
        """"""
        Gets the value of $name or its default value.
        """"""
        return self.getOrDefault(self.$name)'''

    Name = name[0].upper() + name[1:]
    return template \
        .replace(""$name"", name) \
        .replace(""$Name"", Name) \
        .replace(""$doc"", doc) \
        .replace(""$defaultValueStr"", str(defaultValueStr))","['def', '_gen_param_code', '(', 'name', ',', 'doc', ',', 'defaultValueStr', ')', ':', '# TODO: How to correctly inherit instance attributes?', 'template', '=', '\'\'\'\n    def set$Name(self, value):\n        """"""\n        Sets the value of :py:attr:`$name`.\n        """"""\n        return self._set($name=value)\n\n    def get$Name(self):\n        """"""\n        Gets the value of $name or its default value.\n        """"""\n        return self.getOrDefault(self.$name)\'\'\'', 'Name', '=', 'name', '[', '0', ']', '.', 'upper', '(', ')', '+', 'name', '[', '1', ':', ']', 'return', 'template', '.', 'replace', '(', '""$name""', ',', 'name', ')', '.', 'replace', '(', '""$Name""', ',', 'Name', ')', '.', 'replace', '(', '""$doc""', ',', 'doc', ')', '.', 'replace', '(', '""$defaultValueStr""', ',', 'str', '(', 'defaultValueStr', ')', ')']","Generates Python code for a shared param class.

    :param name: param name
    :param doc: param doc
    :param defaultValueStr: string representation of the default value
    :return: code string","['Generates', 'Python', 'code', 'for', 'a', 'shared', 'param', 'class', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/ml/param/_shared_params_code_gen.py#L73-L101,train,Generates code for a shared param class.
apache/spark,python/pyspark/mllib/clustering.py,BisectingKMeans.train,"def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604):
        """"""
        Runs the bisecting k-means algorithm return the model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          The desired number of leaf clusters. The actual number could
          be smaller if there are no divisible leaf clusters.
          (default: 4)
        :param maxIterations:
          Maximum number of iterations allowed to split clusters.
          (default: 20)
        :param minDivisibleClusterSize:
          Minimum number of points (if >= 1.0) or the minimum proportion
          of points (if < 1.0) of a divisible cluster.
          (default: 1)
        :param seed:
          Random seed value for cluster initialization.
          (default: -1888008604 from classOf[BisectingKMeans].getName.##)
        """"""
        java_model = callMLlibFunc(
            ""trainBisectingKMeans"", rdd.map(_convert_to_vector),
            k, maxIterations, minDivisibleClusterSize, seed)
        return BisectingKMeansModel(java_model)",python,"def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604):
        """"""
        Runs the bisecting k-means algorithm return the model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          The desired number of leaf clusters. The actual number could
          be smaller if there are no divisible leaf clusters.
          (default: 4)
        :param maxIterations:
          Maximum number of iterations allowed to split clusters.
          (default: 20)
        :param minDivisibleClusterSize:
          Minimum number of points (if >= 1.0) or the minimum proportion
          of points (if < 1.0) of a divisible cluster.
          (default: 1)
        :param seed:
          Random seed value for cluster initialization.
          (default: -1888008604 from classOf[BisectingKMeans].getName.##)
        """"""
        java_model = callMLlibFunc(
            ""trainBisectingKMeans"", rdd.map(_convert_to_vector),
            k, maxIterations, minDivisibleClusterSize, seed)
        return BisectingKMeansModel(java_model)","['def', 'train', '(', 'self', ',', 'rdd', ',', 'k', '=', '4', ',', 'maxIterations', '=', '20', ',', 'minDivisibleClusterSize', '=', '1.0', ',', 'seed', '=', '-', '1888008604', ')', ':', 'java_model', '=', 'callMLlibFunc', '(', '""trainBisectingKMeans""', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'maxIterations', ',', 'minDivisibleClusterSize', ',', 'seed', ')', 'return', 'BisectingKMeansModel', '(', 'java_model', ')']","Runs the bisecting k-means algorithm return the model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          The desired number of leaf clusters. The actual number could
          be smaller if there are no divisible leaf clusters.
          (default: 4)
        :param maxIterations:
          Maximum number of iterations allowed to split clusters.
          (default: 20)
        :param minDivisibleClusterSize:
          Minimum number of points (if >= 1.0) or the minimum proportion
          of points (if < 1.0) of a divisible cluster.
          (default: 1)
        :param seed:
          Random seed value for cluster initialization.
          (default: -1888008604 from classOf[BisectingKMeans].getName.##)","['Runs', 'the', 'bisecting', 'k', '-', 'means', 'algorithm', 'return', 'the', 'model', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L142-L167,train,Runs the bisecting k - means algorithm and returns the model.
apache/spark,python/pyspark/mllib/clustering.py,KMeans.train,"def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode=""k-means||"",
              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):
        """"""
        Train a k-means clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of clusters to create.
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param runs:
          This param has no effect since Spark 2.0.0.
        :param initializationMode:
          The initialization algorithm. This can be either ""random"" or
          ""k-means||"".
          (default: ""k-means||"")
        :param seed:
          Random seed value for cluster initialization. Set as None to
          generate seed based on system time.
          (default: None)
        :param initializationSteps:
          Number of steps for the k-means|| initialization mode.
          This is an advanced setting -- the default of 2 is almost
          always enough.
          (default: 2)
        :param epsilon:
          Distance threshold within which a center will be considered to
          have converged. If all centers move less than this Euclidean
          distance, iterations are stopped.
          (default: 1e-4)
        :param initialModel:
          Initial cluster centers can be provided as a KMeansModel object
          rather than using the random or k-means|| initializationModel.
          (default: None)
        """"""
        if runs != 1:
            warnings.warn(""The param `runs` has no effect since Spark 2.0.0."")
        clusterInitialModel = []
        if initialModel is not None:
            if not isinstance(initialModel, KMeansModel):
                raise Exception(""initialModel is of ""+str(type(initialModel))+"". It needs ""
                                ""to be of <type 'KMeansModel'>"")
            clusterInitialModel = [_convert_to_vector(c) for c in initialModel.clusterCenters]
        model = callMLlibFunc(""trainKMeansModel"", rdd.map(_convert_to_vector), k, maxIterations,
                              runs, initializationMode, seed, initializationSteps, epsilon,
                              clusterInitialModel)
        centers = callJavaFunc(rdd.context, model.clusterCenters)
        return KMeansModel([c.toArray() for c in centers])",python,"def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode=""k-means||"",
              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None):
        """"""
        Train a k-means clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of clusters to create.
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param runs:
          This param has no effect since Spark 2.0.0.
        :param initializationMode:
          The initialization algorithm. This can be either ""random"" or
          ""k-means||"".
          (default: ""k-means||"")
        :param seed:
          Random seed value for cluster initialization. Set as None to
          generate seed based on system time.
          (default: None)
        :param initializationSteps:
          Number of steps for the k-means|| initialization mode.
          This is an advanced setting -- the default of 2 is almost
          always enough.
          (default: 2)
        :param epsilon:
          Distance threshold within which a center will be considered to
          have converged. If all centers move less than this Euclidean
          distance, iterations are stopped.
          (default: 1e-4)
        :param initialModel:
          Initial cluster centers can be provided as a KMeansModel object
          rather than using the random or k-means|| initializationModel.
          (default: None)
        """"""
        if runs != 1:
            warnings.warn(""The param `runs` has no effect since Spark 2.0.0."")
        clusterInitialModel = []
        if initialModel is not None:
            if not isinstance(initialModel, KMeansModel):
                raise Exception(""initialModel is of ""+str(type(initialModel))+"". It needs ""
                                ""to be of <type 'KMeansModel'>"")
            clusterInitialModel = [_convert_to_vector(c) for c in initialModel.clusterCenters]
        model = callMLlibFunc(""trainKMeansModel"", rdd.map(_convert_to_vector), k, maxIterations,
                              runs, initializationMode, seed, initializationSteps, epsilon,
                              clusterInitialModel)
        centers = callJavaFunc(rdd.context, model.clusterCenters)
        return KMeansModel([c.toArray() for c in centers])","['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'maxIterations', '=', '100', ',', 'runs', '=', '1', ',', 'initializationMode', '=', '""k-means||""', ',', 'seed', '=', 'None', ',', 'initializationSteps', '=', '2', ',', 'epsilon', '=', '1e-4', ',', 'initialModel', '=', 'None', ')', ':', 'if', 'runs', '!=', '1', ':', 'warnings', '.', 'warn', '(', '""The param `runs` has no effect since Spark 2.0.0.""', ')', 'clusterInitialModel', '=', '[', ']', 'if', 'initialModel', 'is', 'not', 'None', ':', 'if', 'not', 'isinstance', '(', 'initialModel', ',', 'KMeansModel', ')', ':', 'raise', 'Exception', '(', '""initialModel is of ""', '+', 'str', '(', 'type', '(', 'initialModel', ')', ')', '+', '"". It needs ""', '""to be of <type \'KMeansModel\'>""', ')', 'clusterInitialModel', '=', '[', '_convert_to_vector', '(', 'c', ')', 'for', 'c', 'in', 'initialModel', '.', 'clusterCenters', ']', 'model', '=', 'callMLlibFunc', '(', '""trainKMeansModel""', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'maxIterations', ',', 'runs', ',', 'initializationMode', ',', 'seed', ',', 'initializationSteps', ',', 'epsilon', ',', 'clusterInitialModel', ')', 'centers', '=', 'callJavaFunc', '(', 'rdd', '.', 'context', ',', 'model', '.', 'clusterCenters', ')', 'return', 'KMeansModel', '(', '[', 'c', '.', 'toArray', '(', ')', 'for', 'c', 'in', 'centers', ']', ')']","Train a k-means clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of clusters to create.
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param runs:
          This param has no effect since Spark 2.0.0.
        :param initializationMode:
          The initialization algorithm. This can be either ""random"" or
          ""k-means||"".
          (default: ""k-means||"")
        :param seed:
          Random seed value for cluster initialization. Set as None to
          generate seed based on system time.
          (default: None)
        :param initializationSteps:
          Number of steps for the k-means|| initialization mode.
          This is an advanced setting -- the default of 2 is almost
          always enough.
          (default: 2)
        :param epsilon:
          Distance threshold within which a center will be considered to
          have converged. If all centers move less than this Euclidean
          distance, iterations are stopped.
          (default: 1e-4)
        :param initialModel:
          Initial cluster centers can be provided as a KMeansModel object
          rather than using the random or k-means|| initializationModel.
          (default: None)","['Train', 'a', 'k', '-', 'means', 'clustering', 'model', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L307-L357,train,Train a k - means clustering model on the given data set.
apache/spark,python/pyspark/mllib/clustering.py,GaussianMixture.train,"def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):
        """"""
        Train a Gaussian Mixture clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of independent Gaussians in the mixture model.
        :param convergenceTol:
          Maximum change in log-likelihood at which convergence is
          considered to have occurred.
          (default: 1e-3)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param seed:
          Random seed for initial Gaussian distribution. Set as None to
          generate seed based on system time.
          (default: None)
        :param initialModel:
          Initial GMM starting point, bypassing the random
          initialization.
          (default: None)
        """"""
        initialModelWeights = None
        initialModelMu = None
        initialModelSigma = None
        if initialModel is not None:
            if initialModel.k != k:
                raise Exception(""Mismatched cluster count, initialModel.k = %s, however k = %s""
                                % (initialModel.k, k))
            initialModelWeights = list(initialModel.weights)
            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]
            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]
        java_model = callMLlibFunc(""trainGaussianMixtureModel"", rdd.map(_convert_to_vector),
                                   k, convergenceTol, maxIterations, seed,
                                   initialModelWeights, initialModelMu, initialModelSigma)
        return GaussianMixtureModel(java_model)",python,"def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None):
        """"""
        Train a Gaussian Mixture clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of independent Gaussians in the mixture model.
        :param convergenceTol:
          Maximum change in log-likelihood at which convergence is
          considered to have occurred.
          (default: 1e-3)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param seed:
          Random seed for initial Gaussian distribution. Set as None to
          generate seed based on system time.
          (default: None)
        :param initialModel:
          Initial GMM starting point, bypassing the random
          initialization.
          (default: None)
        """"""
        initialModelWeights = None
        initialModelMu = None
        initialModelSigma = None
        if initialModel is not None:
            if initialModel.k != k:
                raise Exception(""Mismatched cluster count, initialModel.k = %s, however k = %s""
                                % (initialModel.k, k))
            initialModelWeights = list(initialModel.weights)
            initialModelMu = [initialModel.gaussians[i].mu for i in range(initialModel.k)]
            initialModelSigma = [initialModel.gaussians[i].sigma for i in range(initialModel.k)]
        java_model = callMLlibFunc(""trainGaussianMixtureModel"", rdd.map(_convert_to_vector),
                                   k, convergenceTol, maxIterations, seed,
                                   initialModelWeights, initialModelMu, initialModelSigma)
        return GaussianMixtureModel(java_model)","['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'convergenceTol', '=', '1e-3', ',', 'maxIterations', '=', '100', ',', 'seed', '=', 'None', ',', 'initialModel', '=', 'None', ')', ':', 'initialModelWeights', '=', 'None', 'initialModelMu', '=', 'None', 'initialModelSigma', '=', 'None', 'if', 'initialModel', 'is', 'not', 'None', ':', 'if', 'initialModel', '.', 'k', '!=', 'k', ':', 'raise', 'Exception', '(', '""Mismatched cluster count, initialModel.k = %s, however k = %s""', '%', '(', 'initialModel', '.', 'k', ',', 'k', ')', ')', 'initialModelWeights', '=', 'list', '(', 'initialModel', '.', 'weights', ')', 'initialModelMu', '=', '[', 'initialModel', '.', 'gaussians', '[', 'i', ']', '.', 'mu', 'for', 'i', 'in', 'range', '(', 'initialModel', '.', 'k', ')', ']', 'initialModelSigma', '=', '[', 'initialModel', '.', 'gaussians', '[', 'i', ']', '.', 'sigma', 'for', 'i', 'in', 'range', '(', 'initialModel', '.', 'k', ')', ']', 'java_model', '=', 'callMLlibFunc', '(', '""trainGaussianMixtureModel""', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'k', ',', 'convergenceTol', ',', 'maxIterations', ',', 'seed', ',', 'initialModelWeights', ',', 'initialModelMu', ',', 'initialModelSigma', ')', 'return', 'GaussianMixtureModel', '(', 'java_model', ')']","Train a Gaussian Mixture clustering model.

        :param rdd:
          Training points as an `RDD` of `Vector` or convertible
          sequence types.
        :param k:
          Number of independent Gaussians in the mixture model.
        :param convergenceTol:
          Maximum change in log-likelihood at which convergence is
          considered to have occurred.
          (default: 1e-3)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 100)
        :param seed:
          Random seed for initial Gaussian distribution. Set as None to
          generate seed based on system time.
          (default: None)
        :param initialModel:
          Initial GMM starting point, bypassing the random
          initialization.
          (default: None)","['Train', 'a', 'Gaussian', 'Mixture', 'clustering', 'model', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L515-L553,train,Train a Gaussian Mixture clustering model.
apache/spark,python/pyspark/mllib/clustering.py,PowerIterationClusteringModel.load,"def load(cls, sc, path):
        """"""
        Load a model from the given path.
        """"""
        model = cls._load_java(sc, path)
        wrapper =\
            sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(model)
        return PowerIterationClusteringModel(wrapper)",python,"def load(cls, sc, path):
        """"""
        Load a model from the given path.
        """"""
        model = cls._load_java(sc, path)
        wrapper =\
            sc._jvm.org.apache.spark.mllib.api.python.PowerIterationClusteringModelWrapper(model)
        return PowerIterationClusteringModel(wrapper)","['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'model', '=', 'cls', '.', '_load_java', '(', 'sc', ',', 'path', ')', 'wrapper', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'PowerIterationClusteringModelWrapper', '(', 'model', ')', 'return', 'PowerIterationClusteringModel', '(', 'wrapper', ')']",Load a model from the given path.,"['Load', 'a', 'model', 'from', 'the', 'given', 'path', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L625-L632,train,Loads a PowerIterationClusteringModel from the given path.
apache/spark,python/pyspark/mllib/clustering.py,PowerIterationClustering.train,"def train(cls, rdd, k, maxIterations=100, initMode=""random""):
        r""""""
        :param rdd:
          An RDD of (i, j, s\ :sub:`ij`\) tuples representing the
          affinity matrix, which is the matrix A in the PIC paper.  The
          similarity s\ :sub:`ij`\ must be nonnegative.  This is a symmetric
          matrix and hence s\ :sub:`ij`\ = s\ :sub:`ji`\  For any (i, j) with
          nonzero similarity, there should be either (i, j, s\ :sub:`ij`\) or
          (j, i, s\ :sub:`ji`\) in the input.  Tuples with i = j are ignored,
          because it is assumed s\ :sub:`ij`\ = 0.0.
        :param k:
          Number of clusters.
        :param maxIterations:
          Maximum number of iterations of the PIC algorithm.
          (default: 100)
        :param initMode:
          Initialization mode. This can be either ""random"" to use
          a random vector as vertex properties, or ""degree"" to use
          normalized sum similarities.
          (default: ""random"")
        """"""
        model = callMLlibFunc(""trainPowerIterationClusteringModel"",
                              rdd.map(_convert_to_vector), int(k), int(maxIterations), initMode)
        return PowerIterationClusteringModel(model)",python,"def train(cls, rdd, k, maxIterations=100, initMode=""random""):
        r""""""
        :param rdd:
          An RDD of (i, j, s\ :sub:`ij`\) tuples representing the
          affinity matrix, which is the matrix A in the PIC paper.  The
          similarity s\ :sub:`ij`\ must be nonnegative.  This is a symmetric
          matrix and hence s\ :sub:`ij`\ = s\ :sub:`ji`\  For any (i, j) with
          nonzero similarity, there should be either (i, j, s\ :sub:`ij`\) or
          (j, i, s\ :sub:`ji`\) in the input.  Tuples with i = j are ignored,
          because it is assumed s\ :sub:`ij`\ = 0.0.
        :param k:
          Number of clusters.
        :param maxIterations:
          Maximum number of iterations of the PIC algorithm.
          (default: 100)
        :param initMode:
          Initialization mode. This can be either ""random"" to use
          a random vector as vertex properties, or ""degree"" to use
          normalized sum similarities.
          (default: ""random"")
        """"""
        model = callMLlibFunc(""trainPowerIterationClusteringModel"",
                              rdd.map(_convert_to_vector), int(k), int(maxIterations), initMode)
        return PowerIterationClusteringModel(model)","['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', ',', 'maxIterations', '=', '100', ',', 'initMode', '=', '""random""', ')', ':', 'model', '=', 'callMLlibFunc', '(', '""trainPowerIterationClusteringModel""', ',', 'rdd', '.', 'map', '(', '_convert_to_vector', ')', ',', 'int', '(', 'k', ')', ',', 'int', '(', 'maxIterations', ')', ',', 'initMode', ')', 'return', 'PowerIterationClusteringModel', '(', 'model', ')']","r""""""
        :param rdd:
          An RDD of (i, j, s\ :sub:`ij`\) tuples representing the
          affinity matrix, which is the matrix A in the PIC paper.  The
          similarity s\ :sub:`ij`\ must be nonnegative.  This is a symmetric
          matrix and hence s\ :sub:`ij`\ = s\ :sub:`ji`\  For any (i, j) with
          nonzero similarity, there should be either (i, j, s\ :sub:`ij`\) or
          (j, i, s\ :sub:`ji`\) in the input.  Tuples with i = j are ignored,
          because it is assumed s\ :sub:`ij`\ = 0.0.
        :param k:
          Number of clusters.
        :param maxIterations:
          Maximum number of iterations of the PIC algorithm.
          (default: 100)
        :param initMode:
          Initialization mode. This can be either ""random"" to use
          a random vector as vertex properties, or ""degree"" to use
          normalized sum similarities.
          (default: ""random"")","['r', ':', 'param', 'rdd', ':', 'An', 'RDD', 'of', '(', 'i', 'j', 's', '\\', ':', 'sub', ':', 'ij', '\\', ')', 'tuples', 'representing', 'the', 'affinity', 'matrix', 'which', 'is', 'the', 'matrix', 'A', 'in', 'the', 'PIC', 'paper', '.', 'The', 'similarity', 's', '\\', ':', 'sub', ':', 'ij', '\\', 'must', 'be', 'nonnegative', '.', 'This', 'is', 'a', 'symmetric', 'matrix', 'and', 'hence', 's', '\\', ':', 'sub', ':', 'ij', '\\', '=', 's', '\\', ':', 'sub', ':', 'ji', '\\', 'For', 'any', '(', 'i', 'j', ')', 'with', 'nonzero', 'similarity', 'there', 'should', 'be', 'either', '(', 'i', 'j', 's', '\\', ':', 'sub', ':', 'ij', '\\', ')', 'or', '(', 'j', 'i', 's', '\\', ':', 'sub', ':', 'ji', '\\', ')', 'in', 'the', 'input', '.', 'Tuples', 'with', 'i', '=', 'j', 'are', 'ignored', 'because', 'it', 'is', 'assumed', 's', '\\', ':', 'sub', ':', 'ij', '\\', '=', '0', '.', '0', '.', ':', 'param', 'k', ':', 'Number', 'of', 'clusters', '.', ':', 'param', 'maxIterations', ':', 'Maximum', 'number', 'of', 'iterations', 'of', 'the', 'PIC', 'algorithm', '.', '(', 'default', ':', '100', ')', ':', 'param', 'initMode', ':', 'Initialization', 'mode', '.', 'This', 'can', 'be', 'either', 'random', 'to', 'use', 'a', 'random', 'vector', 'as', 'vertex', 'properties', 'or', 'degree', 'to', 'use', 'normalized', 'sum', 'similarities', '.', '(', 'default', ':', 'random', ')']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L648-L671,train,r Trains a power iteration clustering model on the given RDD.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeansModel.update,"def update(self, data, decayFactor, timeUnit):
        """"""Update the centroids, according to data

        :param data:
          RDD with new data for the model update.
        :param decayFactor:
          Forgetfulness of the previous centroids.
        :param timeUnit:
          Can be ""batches"" or ""points"". If points, then the decay factor
          is raised to the power of number of new points and if batches,
          then decay factor will be used as is.
        """"""
        if not isinstance(data, RDD):
            raise TypeError(""Data should be of an RDD, got %s."" % type(data))
        data = data.map(_convert_to_vector)
        decayFactor = float(decayFactor)
        if timeUnit not in [""batches"", ""points""]:
            raise ValueError(
                ""timeUnit should be 'batches' or 'points', got %s."" % timeUnit)
        vectorCenters = [_convert_to_vector(center) for center in self.centers]
        updatedModel = callMLlibFunc(
            ""updateStreamingKMeansModel"", vectorCenters, self._clusterWeights,
            data, decayFactor, timeUnit)
        self.centers = array(updatedModel[0])
        self._clusterWeights = list(updatedModel[1])
        return self",python,"def update(self, data, decayFactor, timeUnit):
        """"""Update the centroids, according to data

        :param data:
          RDD with new data for the model update.
        :param decayFactor:
          Forgetfulness of the previous centroids.
        :param timeUnit:
          Can be ""batches"" or ""points"". If points, then the decay factor
          is raised to the power of number of new points and if batches,
          then decay factor will be used as is.
        """"""
        if not isinstance(data, RDD):
            raise TypeError(""Data should be of an RDD, got %s."" % type(data))
        data = data.map(_convert_to_vector)
        decayFactor = float(decayFactor)
        if timeUnit not in [""batches"", ""points""]:
            raise ValueError(
                ""timeUnit should be 'batches' or 'points', got %s."" % timeUnit)
        vectorCenters = [_convert_to_vector(center) for center in self.centers]
        updatedModel = callMLlibFunc(
            ""updateStreamingKMeansModel"", vectorCenters, self._clusterWeights,
            data, decayFactor, timeUnit)
        self.centers = array(updatedModel[0])
        self._clusterWeights = list(updatedModel[1])
        return self","['def', 'update', '(', 'self', ',', 'data', ',', 'decayFactor', ',', 'timeUnit', ')', ':', 'if', 'not', 'isinstance', '(', 'data', ',', 'RDD', ')', ':', 'raise', 'TypeError', '(', '""Data should be of an RDD, got %s.""', '%', 'type', '(', 'data', ')', ')', 'data', '=', 'data', '.', 'map', '(', '_convert_to_vector', ')', 'decayFactor', '=', 'float', '(', 'decayFactor', ')', 'if', 'timeUnit', 'not', 'in', '[', '""batches""', ',', '""points""', ']', ':', 'raise', 'ValueError', '(', '""timeUnit should be \'batches\' or \'points\', got %s.""', '%', 'timeUnit', ')', 'vectorCenters', '=', '[', '_convert_to_vector', '(', 'center', ')', 'for', 'center', 'in', 'self', '.', 'centers', ']', 'updatedModel', '=', 'callMLlibFunc', '(', '""updateStreamingKMeansModel""', ',', 'vectorCenters', ',', 'self', '.', '_clusterWeights', ',', 'data', ',', 'decayFactor', ',', 'timeUnit', ')', 'self', '.', 'centers', '=', 'array', '(', 'updatedModel', '[', '0', ']', ')', 'self', '.', '_clusterWeights', '=', 'list', '(', 'updatedModel', '[', '1', ']', ')', 'return', 'self']","Update the centroids, according to data

        :param data:
          RDD with new data for the model update.
        :param decayFactor:
          Forgetfulness of the previous centroids.
        :param timeUnit:
          Can be ""batches"" or ""points"". If points, then the decay factor
          is raised to the power of number of new points and if batches,
          then decay factor will be used as is.","['Update', 'the', 'centroids', 'according', 'to', 'data']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L752-L777,train,Update the k - MEAN model with new data.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.setHalfLife,"def setHalfLife(self, halfLife, timeUnit):
        """"""
        Set number of batches after which the centroids of that
        particular batch has half the weightage.
        """"""
        self._timeUnit = timeUnit
        self._decayFactor = exp(log(0.5) / halfLife)
        return self",python,"def setHalfLife(self, halfLife, timeUnit):
        """"""
        Set number of batches after which the centroids of that
        particular batch has half the weightage.
        """"""
        self._timeUnit = timeUnit
        self._decayFactor = exp(log(0.5) / halfLife)
        return self","['def', 'setHalfLife', '(', 'self', ',', 'halfLife', ',', 'timeUnit', ')', ':', 'self', '.', '_timeUnit', '=', 'timeUnit', 'self', '.', '_decayFactor', '=', 'exp', '(', 'log', '(', '0.5', ')', '/', 'halfLife', ')', 'return', 'self']","Set number of batches after which the centroids of that
        particular batch has half the weightage.","['Set', 'number', 'of', 'batches', 'after', 'which', 'the', 'centroids', 'of', 'that', 'particular', 'batch', 'has', 'half', 'the', 'weightage', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L838-L845,train,Sets the number of batches after which the centroids of that set have half the weightage.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.setInitialCenters,"def setInitialCenters(self, centers, weights):
        """"""
        Set initial centers. Should be set before calling trainOn.
        """"""
        self._model = StreamingKMeansModel(centers, weights)
        return self",python,"def setInitialCenters(self, centers, weights):
        """"""
        Set initial centers. Should be set before calling trainOn.
        """"""
        self._model = StreamingKMeansModel(centers, weights)
        return self","['def', 'setInitialCenters', '(', 'self', ',', 'centers', ',', 'weights', ')', ':', 'self', '.', '_model', '=', 'StreamingKMeansModel', '(', 'centers', ',', 'weights', ')', 'return', 'self']",Set initial centers. Should be set before calling trainOn.,"['Set', 'initial', 'centers', '.', 'Should', 'be', 'set', 'before', 'calling', 'trainOn', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L848-L853,train,Set initial centers for this store.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.setRandomCenters,"def setRandomCenters(self, dim, weight, seed):
        """"""
        Set the initial centres to be random samples from
        a gaussian population with constant weights.
        """"""
        rng = random.RandomState(seed)
        clusterCenters = rng.randn(self._k, dim)
        clusterWeights = tile(weight, self._k)
        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)
        return self",python,"def setRandomCenters(self, dim, weight, seed):
        """"""
        Set the initial centres to be random samples from
        a gaussian population with constant weights.
        """"""
        rng = random.RandomState(seed)
        clusterCenters = rng.randn(self._k, dim)
        clusterWeights = tile(weight, self._k)
        self._model = StreamingKMeansModel(clusterCenters, clusterWeights)
        return self","['def', 'setRandomCenters', '(', 'self', ',', 'dim', ',', 'weight', ',', 'seed', ')', ':', 'rng', '=', 'random', '.', 'RandomState', '(', 'seed', ')', 'clusterCenters', '=', 'rng', '.', 'randn', '(', 'self', '.', '_k', ',', 'dim', ')', 'clusterWeights', '=', 'tile', '(', 'weight', ',', 'self', '.', '_k', ')', 'self', '.', '_model', '=', 'StreamingKMeansModel', '(', 'clusterCenters', ',', 'clusterWeights', ')', 'return', 'self']","Set the initial centres to be random samples from
        a gaussian population with constant weights.","['Set', 'the', 'initial', 'centres', 'to', 'be', 'random', 'samples', 'from', 'a', 'gaussian', 'population', 'with', 'constant', 'weights', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L856-L865,train,Set the initial centres to be random samples from the gaussian population with constant weights.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.trainOn,"def trainOn(self, dstream):
        """"""Train the model on the incoming dstream.""""""
        self._validate(dstream)

        def update(rdd):
            self._model.update(rdd, self._decayFactor, self._timeUnit)

        dstream.foreachRDD(update)",python,"def trainOn(self, dstream):
        """"""Train the model on the incoming dstream.""""""
        self._validate(dstream)

        def update(rdd):
            self._model.update(rdd, self._decayFactor, self._timeUnit)

        dstream.foreachRDD(update)","['def', 'trainOn', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'def', 'update', '(', 'rdd', ')', ':', 'self', '.', '_model', '.', 'update', '(', 'rdd', ',', 'self', '.', '_decayFactor', ',', 'self', '.', '_timeUnit', ')', 'dstream', '.', 'foreachRDD', '(', 'update', ')']",Train the model on the incoming dstream.,"['Train', 'the', 'model', 'on', 'the', 'incoming', 'dstream', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L868-L875,train,Train the model on the incoming dstream.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.predictOn,"def predictOn(self, dstream):
        """"""
        Make predictions on a dstream.
        Returns a transformed dstream object
        """"""
        self._validate(dstream)
        return dstream.map(lambda x: self._model.predict(x))",python,"def predictOn(self, dstream):
        """"""
        Make predictions on a dstream.
        Returns a transformed dstream object
        """"""
        self._validate(dstream)
        return dstream.map(lambda x: self._model.predict(x))","['def', 'predictOn', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'return', 'dstream', '.', 'map', '(', 'lambda', 'x', ':', 'self', '.', '_model', '.', 'predict', '(', 'x', ')', ')']","Make predictions on a dstream.
        Returns a transformed dstream object","['Make', 'predictions', 'on', 'a', 'dstream', '.', 'Returns', 'a', 'transformed', 'dstream', 'object']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L878-L884,train,Make predictions on a dstream.
apache/spark,python/pyspark/mllib/clustering.py,StreamingKMeans.predictOnValues,"def predictOnValues(self, dstream):
        """"""
        Make predictions on a keyed dstream.
        Returns a transformed dstream object.
        """"""
        self._validate(dstream)
        return dstream.mapValues(lambda x: self._model.predict(x))",python,"def predictOnValues(self, dstream):
        """"""
        Make predictions on a keyed dstream.
        Returns a transformed dstream object.
        """"""
        self._validate(dstream)
        return dstream.mapValues(lambda x: self._model.predict(x))","['def', 'predictOnValues', '(', 'self', ',', 'dstream', ')', ':', 'self', '.', '_validate', '(', 'dstream', ')', 'return', 'dstream', '.', 'mapValues', '(', 'lambda', 'x', ':', 'self', '.', '_model', '.', 'predict', '(', 'x', ')', ')']","Make predictions on a keyed dstream.
        Returns a transformed dstream object.","['Make', 'predictions', 'on', 'a', 'keyed', 'dstream', '.', 'Returns', 'a', 'transformed', 'dstream', 'object', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L887-L893,train,Make predictions on a keyed dstream.
apache/spark,python/pyspark/mllib/clustering.py,LDAModel.describeTopics,"def describeTopics(self, maxTermsPerTopic=None):
        """"""Return the topics described by weighted terms.

        WARNING: If vocabSize and k are large, this can return a large object!

        :param maxTermsPerTopic:
          Maximum number of terms to collect for each topic.
          (default: vocabulary size)
        :return:
          Array over topics. Each topic is represented as a pair of
          matching arrays: (term indices, term weights in topic).
          Each topic's terms are sorted in order of decreasing weight.
        """"""
        if maxTermsPerTopic is None:
            topics = self.call(""describeTopics"")
        else:
            topics = self.call(""describeTopics"", maxTermsPerTopic)
        return topics",python,"def describeTopics(self, maxTermsPerTopic=None):
        """"""Return the topics described by weighted terms.

        WARNING: If vocabSize and k are large, this can return a large object!

        :param maxTermsPerTopic:
          Maximum number of terms to collect for each topic.
          (default: vocabulary size)
        :return:
          Array over topics. Each topic is represented as a pair of
          matching arrays: (term indices, term weights in topic).
          Each topic's terms are sorted in order of decreasing weight.
        """"""
        if maxTermsPerTopic is None:
            topics = self.call(""describeTopics"")
        else:
            topics = self.call(""describeTopics"", maxTermsPerTopic)
        return topics","['def', 'describeTopics', '(', 'self', ',', 'maxTermsPerTopic', '=', 'None', ')', ':', 'if', 'maxTermsPerTopic', 'is', 'None', ':', 'topics', '=', 'self', '.', 'call', '(', '""describeTopics""', ')', 'else', ':', 'topics', '=', 'self', '.', 'call', '(', '""describeTopics""', ',', 'maxTermsPerTopic', ')', 'return', 'topics']","Return the topics described by weighted terms.

        WARNING: If vocabSize and k are large, this can return a large object!

        :param maxTermsPerTopic:
          Maximum number of terms to collect for each topic.
          (default: vocabulary size)
        :return:
          Array over topics. Each topic is represented as a pair of
          matching arrays: (term indices, term weights in topic).
          Each topic's terms are sorted in order of decreasing weight.","['Return', 'the', 'topics', 'described', 'by', 'weighted', 'terms', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L955-L972,train,Return the topics described by weighted terms.
apache/spark,python/pyspark/mllib/clustering.py,LDAModel.load,"def load(cls, sc, path):
        """"""Load the LDAModel from disk.

        :param sc:
          SparkContext.
        :param path:
          Path to where the model is stored.
        """"""
        if not isinstance(sc, SparkContext):
            raise TypeError(""sc should be a SparkContext, got type %s"" % type(sc))
        if not isinstance(path, basestring):
            raise TypeError(""path should be a basestring, got type %s"" % type(path))
        model = callMLlibFunc(""loadLDAModel"", sc, path)
        return LDAModel(model)",python,"def load(cls, sc, path):
        """"""Load the LDAModel from disk.

        :param sc:
          SparkContext.
        :param path:
          Path to where the model is stored.
        """"""
        if not isinstance(sc, SparkContext):
            raise TypeError(""sc should be a SparkContext, got type %s"" % type(sc))
        if not isinstance(path, basestring):
            raise TypeError(""path should be a basestring, got type %s"" % type(path))
        model = callMLlibFunc(""loadLDAModel"", sc, path)
        return LDAModel(model)","['def', 'load', '(', 'cls', ',', 'sc', ',', 'path', ')', ':', 'if', 'not', 'isinstance', '(', 'sc', ',', 'SparkContext', ')', ':', 'raise', 'TypeError', '(', '""sc should be a SparkContext, got type %s""', '%', 'type', '(', 'sc', ')', ')', 'if', 'not', 'isinstance', '(', 'path', ',', 'basestring', ')', ':', 'raise', 'TypeError', '(', '""path should be a basestring, got type %s""', '%', 'type', '(', 'path', ')', ')', 'model', '=', 'callMLlibFunc', '(', '""loadLDAModel""', ',', 'sc', ',', 'path', ')', 'return', 'LDAModel', '(', 'model', ')']","Load the LDAModel from disk.

        :param sc:
          SparkContext.
        :param path:
          Path to where the model is stored.","['Load', 'the', 'LDAModel', 'from', 'disk', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L976-L989,train,Load the LDAModel from disk.
apache/spark,python/pyspark/mllib/clustering.py,LDA.train,"def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,
              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=""em""):
        """"""Train a LDA model.

        :param rdd:
          RDD of documents, which are tuples of document IDs and term
          (word) count vectors. The term count vectors are ""bags of
          words"" with a fixed-size vocabulary (where the vocabulary size
          is the length of the vector). Document IDs must be unique
          and >= 0.
        :param k:
          Number of topics to infer, i.e., the number of soft cluster
          centers.
          (default: 10)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 20)
        :param docConcentration:
          Concentration parameter (commonly named ""alpha"") for the prior
          placed on documents' distributions over topics (""theta"").
          (default: -1.0)
        :param topicConcentration:
          Concentration parameter (commonly named ""beta"" or ""eta"") for
          the prior placed on topics' distributions over terms.
          (default: -1.0)
        :param seed:
          Random seed for cluster initialization. Set as None to generate
          seed based on system time.
          (default: None)
        :param checkpointInterval:
          Period (in iterations) between checkpoints.
          (default: 10)
        :param optimizer:
          LDAOptimizer used to perform the actual calculation. Currently
          ""em"", ""online"" are supported.
          (default: ""em"")
        """"""
        model = callMLlibFunc(""trainLDAModel"", rdd, k, maxIterations,
                              docConcentration, topicConcentration, seed,
                              checkpointInterval, optimizer)
        return LDAModel(model)",python,"def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,
              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=""em""):
        """"""Train a LDA model.

        :param rdd:
          RDD of documents, which are tuples of document IDs and term
          (word) count vectors. The term count vectors are ""bags of
          words"" with a fixed-size vocabulary (where the vocabulary size
          is the length of the vector). Document IDs must be unique
          and >= 0.
        :param k:
          Number of topics to infer, i.e., the number of soft cluster
          centers.
          (default: 10)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 20)
        :param docConcentration:
          Concentration parameter (commonly named ""alpha"") for the prior
          placed on documents' distributions over topics (""theta"").
          (default: -1.0)
        :param topicConcentration:
          Concentration parameter (commonly named ""beta"" or ""eta"") for
          the prior placed on topics' distributions over terms.
          (default: -1.0)
        :param seed:
          Random seed for cluster initialization. Set as None to generate
          seed based on system time.
          (default: None)
        :param checkpointInterval:
          Period (in iterations) between checkpoints.
          (default: 10)
        :param optimizer:
          LDAOptimizer used to perform the actual calculation. Currently
          ""em"", ""online"" are supported.
          (default: ""em"")
        """"""
        model = callMLlibFunc(""trainLDAModel"", rdd, k, maxIterations,
                              docConcentration, topicConcentration, seed,
                              checkpointInterval, optimizer)
        return LDAModel(model)","['def', 'train', '(', 'cls', ',', 'rdd', ',', 'k', '=', '10', ',', 'maxIterations', '=', '20', ',', 'docConcentration', '=', '-', '1.0', ',', 'topicConcentration', '=', '-', '1.0', ',', 'seed', '=', 'None', ',', 'checkpointInterval', '=', '10', ',', 'optimizer', '=', '""em""', ')', ':', 'model', '=', 'callMLlibFunc', '(', '""trainLDAModel""', ',', 'rdd', ',', 'k', ',', 'maxIterations', ',', 'docConcentration', ',', 'topicConcentration', ',', 'seed', ',', 'checkpointInterval', ',', 'optimizer', ')', 'return', 'LDAModel', '(', 'model', ')']","Train a LDA model.

        :param rdd:
          RDD of documents, which are tuples of document IDs and term
          (word) count vectors. The term count vectors are ""bags of
          words"" with a fixed-size vocabulary (where the vocabulary size
          is the length of the vector). Document IDs must be unique
          and >= 0.
        :param k:
          Number of topics to infer, i.e., the number of soft cluster
          centers.
          (default: 10)
        :param maxIterations:
          Maximum number of iterations allowed.
          (default: 20)
        :param docConcentration:
          Concentration parameter (commonly named ""alpha"") for the prior
          placed on documents' distributions over topics (""theta"").
          (default: -1.0)
        :param topicConcentration:
          Concentration parameter (commonly named ""beta"" or ""eta"") for
          the prior placed on topics' distributions over terms.
          (default: -1.0)
        :param seed:
          Random seed for cluster initialization. Set as None to generate
          seed based on system time.
          (default: None)
        :param checkpointInterval:
          Period (in iterations) between checkpoints.
          (default: 10)
        :param optimizer:
          LDAOptimizer used to perform the actual calculation. Currently
          ""em"", ""online"" are supported.
          (default: ""em"")","['Train', 'a', 'LDA', 'model', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/clustering.py#L999-L1039,train,Train a LDA model.
apache/spark,python/pyspark/mllib/common.py,_to_java_object_rdd,"def _to_java_object_rdd(rdd):
    """""" Return a JavaRDD of Object by unpickling

    It will convert each Python object into Java object by Pyrolite, whenever the
    RDD is serialized in batch or not.
    """"""
    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))
    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)",python,"def _to_java_object_rdd(rdd):
    """""" Return a JavaRDD of Object by unpickling

    It will convert each Python object into Java object by Pyrolite, whenever the
    RDD is serialized in batch or not.
    """"""
    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))
    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)","['def', '_to_java_object_rdd', '(', 'rdd', ')', ':', 'rdd', '=', 'rdd', '.', '_reserialize', '(', 'AutoBatchedSerializer', '(', 'PickleSerializer', '(', ')', ')', ')', 'return', 'rdd', '.', 'ctx', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'SerDe', '.', 'pythonToJava', '(', 'rdd', '.', '_jrdd', ',', 'True', ')']","Return a JavaRDD of Object by unpickling

    It will convert each Python object into Java object by Pyrolite, whenever the
    RDD is serialized in batch or not.","['Return', 'a', 'JavaRDD', 'of', 'Object', 'by', 'unpickling']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L62-L69,train,"Return a JavaRDD of Object by unpickling
   "
apache/spark,python/pyspark/mllib/common.py,_py2java,"def _py2java(sc, obj):
    """""" Convert Python object into Java """"""
    if isinstance(obj, RDD):
        obj = _to_java_object_rdd(obj)
    elif isinstance(obj, DataFrame):
        obj = obj._jdf
    elif isinstance(obj, SparkContext):
        obj = obj._jsc
    elif isinstance(obj, list):
        obj = [_py2java(sc, x) for x in obj]
    elif isinstance(obj, JavaObject):
        pass
    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):
        pass
    else:
        data = bytearray(PickleSerializer().dumps(obj))
        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)
    return obj",python,"def _py2java(sc, obj):
    """""" Convert Python object into Java """"""
    if isinstance(obj, RDD):
        obj = _to_java_object_rdd(obj)
    elif isinstance(obj, DataFrame):
        obj = obj._jdf
    elif isinstance(obj, SparkContext):
        obj = obj._jsc
    elif isinstance(obj, list):
        obj = [_py2java(sc, x) for x in obj]
    elif isinstance(obj, JavaObject):
        pass
    elif isinstance(obj, (int, long, float, bool, bytes, unicode)):
        pass
    else:
        data = bytearray(PickleSerializer().dumps(obj))
        obj = sc._jvm.org.apache.spark.mllib.api.python.SerDe.loads(data)
    return obj","['def', '_py2java', '(', 'sc', ',', 'obj', ')', ':', 'if', 'isinstance', '(', 'obj', ',', 'RDD', ')', ':', 'obj', '=', '_to_java_object_rdd', '(', 'obj', ')', 'elif', 'isinstance', '(', 'obj', ',', 'DataFrame', ')', ':', 'obj', '=', 'obj', '.', '_jdf', 'elif', 'isinstance', '(', 'obj', ',', 'SparkContext', ')', ':', 'obj', '=', 'obj', '.', '_jsc', 'elif', 'isinstance', '(', 'obj', ',', 'list', ')', ':', 'obj', '=', '[', '_py2java', '(', 'sc', ',', 'x', ')', 'for', 'x', 'in', 'obj', ']', 'elif', 'isinstance', '(', 'obj', ',', 'JavaObject', ')', ':', 'pass', 'elif', 'isinstance', '(', 'obj', ',', '(', 'int', ',', 'long', ',', 'float', ',', 'bool', ',', 'bytes', ',', 'unicode', ')', ')', ':', 'pass', 'else', ':', 'data', '=', 'bytearray', '(', 'PickleSerializer', '(', ')', '.', 'dumps', '(', 'obj', ')', ')', 'obj', '=', 'sc', '.', '_jvm', '.', 'org', '.', 'apache', '.', 'spark', '.', 'mllib', '.', 'api', '.', 'python', '.', 'SerDe', '.', 'loads', '(', 'data', ')', 'return', 'obj']",Convert Python object into Java,"['Convert', 'Python', 'object', 'into', 'Java']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L72-L89,train,Convert Python object into Java object.
apache/spark,python/pyspark/mllib/common.py,callJavaFunc,"def callJavaFunc(sc, func, *args):
    """""" Call Java Function """"""
    args = [_py2java(sc, a) for a in args]
    return _java2py(sc, func(*args))",python,"def callJavaFunc(sc, func, *args):
    """""" Call Java Function """"""
    args = [_py2java(sc, a) for a in args]
    return _java2py(sc, func(*args))","['def', 'callJavaFunc', '(', 'sc', ',', 'func', ',', '*', 'args', ')', ':', 'args', '=', '[', '_py2java', '(', 'sc', ',', 'a', ')', 'for', 'a', 'in', 'args', ']', 'return', '_java2py', '(', 'sc', ',', 'func', '(', '*', 'args', ')', ')']",Call Java Function,"['Call', 'Java', 'Function']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L120-L123,train,Call Java Function
apache/spark,python/pyspark/mllib/common.py,callMLlibFunc,"def callMLlibFunc(name, *args):
    """""" Call API in PythonMLLibAPI """"""
    sc = SparkContext.getOrCreate()
    api = getattr(sc._jvm.PythonMLLibAPI(), name)
    return callJavaFunc(sc, api, *args)",python,"def callMLlibFunc(name, *args):
    """""" Call API in PythonMLLibAPI """"""
    sc = SparkContext.getOrCreate()
    api = getattr(sc._jvm.PythonMLLibAPI(), name)
    return callJavaFunc(sc, api, *args)","['def', 'callMLlibFunc', '(', 'name', ',', '*', 'args', ')', ':', 'sc', '=', 'SparkContext', '.', 'getOrCreate', '(', ')', 'api', '=', 'getattr', '(', 'sc', '.', '_jvm', '.', 'PythonMLLibAPI', '(', ')', ',', 'name', ')', 'return', 'callJavaFunc', '(', 'sc', ',', 'api', ',', '*', 'args', ')']",Call API in PythonMLLibAPI,"['Call', 'API', 'in', 'PythonMLLibAPI']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L126-L130,train,Call the specified PythonMLLib API with the given arguments.
apache/spark,python/pyspark/mllib/common.py,inherit_doc,"def inherit_doc(cls):
    """"""
    A decorator that makes a class inherit documentation from its parents.
    """"""
    for name, func in vars(cls).items():
        # only inherit docstring for public functions
        if name.startswith(""_""):
            continue
        if not func.__doc__:
            for parent in cls.__bases__:
                parent_func = getattr(parent, name, None)
                if parent_func and getattr(parent_func, ""__doc__"", None):
                    func.__doc__ = parent_func.__doc__
                    break
    return cls",python,"def inherit_doc(cls):
    """"""
    A decorator that makes a class inherit documentation from its parents.
    """"""
    for name, func in vars(cls).items():
        # only inherit docstring for public functions
        if name.startswith(""_""):
            continue
        if not func.__doc__:
            for parent in cls.__bases__:
                parent_func = getattr(parent, name, None)
                if parent_func and getattr(parent_func, ""__doc__"", None):
                    func.__doc__ = parent_func.__doc__
                    break
    return cls","['def', 'inherit_doc', '(', 'cls', ')', ':', 'for', 'name', ',', 'func', 'in', 'vars', '(', 'cls', ')', '.', 'items', '(', ')', ':', '# only inherit docstring for public functions', 'if', 'name', '.', 'startswith', '(', '""_""', ')', ':', 'continue', 'if', 'not', 'func', '.', '__doc__', ':', 'for', 'parent', 'in', 'cls', '.', '__bases__', ':', 'parent_func', '=', 'getattr', '(', 'parent', ',', 'name', ',', 'None', ')', 'if', 'parent_func', 'and', 'getattr', '(', 'parent_func', ',', '""__doc__""', ',', 'None', ')', ':', 'func', '.', '__doc__', '=', 'parent_func', '.', '__doc__', 'break', 'return', 'cls']",A decorator that makes a class inherit documentation from its parents.,"['A', 'decorator', 'that', 'makes', 'a', 'class', 'inherit', 'documentation', 'from', 'its', 'parents', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L149-L163,train,A decorator that makes a class inherit documentation from its parents.
apache/spark,python/pyspark/mllib/common.py,JavaModelWrapper.call,"def call(self, name, *a):
        """"""Call method of java_model""""""
        return callJavaFunc(self._sc, getattr(self._java_model, name), *a)",python,"def call(self, name, *a):
        """"""Call method of java_model""""""
        return callJavaFunc(self._sc, getattr(self._java_model, name), *a)","['def', 'call', '(', 'self', ',', 'name', ',', '*', 'a', ')', ':', 'return', 'callJavaFunc', '(', 'self', '.', '_sc', ',', 'getattr', '(', 'self', '.', '_java_model', ',', 'name', ')', ',', '*', 'a', ')']",Call method of java_model,"['Call', 'method', 'of', 'java_model']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/mllib/common.py#L144-L146,train,Call method of java_model
apache/spark,python/pyspark/streaming/dstream.py,DStream.count,"def count(self):
        """"""
        Return a new DStream in which each RDD has a single element
        generated by counting each RDD of this DStream.
        """"""
        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).reduce(operator.add)",python,"def count(self):
        """"""
        Return a new DStream in which each RDD has a single element
        generated by counting each RDD of this DStream.
        """"""
        return self.mapPartitions(lambda i: [sum(1 for _ in i)]).reduce(operator.add)","['def', 'count', '(', 'self', ')', ':', 'return', 'self', '.', 'mapPartitions', '(', 'lambda', 'i', ':', '[', 'sum', '(', '1', 'for', '_', 'in', 'i', ')', ']', ')', '.', 'reduce', '(', 'operator', '.', 'add', ')']","Return a new DStream in which each RDD has a single element
        generated by counting each RDD of this DStream.","['Return', 'a', 'new', 'DStream', 'in', 'which', 'each', 'RDD', 'has', 'a', 'single', 'element', 'generated', 'by', 'counting', 'each', 'RDD', 'of', 'this', 'DStream', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L73-L78,train,"Return a new DStream in which each RDD has a single element
        generated by counting each element of this DStream."
apache/spark,python/pyspark/streaming/dstream.py,DStream.filter,"def filter(self, f):
        """"""
        Return a new DStream containing only the elements that satisfy predicate.
        """"""
        def func(iterator):
            return filter(f, iterator)
        return self.mapPartitions(func, True)",python,"def filter(self, f):
        """"""
        Return a new DStream containing only the elements that satisfy predicate.
        """"""
        def func(iterator):
            return filter(f, iterator)
        return self.mapPartitions(func, True)","['def', 'filter', '(', 'self', ',', 'f', ')', ':', 'def', 'func', '(', 'iterator', ')', ':', 'return', 'filter', '(', 'f', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ',', 'True', ')']",Return a new DStream containing only the elements that satisfy predicate.,"['Return', 'a', 'new', 'DStream', 'containing', 'only', 'the', 'elements', 'that', 'satisfy', 'predicate', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L80-L86,train,Return a new DStream containing only the elements that satisfy the predicate.
apache/spark,python/pyspark/streaming/dstream.py,DStream.map,"def map(self, f, preservesPartitioning=False):
        """"""
        Return a new DStream by applying a function to each element of DStream.
        """"""
        def func(iterator):
            return map(f, iterator)
        return self.mapPartitions(func, preservesPartitioning)",python,"def map(self, f, preservesPartitioning=False):
        """"""
        Return a new DStream by applying a function to each element of DStream.
        """"""
        def func(iterator):
            return map(f, iterator)
        return self.mapPartitions(func, preservesPartitioning)","['def', 'map', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'def', 'func', '(', 'iterator', ')', ':', 'return', 'map', '(', 'f', ',', 'iterator', ')', 'return', 'self', '.', 'mapPartitions', '(', 'func', ',', 'preservesPartitioning', ')']",Return a new DStream by applying a function to each element of DStream.,"['Return', 'a', 'new', 'DStream', 'by', 'applying', 'a', 'function', 'to', 'each', 'element', 'of', 'DStream', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L97-L103,train,Returns a new DStream by applying a function to each element of the DStream.
apache/spark,python/pyspark/streaming/dstream.py,DStream.mapPartitionsWithIndex,"def mapPartitionsWithIndex(self, f, preservesPartitioning=False):
        """"""
        Return a new DStream in which each RDD is generated by applying
        mapPartitionsWithIndex() to each RDDs of this DStream.
        """"""
        return self.transform(lambda rdd: rdd.mapPartitionsWithIndex(f, preservesPartitioning))",python,"def mapPartitionsWithIndex(self, f, preservesPartitioning=False):
        """"""
        Return a new DStream in which each RDD is generated by applying
        mapPartitionsWithIndex() to each RDDs of this DStream.
        """"""
        return self.transform(lambda rdd: rdd.mapPartitionsWithIndex(f, preservesPartitioning))","['def', 'mapPartitionsWithIndex', '(', 'self', ',', 'f', ',', 'preservesPartitioning', '=', 'False', ')', ':', 'return', 'self', '.', 'transform', '(', 'lambda', 'rdd', ':', 'rdd', '.', 'mapPartitionsWithIndex', '(', 'f', ',', 'preservesPartitioning', ')', ')']","Return a new DStream in which each RDD is generated by applying
        mapPartitionsWithIndex() to each RDDs of this DStream.","['Return', 'a', 'new', 'DStream', 'in', 'which', 'each', 'RDD', 'is', 'generated', 'by', 'applying', 'mapPartitionsWithIndex', '()', 'to', 'each', 'RDDs', 'of', 'this', 'DStream', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L114-L119,train,"Returns a new DStream by applying a function to each RDD of each RDD in which each RDD is generated by applying
        mapPartitionsWithIndex."
apache/spark,python/pyspark/streaming/dstream.py,DStream.reduce,"def reduce(self, func):
        """"""
        Return a new DStream in which each RDD has a single element
        generated by reducing each RDD of this DStream.
        """"""
        return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])",python,"def reduce(self, func):
        """"""
        Return a new DStream in which each RDD has a single element
        generated by reducing each RDD of this DStream.
        """"""
        return self.map(lambda x: (None, x)).reduceByKey(func, 1).map(lambda x: x[1])","['def', 'reduce', '(', 'self', ',', 'func', ')', ':', 'return', 'self', '.', 'map', '(', 'lambda', 'x', ':', '(', 'None', ',', 'x', ')', ')', '.', 'reduceByKey', '(', 'func', ',', '1', ')', '.', 'map', '(', 'lambda', 'x', ':', 'x', '[', '1', ']', ')']","Return a new DStream in which each RDD has a single element
        generated by reducing each RDD of this DStream.","['Return', 'a', 'new', 'DStream', 'in', 'which', 'each', 'RDD', 'has', 'a', 'single', 'element', 'generated', 'by', 'reducing', 'each', 'RDD', 'of', 'this', 'DStream', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L121-L126,train,"Return a new DStream in which each RDD has a single element
        generated by reducing each element of each RDD."
apache/spark,python/pyspark/streaming/dstream.py,DStream.reduceByKey,"def reduceByKey(self, func, numPartitions=None):
        """"""
        Return a new DStream by applying reduceByKey to each RDD.
        """"""
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism
        return self.combineByKey(lambda x: x, func, func, numPartitions)",python,"def reduceByKey(self, func, numPartitions=None):
        """"""
        Return a new DStream by applying reduceByKey to each RDD.
        """"""
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism
        return self.combineByKey(lambda x: x, func, func, numPartitions)","['def', 'reduceByKey', '(', 'self', ',', 'func', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'return', 'self', '.', 'combineByKey', '(', 'lambda', 'x', ':', 'x', ',', 'func', ',', 'func', ',', 'numPartitions', ')']",Return a new DStream by applying reduceByKey to each RDD.,"['Return', 'a', 'new', 'DStream', 'by', 'applying', 'reduceByKey', 'to', 'each', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L128-L134,train,Return a new DStream by applying reduceByKey to each RDD.
apache/spark,python/pyspark/streaming/dstream.py,DStream.combineByKey,"def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None):
        """"""
        Return a new DStream by applying combineByKey to each RDD.
        """"""
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism

        def func(rdd):
            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)
        return self.transform(func)",python,"def combineByKey(self, createCombiner, mergeValue, mergeCombiners,
                     numPartitions=None):
        """"""
        Return a new DStream by applying combineByKey to each RDD.
        """"""
        if numPartitions is None:
            numPartitions = self._sc.defaultParallelism

        def func(rdd):
            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)
        return self.transform(func)","['def', 'combineByKey', '(', 'self', ',', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', '=', 'None', ')', ':', 'if', 'numPartitions', 'is', 'None', ':', 'numPartitions', '=', 'self', '.', '_sc', '.', 'defaultParallelism', 'def', 'func', '(', 'rdd', ')', ':', 'return', 'rdd', '.', 'combineByKey', '(', 'createCombiner', ',', 'mergeValue', ',', 'mergeCombiners', ',', 'numPartitions', ')', 'return', 'self', '.', 'transform', '(', 'func', ')']",Return a new DStream by applying combineByKey to each RDD.,"['Return', 'a', 'new', 'DStream', 'by', 'applying', 'combineByKey', 'to', 'each', 'RDD', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L136-L146,train,Return a new DStream by applying combineByKey to each RDD.
apache/spark,python/pyspark/streaming/dstream.py,DStream.partitionBy,"def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """"""
        Return a copy of the DStream in which each RDD are partitioned
        using the specified partitioner.
        """"""
        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))",python,"def partitionBy(self, numPartitions, partitionFunc=portable_hash):
        """"""
        Return a copy of the DStream in which each RDD are partitioned
        using the specified partitioner.
        """"""
        return self.transform(lambda rdd: rdd.partitionBy(numPartitions, partitionFunc))","['def', 'partitionBy', '(', 'self', ',', 'numPartitions', ',', 'partitionFunc', '=', 'portable_hash', ')', ':', 'return', 'self', '.', 'transform', '(', 'lambda', 'rdd', ':', 'rdd', '.', 'partitionBy', '(', 'numPartitions', ',', 'partitionFunc', ')', ')']","Return a copy of the DStream in which each RDD are partitioned
        using the specified partitioner.","['Return', 'a', 'copy', 'of', 'the', 'DStream', 'in', 'which', 'each', 'RDD', 'are', 'partitioned', 'using', 'the', 'specified', 'partitioner', '.']",618d6bff71073c8c93501ab7392c3cc579730f0b,https://github.com/apache/spark/blob/618d6bff71073c8c93501ab7392c3cc579730f0b/python/pyspark/streaming/dstream.py#L148-L153,train,Return a new DStream in which each RDD is partitioned by numPartitions partitions.
